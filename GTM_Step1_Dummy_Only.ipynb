{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ GTM Step 1: Temporal Features + Google Trends\n",
    "\n",
    "## ğŸ“š íŠ¹ê°• 1ë‹¨ê³„: ê¸°ë³¸ ì‹œê³„ì—´ ì˜ˆì¸¡\n",
    "- **ì‚¬ìš© ëª¨ë‹¬ë¦¬í‹°**: Temporal Features (ë‚ ì§œ ì •ë³´) + Google Trends\n",
    "- **ëª©ì **: ì‹œê³„ì—´ ë°ì´í„°ë§Œìœ¼ë¡œ ë§¤ì¶œ ì˜ˆì¸¡ì˜ ê¸°ì´ˆ êµ¬í˜„\n",
    "- **í•™ìŠµ ëª©í‘œ**: \n",
    "  - Transformer ê¸°ë³¸ êµ¬ì¡° ì´í•´\n",
    "  - ì‹œê³„ì—´ ì¸ì½”ë”© (Positional Encoding)\n",
    "  - Google Trends ë°ì´í„° í™œìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install lightning --upgrade --quiet\n",
    "!pip install transformers scikit-learn pillow --quiet\n",
    "\n",
    "# Import\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import Adafactor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ… Lightning: {L.__version__}\")\n",
    "print(f\"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ğŸ§  ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ì •ì˜\n",
    "### 1ë‹¨ê³„ì—ì„œëŠ” ìµœì†Œí•œì˜ ì»´í¬ë„ŒíŠ¸ë§Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ëª¨ë“ˆë“¤\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=52):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=True):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  \n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))\n",
    "\n",
    "        return y\n",
    "\n",
    "print(\"âœ… ê¸°ë³¸ ëª¨ë“ˆ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ë‹¨ê³„: Dummy (ì‹œê°„) + GTrends ì¸ì½”ë”ë§Œ ì‚¬ìš©\n",
    "class DummyEmbedder(nn.Module):\n",
    "    \"\"\"ì‹œê°„ ì •ë³´ (ë‚ ì§œ) ì„ë² ë”©\"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.day_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.week_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.month_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.year_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.dummy_fusion = nn.Linear(embedding_dim*4, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, temporal_features):\n",
    "        d, w, m, y = temporal_features[:, 0].unsqueeze(1), temporal_features[:, 1].unsqueeze(1), \\\n",
    "            temporal_features[:, 2].unsqueeze(1), temporal_features[:, 3].unsqueeze(1)\n",
    "        d_emb, w_emb, m_emb, y_emb = self.day_embedding(d), self.week_embedding(w), self.month_embedding(m), self.year_embedding(y)\n",
    "        temporal_embeddings = self.dummy_fusion(torch.cat([d_emb, w_emb, m_emb, y_emb], dim=1))\n",
    "        temporal_embeddings = self.dropout(temporal_embeddings)\n",
    "        return temporal_embeddings\n",
    "\n",
    "class GTrendEmbedder(nn.Module):\n",
    "    \"\"\"Google Trends ë°ì´í„° ì¸ì½”ë”©\"\"\"\n",
    "    def __init__(self, forecast_horizon, embedding_dim, use_mask, trend_len, num_trends, gpu_num):\n",
    "        super().__init__()\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.input_linear = TimeDistributed(nn.Linear(num_trends, embedding_dim))\n",
    "        self.pos_embedding = PositionalEncoding(embedding_dim, max_len=trend_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dropout=0.2)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.use_mask = use_mask\n",
    "        self.gpu_num = gpu_num\n",
    "\n",
    "    def _generate_encoder_mask(self, size, forecast_horizon):\n",
    "        mask = torch.zeros((size, size))\n",
    "        split = math.gcd(size, forecast_horizon)\n",
    "        for i in range(0, size, split):\n",
    "            mask[i:i+split, i:i+split] = 1\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, gtrends):\n",
    "        gtrend_emb = self.input_linear(gtrends.permute(0,2,1))\n",
    "        gtrend_emb = self.pos_embedding(gtrend_emb.permute(1,0,2))\n",
    "        input_mask = self._generate_encoder_mask(gtrend_emb.shape[0], self.forecast_horizon).to(gtrend_emb.device)\n",
    "        if self.use_mask == 1:\n",
    "            gtrend_emb = self.encoder(gtrend_emb, input_mask)\n",
    "        else:\n",
    "            gtrend_emb = self.encoder(gtrend_emb)\n",
    "        return gtrend_emb\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \"\"\"ì»¤ìŠ¤í…€ íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” ë ˆì´ì–´\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, \n",
    "            memory_key_padding_mask=None, tgt_is_causal=None, memory_is_causal=None):\n",
    "        \n",
    "        # Self-attention block\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        # Cross-attention block\n",
    "        tgt2, attn_weights = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                                  key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        # Feedforward block\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        \n",
    "        return tgt, attn_weights\n",
    "\n",
    "print(\"âœ… 1ë‹¨ê³„ ì¸ì½”ë” (Dummy + GTrends) ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ğŸ¯ GTM Step 1 ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTM_Step1(L.LightningModule):\n",
    "    \"\"\"1ë‹¨ê³„: Temporal Features + Google Trendsë§Œ ì‚¬ìš©\"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_heads, num_layers, \n",
    "                 cat_dict, col_dict, fab_dict, trend_len, num_trends, gpu_num, use_encoder_mask=1, autoregressive=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_len = output_dim\n",
    "        self.use_encoder_mask = use_encoder_mask\n",
    "        self.autoregressive = autoregressive\n",
    "        self.gpu_num = gpu_num\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # 1ë‹¨ê³„ì—ì„œëŠ” Dummyì™€ GTrendë§Œ ì‚¬ìš©\n",
    "        self.dummy_encoder = DummyEmbedder(embedding_dim)\n",
    "        self.gtrend_encoder = GTrendEmbedder(output_dim, hidden_dim, use_encoder_mask, trend_len, num_trends, gpu_num)\n",
    "        \n",
    "        # ê°„ë‹¨í•œ ìœµí•© ë„¤íŠ¸ì›Œí¬ (ì‹œê°„ ì •ë³´ë§Œ)\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_layer = TransformerDecoderLayer(d_model=self.hidden_dim, nhead=num_heads, \n",
    "                                                    dim_feedforward=self.hidden_dim * 4, dropout=0.1)\n",
    "        \n",
    "        if self.autoregressive: \n",
    "            self.pos_encoder = PositionalEncoding(hidden_dim, max_len=12)\n",
    "        \n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, self.output_len if not self.autoregressive else 1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, category, color, fabric, temporal_features, gtrends, images):\n",
    "        # 1ë‹¨ê³„: ì‹œê°„ ì •ë³´ë§Œ ì‚¬ìš©\n",
    "        dummy_encoding = self.dummy_encoder(temporal_features)\n",
    "        gtrend_encoding = self.gtrend_encoder(gtrends)\n",
    "\n",
    "        # ê°„ë‹¨í•œ íŠ¹ì„± ìœµí•© (ì‹œê°„ ì •ë³´ë§Œ)\n",
    "        static_feature_fusion = self.feature_fusion(dummy_encoding)\n",
    "\n",
    "        # Decoder\n",
    "        tgt = static_feature_fusion.unsqueeze(0)\n",
    "        memory = gtrend_encoding\n",
    "        \n",
    "        decoder_out, attn_weights = self.decoder_layer(tgt, memory)\n",
    "        forecast = self.decoder_fc(decoder_out)\n",
    "\n",
    "        return forecast.view(-1, self.output_len), attn_weights\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adafactor(self.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        item_sales, category, color, fabric, temporal_features, gtrends, images = batch \n",
    "        \n",
    "        temporal_features = temporal_features.requires_grad_(True)\n",
    "        gtrends = gtrends.requires_grad_(True)\n",
    "        \n",
    "        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n",
    "        loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        item_sales, category, color, fabric, temporal_features, gtrends, images = batch \n",
    "        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n",
    "        \n",
    "        if not hasattr(self, 'validation_step_outputs'):\n",
    "            self.validation_step_outputs = []\n",
    "        self.validation_step_outputs.append((item_sales.squeeze(), forecasted_sales.squeeze()))\n",
    "        \n",
    "        return item_sales.squeeze(), forecasted_sales.squeeze()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if hasattr(self, 'validation_step_outputs'):\n",
    "            val_step_outputs = self.validation_step_outputs\n",
    "            item_sales, forecasted_sales = [x[0] for x in val_step_outputs], [x[1] for x in val_step_outputs]\n",
    "            item_sales, forecasted_sales = torch.stack(item_sales), torch.stack(forecasted_sales)\n",
    "            rescaled_item_sales, rescaled_forecasted_sales = item_sales*1065, forecasted_sales*1065\n",
    "            loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
    "            mae = F.l1_loss(rescaled_item_sales, rescaled_forecasted_sales)\n",
    "            \n",
    "            self.log('val_mae', mae, prog_bar=True)\n",
    "            self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "            print(f'Step 1 - Validation MAE: {mae.detach().cpu().numpy():.2f}, LR: {self.optimizers().param_groups[0][\"lr\"]:.2e}')\n",
    "            self.validation_step_outputs.clear()\n",
    "\n",
    "print(\"âœ… GTM Step 1 ëª¨ë¸ ì •ì˜ ì™„ë£Œ (Temporal + GTrends)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ğŸ“Š ë°ì´í„°ì…‹ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotDataset():\n",
    "    def __init__(self, data_df, img_root, gtrends, cat_dict, col_dict, fab_dict, trend_len):\n",
    "        self.data_df = data_df\n",
    "        self.gtrends = gtrends\n",
    "        self.cat_dict = cat_dict\n",
    "        self.col_dict = col_dict\n",
    "        self.fab_dict = fab_dict\n",
    "        self.trend_len = trend_len\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_df.iloc[idx, :]\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        data = self.data_df\n",
    "\n",
    "        gtrends, image_features = [], []\n",
    "        img_transforms = Compose([Resize((256, 256)), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "        for (idx, row) in tqdm(data.iterrows(), total=len(data), ascii=True, desc=\"ë°ì´í„° ì „ì²˜ë¦¬\"):\n",
    "            cat, col, fab, fiq_attr, start_date, img_path = row['category'], row['color'], row['fabric'], row['extra'], \\\n",
    "                row['release_date'], row['image_path']\n",
    "\n",
    "            # Google Trends ë°ì´í„° ì²˜ë¦¬\n",
    "            gtrend_start = start_date - pd.DateOffset(weeks=52)\n",
    "            cat_gtrend = self.gtrends.loc[gtrend_start:start_date][cat][-52:].values[:self.trend_len]\n",
    "            col_gtrend = self.gtrends.loc[gtrend_start:start_date][col][-52:].values[:self.trend_len]\n",
    "            fab_gtrend = self.gtrends.loc[gtrend_start:start_date][fab][-52:].values[:self.trend_len]\n",
    "\n",
    "            cat_gtrend = MinMaxScaler().fit_transform(cat_gtrend.reshape(-1,1)).flatten()\n",
    "            col_gtrend = MinMaxScaler().fit_transform(col_gtrend.reshape(-1,1)).flatten()\n",
    "            fab_gtrend = MinMaxScaler().fit_transform(fab_gtrend.reshape(-1,1)).flatten()\n",
    "            multitrends = np.vstack([cat_gtrend, col_gtrend, fab_gtrend])\n",
    "\n",
    "            # ì´ë¯¸ì§€ ì²˜ë¦¬ (1ë‹¨ê³„ì—ì„œëŠ” ì‚¬ìš©ì•ˆí•˜ì§€ë§Œ ë°ì´í„° êµ¬ì¡° ìœ ì§€)\n",
    "            img = Image.open(os.path.join(self.img_root, img_path)).convert('RGB')\n",
    "\n",
    "            gtrends.append(multitrends)\n",
    "            image_features.append(img_transforms(img))\n",
    "\n",
    "        gtrends = np.array(gtrends)\n",
    "\n",
    "        data = data.copy()\n",
    "        data.drop(['external_code', 'season', 'release_date', 'image_path'], axis=1, inplace=True)\n",
    "\n",
    "        # í…ì„œ ìƒì„±\n",
    "        item_sales, temporal_features = torch.FloatTensor(data.iloc[:, :12].values), torch.FloatTensor(\n",
    "            data.iloc[:, 13:17].values)\n",
    "        categories, colors, fabrics = [self.cat_dict[val] for val in data.iloc[:].category.values], \\\n",
    "                                       [self.col_dict[val] for val in data.iloc[:].color.values], \\\n",
    "                                       [self.fab_dict[val] for val in data.iloc[:].fabric.values]\n",
    "\n",
    "        categories, colors, fabrics = torch.LongTensor(categories), torch.LongTensor(colors), torch.LongTensor(fabrics)\n",
    "        gtrends = torch.FloatTensor(gtrends)\n",
    "        images = torch.stack(image_features)\n",
    "\n",
    "        return TensorDataset(item_sales, categories, colors, fabrics, temporal_features, gtrends, images)\n",
    "\n",
    "    def get_loader(self, batch_size, train=True):\n",
    "        print('ğŸ“Š 1ë‹¨ê³„ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...')\n",
    "        data_with_gtrends = self.preprocess_data()\n",
    "        if train:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        else:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=1, shuffle=False, num_workers=2)\n",
    "        print('âœ… 1ë‹¨ê³„ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ')\n",
    "        return data_loader\n",
    "\n",
    "print(\"âœ… ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ğŸš€ 1ë‹¨ê³„ ì‹¤í–‰ ì½”ë“œ\n",
    "### ë°ì´í„° ë¡œë”©ë¶€í„° ëª¨ë¸ í›ˆë ¨ê¹Œì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
    "dataset_path = Path('/content/drive/MyDrive/GTM-dataset-small/')\n",
    "\n",
    "# ë°ì´í„° ë¡œë”©\n",
    "print(\"ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "train_df = pd.read_csv(dataset_path / 'train.csv', parse_dates=['release_date'])\n",
    "test_df = pd.read_csv(dataset_path / 'test.csv', parse_dates=['release_date'])\n",
    "gtrends = pd.read_csv(dataset_path / 'gtrends.csv', index_col=[0], parse_dates=True)\n",
    "\n",
    "cat_dict = torch.load(dataset_path / 'category_labels.pt', weights_only=False)\n",
    "col_dict = torch.load(dataset_path / 'color_labels.pt', weights_only=False)\n",
    "fab_dict = torch.load(dataset_path / 'fabric_labels.pt', weights_only=False)\n",
    "\n",
    "print(f\"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_df):,}ê°œ\")\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ\")\n",
    "print(f\"âœ… Google Trends: {len(gtrends):,}ê°œ ì‹œì \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "train_dataset = ZeroShotDataset(train_df, dataset_path / 'images', gtrends, cat_dict, col_dict, fab_dict, trend_len=52)\n",
    "test_dataset = ZeroShotDataset(test_df, dataset_path / 'images', gtrends, cat_dict, col_dict, fab_dict, trend_len=52)\n",
    "\n",
    "BATCH_SIZE = 8 if torch.cuda.is_available() else 4\n",
    "train_loader = train_dataset.get_loader(batch_size=BATCH_SIZE, train=True)\n",
    "test_loader = test_dataset.get_loader(batch_size=1, train=False)\n",
    "\n",
    "print(f\"âœ… ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
    "print(f\"âœ… í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}\")\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìˆ˜: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ë‹¨ê³„ ëª¨ë¸ ìƒì„±\n",
    "print(\"ğŸ¯ GTM Step 1 ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
    "\n",
    "model = GTM_Step1(\n",
    "    embedding_dim=32,\n",
    "    hidden_dim=64,\n",
    "    output_dim=12,\n",
    "    num_heads=4,\n",
    "    num_layers=1,\n",
    "    cat_dict=cat_dict,\n",
    "    col_dict=col_dict,\n",
    "    fab_dict=fab_dict,\n",
    "    trend_len=52,\n",
    "    num_trends=3,\n",
    "    gpu_num=0,\n",
    "    use_encoder_mask=1,\n",
    "    autoregressive=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Step 1 ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"\\nğŸ” ì‚¬ìš© ëª¨ë‹¬ë¦¬í‹°: Temporal Features (ì‹œê°„ ì •ë³´) + Google Trends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer ì„¤ì • ë° í›ˆë ¨\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "EPOCHS = 5\n",
    "ACCELERATOR = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='./checkpoints/',\n",
    "    filename='gtm-step1-{epoch:02d}-{val_mae:.2f}',\n",
    "    monitor='val_mae',\n",
    "    mode='min',\n",
    "    save_top_k=2\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='./logs/', name='gtm_step1')\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=ACCELERATOR,\n",
    "    max_epochs=EPOCHS,\n",
    "    logger=csv_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ GTM Step 1 í›ˆë ¨ ì‹œì‘!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)\n",
    "    print(\"\\nğŸ‰ Step 1 í›ˆë ¨ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ’¾ ìµœê³  ëª¨ë¸: {checkpoint_callback.best_model_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Step 1 í›ˆë ¨ ì‹¤íŒ¨: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ 1ë‹¨ê³„ ìš”ì•½\n",
    "\n",
    "### âœ… êµ¬í˜„ ì™„ë£Œ\n",
    "- **ì‹œê°„ì  íŠ¹ì„± ì„ë² ë”©**: ë‚ ì§œ ì •ë³´ (ì¼, ì£¼, ì›”, ë…„)ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
    "- **Google Trends ì¸ì½”ë”©**: Transformer Encoderë¡œ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ\n",
    "- **ê¸°ë³¸ Cross-Attention**: ì‹œê°„ ì •ë³´ì™€ íŠ¸ë Œë“œ ë°ì´í„° ê°„ ê´€ê³„ í•™ìŠµ\n",
    "\n",
    "### ğŸ¯ í•™ìŠµ ëª©í‘œ ë‹¬ì„±\n",
    "- Transformer ê¸°ë³¸ êµ¬ì¡° ì´í•´\n",
    "- ì‹œê³„ì—´ ë°ì´í„° ì¸ì½”ë”© ë°©ë²•\n",
    "- Multi-modal ì…ë ¥ ì²˜ë¦¬ ê¸°ì´ˆ\n",
    "\n",
    "### ğŸ”œ ë‹¤ìŒ ë‹¨ê³„ ì˜ˆê³ \n",
    "**Step 2**ì—ì„œëŠ” **ì´ë¯¸ì§€ ì •ë³´**ë¥¼ ì¶”ê°€í•˜ì—¬ ì‹œê°ì  íŠ¹ì„±ë„ í•¨ê»˜ í•™ìŠµí•©ë‹ˆë‹¤!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}