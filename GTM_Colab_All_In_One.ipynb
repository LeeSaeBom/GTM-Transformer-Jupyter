{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ GTM (Google Trends Transformer) - All-in-One Version\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ëª¨ë“  ì½”ë“œê°€ í¬í•¨ëœ ë…ë¦½ì‹¤í–‰í˜• ë²„ì „ì…ë‹ˆë‹¤.\n",
    "- ì™¸ë¶€ py íŒŒì¼ ì—†ì´ ë…¸íŠ¸ë¶ ë‚´ì—ì„œ ëª¨ë“  í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ ì •ì˜\n",
    "- Google Driveì˜ ì‘ì€ ë°ì´í„°ì…‹ë§Œ í•„ìš”\n",
    "- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ë° ì‹¤í—˜ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Colab í™˜ê²½ì—ì„œ ì•ˆì •ì ì¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜\nimport subprocess\nimport sys\n\ndef install_package(package):\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n        print(f\"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"âŒ {package} ì„¤ì¹˜ ì‹¤íŒ¨: {e}\")\n        return False\n\n# tokenizers ë¬¸ì œ í•´ê²° - ë¯¸ë¦¬ ì»´íŒŒì¼ëœ ë²„ì „ ì‚¬ìš©\nprint(\"ğŸ”§ tokenizers ì„¤ì¹˜ ì¤‘...\")\n!pip install tokenizers --no-build-isolation --quiet\n\n# transformers í˜¸í™˜ ë²„ì „ ì„¤ì¹˜\nprint(\"ğŸ”§ transformers ì„¤ì¹˜ ì¤‘...\")\n!pip install transformers==4.21.0 --quiet\n\n# PyTorch Lightning ì•ˆì • ë²„ì „\nprint(\"ğŸ”§ PyTorch Lightning ì„¤ì¹˜ ì¤‘...\")\n!pip install pytorch-lightning==1.9.5 --quiet\n\n# ê¸°íƒ€ íŒ¨í‚¤ì§€\nprint(\"ğŸ”§ ê¸°íƒ€ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\n!pip install scikit-learn pillow --quiet\n\nprint(\"\\nğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ! import ì‹œì‘...\")\n\n# ëª¨ë“  import\nimport math\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image, ImageFile\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision.transforms import Resize, ToTensor, Normalize, Compose\nfrom torchvision import models\nfrom sklearn.preprocessing import MinMaxScaler\nfrom transformers import pipeline, Adafactor\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n# Google Drive ë§ˆìš´íŠ¸\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nprint(f\"âœ… PyTorch: {torch.__version__}\")\nprint(f\"âœ… PyTorch Lightning: {pl.__version__}\")\nprint(f\"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ğŸ§  ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (GTM.py ë‚´ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ê¸°ë³¸ ëª¨ë“ˆë“¤ ì •ì˜\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=52):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass TimeDistributed(nn.Module):\n    def __init__(self, module, batch_first=True):\n        super(TimeDistributed, self).__init__()\n        self.module = module\n        self.batch_first = batch_first\n\n    def forward(self, x):\n        if len(x.size()) <= 2:\n            return self.module(x)\n\n        x_reshape = x.contiguous().view(-1, x.size(-1))  \n        y = self.module(x_reshape)\n\n        if self.batch_first:\n            y = y.contiguous().view(x.size(0), -1, y.size(-1))\n        else:\n            y = y.view(-1, x.size(1), y.size(-1))\n\n        return y\n\nprint(\"âœ… ê¸°ë³¸ ëª¨ë“ˆ ì •ì˜ ì™„ë£Œ\")"
  },
  {
   "cell_type": "code",
   "source": "# ì¸ì½”ë” í´ë˜ìŠ¤ë“¤ ì •ì˜\nclass FusionNetwork(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, use_img, use_text, dropout=0.2):\n        super(FusionNetwork, self).__init__()\n        \n        self.img_pool = nn.AdaptiveAvgPool2d((1,1))\n        self.img_linear = nn.Linear(2048, embedding_dim)\n        self.use_img = use_img\n        self.use_text = use_text\n        input_dim = embedding_dim + (embedding_dim*use_img) + (embedding_dim*use_text)\n        self.feature_fusion = nn.Sequential(\n            nn.BatchNorm1d(input_dim),\n            nn.Linear(input_dim, input_dim, bias=False),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(input_dim, hidden_dim)\n        )\n\n    def forward(self, img_encoding, text_encoding, dummy_encoding):\n        pooled_img = self.img_pool(img_encoding)\n        condensed_img = self.img_linear(pooled_img.flatten(1))\n\n        decoder_inputs = []\n        if self.use_img == 1:\n            decoder_inputs.append(condensed_img) \n        if self.use_text == 1:\n            decoder_inputs.append(text_encoding) \n        decoder_inputs.append(dummy_encoding)\n        concat_features = torch.cat(decoder_inputs, dim=1)\n\n        final = self.feature_fusion(concat_features)\n        return final\n\nclass GTrendEmbedder(nn.Module):\n    def __init__(self, forecast_horizon, embedding_dim, use_mask, trend_len, num_trends, gpu_num):\n        super().__init__()\n        self.forecast_horizon = forecast_horizon\n        self.input_linear = TimeDistributed(nn.Linear(num_trends, embedding_dim))\n        self.pos_embedding = PositionalEncoding(embedding_dim, max_len=trend_len)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dropout=0.2)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n        self.use_mask = use_mask\n        self.gpu_num = gpu_num\n\n    def _generate_encoder_mask(self, size, forecast_horizon):\n        mask = torch.zeros((size, size))\n        split = math.gcd(size, forecast_horizon)\n        for i in range(0, size, split):\n            mask[i:i+split, i:i+split] = 1\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def forward(self, gtrends):\n        gtrend_emb = self.input_linear(gtrends.permute(0,2,1))\n        gtrend_emb = self.pos_embedding(gtrend_emb.permute(1,0,2))\n        input_mask = self._generate_encoder_mask(gtrend_emb.shape[0], self.forecast_horizon).to(gtrend_emb.device)\n        if self.use_mask == 1:\n            gtrend_emb = self.encoder(gtrend_emb, input_mask)\n        else:\n            gtrend_emb = self.encoder(gtrend_emb)\n        return gtrend_emb\n\nclass TextEmbedder(nn.Module):\n    def __init__(self, embedding_dim, cat_dict, col_dict, fab_dict, gpu_num):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.cat_dict = {v: k for k, v in cat_dict.items()}\n        self.col_dict = {v: k for k, v in col_dict.items()}\n        self.fab_dict = {v: k for k, v in fab_dict.items()}\n        self.word_embedder = pipeline('feature-extraction', model='bert-base-uncased')\n        self.fc = nn.Linear(768, embedding_dim)\n        self.dropout = nn.Dropout(0.1)\n        self.gpu_num = gpu_num\n\n    def forward(self, category, color, fabric):\n        textual_description = [self.col_dict[color.detach().cpu().numpy().tolist()[i]] + ' ' \\\n                + self.fab_dict[fabric.detach().cpu().numpy().tolist()[i]] + ' ' \\\n                + self.cat_dict[category.detach().cpu().numpy().tolist()[i]] for i in range(len(category))]\n\n        word_embeddings = self.word_embedder(textual_description)\n        word_embeddings = [torch.tensor(x[0][1:-1], dtype=torch.float32).mean(axis=0) if len(x[0]) > 2 else torch.tensor(x[0], dtype=torch.float32).mean(axis=0) for x in word_embeddings] \n        word_embeddings = torch.stack(word_embeddings).to(self.fc.weight.device).requires_grad_()\n        \n        word_embeddings = self.dropout(self.fc(word_embeddings))\n        return word_embeddings\n\nclass ImageEmbedder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        resnet = models.resnet50(pretrained=True)\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        \n        # ëª¨ë“  ResNet íŒŒë¼ë¯¸í„°ë¥¼ trainableë¡œ ì„¤ì • (gradient ë¬¸ì œ í•´ê²°)\n        for p in self.resnet.parameters():\n            p.requires_grad = True\n        \n    def forward(self, images):        \n        img_embeddings = self.resnet(images)  \n        size = img_embeddings.size()\n        out = img_embeddings.view(*size[:2],-1)\n        return out.view(*size).contiguous()\n\nclass DummyEmbedder(nn.Module):\n    def __init__(self, embedding_dim):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.day_embedding = nn.Linear(1, embedding_dim)\n        self.week_embedding = nn.Linear(1, embedding_dim)\n        self.month_embedding = nn.Linear(1, embedding_dim)\n        self.year_embedding = nn.Linear(1, embedding_dim)\n        self.dummy_fusion = nn.Linear(embedding_dim*4, embedding_dim)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, temporal_features):\n        d, w, m, y = temporal_features[:, 0].unsqueeze(1), temporal_features[:, 1].unsqueeze(1), \\\n            temporal_features[:, 2].unsqueeze(1), temporal_features[:, 3].unsqueeze(1)\n        d_emb, w_emb, m_emb, y_emb = self.day_embedding(d), self.week_embedding(w), self.month_embedding(m), self.year_embedding(y)\n        temporal_embeddings = self.dummy_fusion(torch.cat([d_emb, w_emb, m_emb, y_emb], dim=1))\n        temporal_embeddings = self.dropout(temporal_embeddings)\n        return temporal_embeddings\n\nclass TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n        super(TransformerDecoderLayer, self).__init__()\n        \n        # Add self_attn for compatibility with nn.TransformerDecoder\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = F.relu\n\n    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, \n            memory_key_padding_mask=None, tgt_is_causal=None, memory_is_causal=None):\n        \n        # Self-attention block\n        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n                              key_padding_mask=tgt_key_padding_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        \n        # Cross-attention block\n        tgt2, attn_weights = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n                                                  key_padding_mask=memory_key_padding_mask)\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        \n        # Feedforward block\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        \n        return tgt, attn_weights\n\nprint(\"âœ… ëª¨ë“  ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ì •ì˜ ì™„ë£Œ (PyTorch 2.x í˜¸í™˜, gradient ë¬¸ì œ í•´ê²°)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ğŸ¯ GTM ë©”ì¸ ëª¨ë¸ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class GTM(pl.LightningModule):\n    def __init__(self, embedding_dim, hidden_dim, output_dim, num_heads, num_layers, use_text, use_img, \\\n                cat_dict, col_dict, fab_dict, trend_len, num_trends, gpu_num, use_encoder_mask=1, autoregressive=False):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n        self.output_len = output_dim\n        self.use_encoder_mask = use_encoder_mask\n        self.autoregressive = autoregressive\n        self.gpu_num = gpu_num\n        self.save_hyperparameters()\n\n        # Encoder\n        self.dummy_encoder = DummyEmbedder(embedding_dim)\n        self.image_encoder = ImageEmbedder()\n        self.text_encoder = TextEmbedder(embedding_dim, cat_dict, col_dict, fab_dict, gpu_num)\n        self.gtrend_encoder = GTrendEmbedder(output_dim, hidden_dim, use_encoder_mask, trend_len, num_trends, gpu_num)\n        self.static_feature_encoder = FusionNetwork(embedding_dim, hidden_dim, use_img, use_text)\n\n        # Decoder - ë‹¨ì¼ ë ˆì´ì–´ë§Œ ì‚¬ìš©\n        self.decoder_layer = TransformerDecoderLayer(d_model=self.hidden_dim, nhead=num_heads, \n                                                    dim_feedforward=self.hidden_dim * 4, dropout=0.1)\n        \n        if self.autoregressive: \n            self.pos_encoder = PositionalEncoding(hidden_dim, max_len=12)\n        \n        self.decoder_fc = nn.Sequential(\n            nn.Linear(hidden_dim, self.output_len if not self.autoregressive else 1),\n            nn.Dropout(0.2)\n        )\n        \n    def _generate_square_subsequent_mask(self, size):\n        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def forward(self, category, color, fabric, temporal_features, gtrends, images):\n        # Encode features and get inputs\n        img_encoding = self.image_encoder(images)\n        dummy_encoding = self.dummy_encoder(temporal_features)\n        text_encoding = self.text_encoder(category, color, fabric)\n        gtrend_encoding = self.gtrend_encoder(gtrends)\n\n        # Fuse static features together\n        static_feature_fusion = self.static_feature_encoder(img_encoding, text_encoding, dummy_encoding)\n\n        if self.autoregressive == 1:\n            # Decode\n            tgt = torch.zeros(self.output_len, gtrend_encoding.shape[1], gtrend_encoding.shape[-1]).to(gtrend_encoding.device)\n            tgt[0] = static_feature_fusion\n            tgt = self.pos_encoder(tgt)\n            tgt_mask = self._generate_square_subsequent_mask(self.output_len).to(tgt.device)\n            memory = gtrend_encoding\n            \n            # ë‹¨ì¼ decoder layer ì‚¬ìš©\n            decoder_out, attn_weights = self.decoder_layer(tgt, memory, tgt_mask)\n            forecast = self.decoder_fc(decoder_out)\n        else:\n            # Decode (generatively/non-autoregressively)\n            tgt = static_feature_fusion.unsqueeze(0)\n            memory = gtrend_encoding\n            \n            # ë‹¨ì¼ decoder layer ì‚¬ìš©\n            decoder_out, attn_weights = self.decoder_layer(tgt, memory)\n            forecast = self.decoder_fc(decoder_out)\n\n        return forecast.view(-1, self.output_len), attn_weights\n\n    def configure_optimizers(self):\n        optimizer = Adafactor(self.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n        return [optimizer]\n\n    def training_step(self, train_batch, batch_idx):\n        item_sales, category, color, fabric, temporal_features, gtrends, images = train_batch \n        \n        # ì…ë ¥ í…ì„œë“¤ì˜ gradient í™œì„±í™”\n        temporal_features = temporal_features.requires_grad_(True)\n        gtrends = gtrends.requires_grad_(True)\n        images = images.requires_grad_(True)\n        \n        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n        loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, test_batch, batch_idx):\n        item_sales, category, color, fabric, temporal_features, gtrends, images = test_batch \n        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n        \n        if not hasattr(self, 'validation_step_outputs'):\n            self.validation_step_outputs = []\n        self.validation_step_outputs.append((item_sales.squeeze(), forecasted_sales.squeeze()))\n        \n        return item_sales.squeeze(), forecasted_sales.squeeze()\n\n    def on_validation_epoch_end(self):\n        if hasattr(self, 'validation_step_outputs'):\n            val_step_outputs = self.validation_step_outputs\n            item_sales, forecasted_sales = [x[0] for x in val_step_outputs], [x[1] for x in val_step_outputs]\n            item_sales, forecasted_sales = torch.stack(item_sales), torch.stack(forecasted_sales)\n            rescaled_item_sales, rescaled_forecasted_sales = item_sales*1065, forecasted_sales*1065\n            loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n            mae = F.l1_loss(rescaled_item_sales, rescaled_forecasted_sales)\n            self.log('val_mae', mae)\n            self.log('val_loss', loss)\n\n            print('Validation MAE:', mae.detach().cpu().numpy(), 'LR:', self.optimizers().param_groups[0]['lr'])\n            self.validation_step_outputs.clear()\n\nprint(\"âœ… GTM ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ (decoder layer ìˆ˜ì •)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ğŸ“Š ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (data_multitrends.py ë‚´ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotDataset():\n",
    "    def __init__(self, data_df, img_root, gtrends, cat_dict, col_dict, fab_dict, trend_len):\n",
    "        self.data_df = data_df\n",
    "        self.gtrends = gtrends\n",
    "        self.cat_dict = cat_dict\n",
    "        self.col_dict = col_dict\n",
    "        self.fab_dict = fab_dict\n",
    "        self.trend_len = trend_len\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_df.iloc[idx, :]\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        data = self.data_df\n",
    "\n",
    "        gtrends, image_features = [], []\n",
    "        img_transforms = Compose([Resize((256, 256)), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "        for (idx, row) in tqdm(data.iterrows(), total=len(data), ascii=True, desc=\"ë°ì´í„° ì „ì²˜ë¦¬\"):\n",
    "            cat, col, fab, fiq_attr, start_date, img_path = row['category'], row['color'], row['fabric'], row['extra'], \\\n",
    "                row['release_date'], row['image_path']\n",
    "\n",
    "            # Get the gtrend signal up to the previous year (52 weeks) of the release date\n",
    "            gtrend_start = start_date - pd.DateOffset(weeks=52)\n",
    "            cat_gtrend = self.gtrends.loc[gtrend_start:start_date][cat][-52:].values[:self.trend_len]\n",
    "            col_gtrend = self.gtrends.loc[gtrend_start:start_date][col][-52:].values[:self.trend_len]\n",
    "            fab_gtrend = self.gtrends.loc[gtrend_start:start_date][fab][-52:].values[:self.trend_len]\n",
    "\n",
    "            cat_gtrend = MinMaxScaler().fit_transform(cat_gtrend.reshape(-1,1)).flatten()\n",
    "            col_gtrend = MinMaxScaler().fit_transform(col_gtrend.reshape(-1,1)).flatten()\n",
    "            fab_gtrend = MinMaxScaler().fit_transform(fab_gtrend.reshape(-1,1)).flatten()\n",
    "            multitrends = np.vstack([cat_gtrend, col_gtrend, fab_gtrend])\n",
    "\n",
    "            # Read images\n",
    "            img = Image.open(os.path.join(self.img_root, img_path)).convert('RGB')\n",
    "\n",
    "            gtrends.append(multitrends)\n",
    "            image_features.append(img_transforms(img))\n",
    "\n",
    "        gtrends = np.array(gtrends)\n",
    "\n",
    "        # Remove non-numerical information\n",
    "        data = data.copy()  # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ë³´ì¡´\n",
    "        data.drop(['external_code', 'season', 'release_date', 'image_path'], axis=1, inplace=True)\n",
    "\n",
    "        # Create tensors for each part of the input/output\n",
    "        item_sales, temporal_features = torch.FloatTensor(data.iloc[:, :12].values), torch.FloatTensor(\n",
    "            data.iloc[:, 13:17].values)\n",
    "        categories, colors, fabrics = [self.cat_dict[val] for val in data.iloc[:].category.values], \\\n",
    "                                       [self.col_dict[val] for val in data.iloc[:].color.values], \\\n",
    "                                       [self.fab_dict[val] for val in data.iloc[:].fabric.values]\n",
    "\n",
    "        categories, colors, fabrics = torch.LongTensor(categories), torch.LongTensor(colors), torch.LongTensor(fabrics)\n",
    "        gtrends = torch.FloatTensor(gtrends)\n",
    "        images = torch.stack(image_features)\n",
    "\n",
    "        return TensorDataset(item_sales, categories, colors, fabrics, temporal_features, gtrends, images)\n",
    "\n",
    "    def get_loader(self, batch_size, train=True):\n",
    "        print('ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...')\n",
    "        data_with_gtrends = self.preprocess_data()\n",
    "        if train:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        else:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=1, shuffle=False, num_workers=2)\n",
    "        print('ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ.')\n",
    "        return data_loader\n",
    "\n",
    "print(\"âœ… ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ğŸ“‚ ë°ì´í„° ë¡œë”© ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
    "dataset_path = Path('/content/drive/MyDrive/GTM-dataset-small/')\n",
    "\n",
    "# í•„ìš”í•œ íŒŒì¼ë“¤ í™•ì¸\n",
    "required_files = ['train.csv', 'test.csv', 'gtrends.csv', 'category_labels.pt', 'color_labels.pt', 'fabric_labels.pt']\n",
    "print(\"ğŸ“‚ ë°ì´í„° íŒŒì¼ í™•ì¸:\")\n",
    "for file in required_files:\n",
    "    file_path = dataset_path / file\n",
    "    if file_path.exists():\n",
    "        size = file_path.stat().st_size\n",
    "        print(f\"  âœ… {file}: {size/1024:.1f} KB\")\n",
    "    else:\n",
    "        print(f\"  âŒ {file}: íŒŒì¼ ì—†ìŒ!\")\n",
    "\n",
    "# ì´ë¯¸ì§€ í´ë” í™•ì¸\n",
    "image_path = dataset_path / 'images'\n",
    "if image_path.exists():\n",
    "    total_images = 0\n",
    "    print(f\"ğŸ“ ì´ë¯¸ì§€ í´ë” êµ¬ì¡°:\")\n",
    "    for subdir in sorted(image_path.iterdir()):\n",
    "        if subdir.is_dir():\n",
    "            subdir_images = list(subdir.glob('*.png')) + list(subdir.glob('*.jpg'))\n",
    "            print(f\"  ğŸ“‚ {subdir.name}: {len(subdir_images)}ê°œ\")\n",
    "            total_images += len(subdir_images)\n",
    "    print(f\"  ğŸ–¼ï¸ ì´ ì´ë¯¸ì§€: {total_images}ê°œ\")\n",
    "else:\n",
    "    print(f\"  âŒ images/ í´ë” ì—†ìŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë”©\n",
    "print(\"ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "train_df = pd.read_csv(dataset_path / 'train.csv', parse_dates=['release_date'])\n",
    "test_df = pd.read_csv(dataset_path / 'test.csv', parse_dates=['release_date'])\n",
    "gtrends = pd.read_csv(dataset_path / 'gtrends.csv', index_col=[0], parse_dates=True)\n",
    "\n",
    "print(f\"  - í›ˆë ¨ ë°ì´í„°: {len(train_df):,}ê°œ\")\n",
    "print(f\"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ\")\n",
    "print(f\"  - Google Trends: {len(gtrends):,}ê°œ ì‹œì \")\n",
    "\n",
    "# ë¼ë²¨ ë”•ì…”ë„ˆë¦¬ ë¡œë”©\n",
    "print(\"ğŸ“‹ ë¼ë²¨ ë”•ì…”ë„ˆë¦¬ ë¡œë”© ì¤‘...\")\n",
    "cat_dict = torch.load(dataset_path / 'category_labels.pt', weights_only=False)\n",
    "col_dict = torch.load(dataset_path / 'color_labels.pt', weights_only=False)\n",
    "fab_dict = torch.load(dataset_path / 'fabric_labels.pt', weights_only=False)\n",
    "\n",
    "print(f\"  - ì¹´í…Œê³ ë¦¬: {len(cat_dict)}ê°œ\")\n",
    "print(f\"  - ìƒ‰ìƒ: {len(col_dict)}ê°œ\")\n",
    "print(f\"  - ì†Œì¬: {len(fab_dict)}ê°œ\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ë°ì´í„° ë¡œë”© ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ğŸ”§ ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "print(\"ğŸ”„ í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n",
    "train_dataset = ZeroShotDataset(\n",
    "    train_df, \n",
    "    dataset_path / 'images',\n",
    "    gtrends, \n",
    "    cat_dict, \n",
    "    col_dict, \n",
    "    fab_dict, \n",
    "    trend_len=52\n",
    ")\n",
    "\n",
    "print(\"ğŸ”„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n",
    "test_dataset = ZeroShotDataset(\n",
    "    test_df, \n",
    "    dataset_path / 'images',\n",
    "    gtrends, \n",
    "    cat_dict, \n",
    "    col_dict, \n",
    "    fab_dict, \n",
    "    trend_len=52\n",
    ")\n",
    "\n",
    "# DataLoader ìƒì„±\n",
    "BATCH_SIZE = 8 if torch.cuda.is_available() else 4\n",
    "\n",
    "print(f\"ğŸ”„ DataLoader ìƒì„± ì¤‘... (ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE})\")\n",
    "train_loader = train_dataset.get_loader(batch_size=BATCH_SIZE, train=True)\n",
    "test_loader = test_dataset.get_loader(batch_size=1, train=False)\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"  - í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}\")\n",
    "print(f\"  - í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìˆ˜: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ğŸ¤– GTM ëª¨ë¸ ìƒì„± ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTM ëª¨ë¸ ìƒì„±\n",
    "print(\"ğŸ¯ GTM ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
    "\n",
    "model = GTM(\n",
    "    embedding_dim=32,\n",
    "    hidden_dim=64,\n",
    "    output_dim=12,\n",
    "    num_heads=4,\n",
    "    num_layers=1,\n",
    "    use_text=True,\n",
    "    use_img=True,\n",
    "    cat_dict=cat_dict,\n",
    "    col_dict=col_dict,\n",
    "    fab_dict=fab_dict,\n",
    "    trend_len=52,\n",
    "    num_trends=3,\n",
    "    gpu_num=0,\n",
    "    use_encoder_mask=1,\n",
    "    autoregressive=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… GTM ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ í¬ì›Œë“œ íŒ¨ìŠ¤ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\nğŸ”¬ í¬ì›Œë“œ íŒ¨ìŠ¤ í…ŒìŠ¤íŠ¸...\")\n",
    "try:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    item_sales, category, color, fabric, temporal_features, gtrends_batch, images = sample_batch\n",
    "    \n",
    "    print(f\"  ì…ë ¥ shape:\")\n",
    "    print(f\"    - item_sales: {item_sales.shape}\")\n",
    "    print(f\"    - images: {images.shape}\")\n",
    "    print(f\"    - gtrends: {gtrends_batch.shape}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output, attn = model(category, color, fabric, temporal_features, gtrends_batch, images)\n",
    "        \n",
    "    print(f\"  ì¶œë ¥ shape: {output.shape}\")\n",
    "    print(f\"  âœ… í¬ì›Œë“œ íŒ¨ìŠ¤ ì„±ê³µ!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ í¬ì›Œë“œ íŒ¨ìŠ¤ ì‹¤íŒ¨: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. âš¡ PyTorch Lightning Trainer ì„¤ì • ë° í›ˆë ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer ì„¤ì •\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "EPOCHS = 5  # ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•´\n",
    "ACCELERATOR = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ ì½œë°±\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='./checkpoints/',\n",
    "    filename='gtm-all-in-one-{epoch:02d}-{val_mae:.2f}',\n",
    "    monitor='val_mae',\n",
    "    mode='min',\n",
    "    save_top_k=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# CSV ë¡œê±°\n",
    "csv_logger = CSVLogger(\n",
    "    save_dir='./logs/',\n",
    "    name='gtm_all_in_one'\n",
    ")\n",
    "\n",
    "# Trainer ìƒì„±\n",
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=ACCELERATOR,\n",
    "    max_epochs=EPOCHS,\n",
    "    check_val_every_n_epoch=1,\n",
    "    logger=csv_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=20\n",
    ")\n",
    "\n",
    "print(f\"ğŸš€ Trainer ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"  - ê°€ì†ê¸°: {ACCELERATOR}\")\n",
    "print(f\"  - ì—í¬í¬: {EPOCHS}\")\n",
    "print(f\"  - ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰\n",
    "print(\"ğŸš€ GTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    trainer.fit(\n",
    "        model, \n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=test_loader\n",
    "    )\n",
    "    \n",
    "    print(\"\\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ’¾ ìµœê³  ëª¨ë¸: {checkpoint_callback.best_model_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ í›ˆë ¨ ì‹¤íŒ¨: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ğŸ“Š ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í›ˆë ¨ ë©”íŠ¸ë¦­ ì‹œê°í™”\n",
    "log_dir = './logs/gtm_all_in_one/'\n",
    "version_dirs = [d for d in os.listdir(log_dir) if d.startswith('version_')]\n",
    "\n",
    "if version_dirs:\n",
    "    latest_version = max(version_dirs, key=lambda x: int(x.split('_')[1]))\n",
    "    metrics_path = os.path.join(log_dir, latest_version, 'metrics.csv')\n",
    "    \n",
    "    if os.path.exists(metrics_path):\n",
    "        metrics_df = pd.read_csv(metrics_path)\n",
    "        \n",
    "        # ë©”íŠ¸ë¦­ í”Œë¡¯\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss í”Œë¡¯\n",
    "        train_loss = metrics_df.dropna(subset=['train_loss'])\n",
    "        val_loss = metrics_df.dropna(subset=['val_loss'])\n",
    "        \n",
    "        if len(train_loss) > 0:\n",
    "            axes[0].plot(train_loss['step'], train_loss['train_loss'], label='Training Loss', alpha=0.7)\n",
    "        if len(val_loss) > 0:\n",
    "            axes[0].plot(val_loss['step'], val_loss['val_loss'], label='Validation Loss', marker='o')\n",
    "        axes[0].set_title('ğŸ“‰ Training/Validation Loss')\n",
    "        axes[0].set_xlabel('Steps')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # MAE í”Œë¡¯\n",
    "        val_mae = metrics_df.dropna(subset=['val_mae'])\n",
    "        if len(val_mae) > 0:\n",
    "            axes[1].plot(val_mae['step'], val_mae['val_mae'], label='Validation MAE', marker='s', color='red')\n",
    "            final_mae = val_mae['val_mae'].iloc[-1]\n",
    "            print(f\"ğŸ¯ ìµœì¢… Validation MAE: {final_mae:.2f}\")\n",
    "        \n",
    "        axes[1].set_title('ğŸ“Š Validation MAE')\n",
    "        axes[1].set_xlabel('Steps')\n",
    "        axes[1].set_ylabel('MAE')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"âš ï¸ ë©”íŠ¸ë¦­ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"âš ï¸ ë¡œê·¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ğŸ”® ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(test_loader))\n",
    "    item_sales, category, color, fabric, temporal_features, gtrends_batch, images = sample_batch\n",
    "    \n",
    "    # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    predictions, attention_weights = model(category, color, fabric, temporal_features, gtrends_batch, images)\n",
    "    \n",
    "    # ì •ê·œí™” í•´ì œ (1065ëŠ” ì •ê·œí™” íŒ©í„°)\n",
    "    actual_sales = item_sales * 1065\n",
    "    predicted_sales = predictions * 1065\n",
    "\n",
    "# ì˜ˆì¸¡ ì‹œê°í™”\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(4, len(predictions))):\n",
    "    actual = actual_sales[i].cpu().numpy()\n",
    "    predicted = predicted_sales[i].cpu().numpy()\n",
    "    \n",
    "    axes[i].plot(months, actual, label='ì‹¤ì œ ë§¤ì¶œ', marker='o', linewidth=2)\n",
    "    axes[i].plot(months, predicted, label='ì˜ˆì¸¡ ë§¤ì¶œ', marker='s', linewidth=2, alpha=0.8)\n",
    "    axes[i].set_title(f'All-in-One ì˜ˆì¸¡ ê²°ê³¼ {i+1}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    mae = np.mean(np.abs(actual - predicted))\n",
    "    axes[i].text(0.02, 0.98, f'MAE: {mae:.1f}', transform=axes[i].transAxes, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7),\n",
    "                verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ì „ì²´ ì„±ëŠ¥\n",
    "overall_mae = np.mean(np.abs(actual_sales.cpu().numpy() - predicted_sales.cpu().numpy()))\n",
    "print(f\"ğŸ”® All-in-One ì „ì²´ ì˜ˆì¸¡ MAE: {overall_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ğŸ’¾ ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveì— ê²°ê³¼ ì €ì¥\n",
    "drive_save_path = '/content/drive/MyDrive/GTM-Results-All-In-One/'\n",
    "os.makedirs(drive_save_path, exist_ok=True)\n",
    "\n",
    "# ìµœê³  ëª¨ë¸ì„ Google Driveì— ë³µì‚¬\n",
    "if checkpoint_callback.best_model_path:\n",
    "    import shutil\n",
    "    best_model_name = f\"gtm_all_in_one_best_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.ckpt\"\n",
    "    shutil.copy2(checkpoint_callback.best_model_path, drive_save_path + best_model_name)\n",
    "    print(f\"ğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥: {drive_save_path + best_model_name}\")\n",
    "\n",
    "# ë©”íŠ¸ë¦­ CSVë„ ì €ì¥\n",
    "if 'metrics_path' in locals() and os.path.exists(metrics_path):\n",
    "    shutil.copy2(metrics_path, drive_save_path + 'training_metrics.csv')\n",
    "    print(f\"ğŸ“Š í›ˆë ¨ ë©”íŠ¸ë¦­ ì €ì¥: {drive_save_path}training_metrics.csv\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ê²°ê³¼ê°€ Google Driveì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {drive_save_path}\")\n",
    "\n",
    "print(\"\\nğŸ‰ All-in-One GTM ëª¨ë¸ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(\"ğŸ“ ì´ ë…¸íŠ¸ë¶ì€ ì™¸ë¶€ íŒŒì¼ ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ì‚¬ìš© ê°€ì´ë“œ\n",
    "\n",
    "### âœ… ì‹¤í–‰ ì „ ì¤€ë¹„ì‚¬í•­\n",
    "1. Google Driveì— `GTM-dataset-small` í´ë” ì—…ë¡œë“œ\n",
    "2. GPU ëŸ°íƒ€ì„ ì„¤ì • (ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ GPU)\n",
    "\n",
    "### ğŸš€ ì‹¤í–‰ ë°©ë²•\n",
    "1. ëª¨ë“  ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n",
    "2. ì²« ë²ˆì§¸ ì…€ì—ì„œ Google Drive ë§ˆìš´íŠ¸ í—ˆìš©\n",
    "3. ìë™ìœ¼ë¡œ ëª¨ë“  ê³¼ì • ì§„í–‰\n",
    "\n",
    "### ğŸ“Š ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„\n",
    "- ë°ì´í„° ë¡œë”©: ~2-3ë¶„\n",
    "- ëª¨ë¸ í›ˆë ¨ (5 ì—í¬í¬): ~5-10ë¶„\n",
    "- ì „ì²´: ~10-15ë¶„\n",
    "\n",
    "### ğŸ’¡ íŠ¹ì§•\n",
    "- âœ… ì™¸ë¶€ .py íŒŒì¼ ë¶ˆí•„ìš”\n",
    "- âœ… ëª¨ë“  ì½”ë“œê°€ ë…¸íŠ¸ë¶ ë‚´ í¬í•¨\n",
    "- âœ… Gradient ë¬¸ì œ í•´ê²°ë¨\n",
    "- âœ… TransformerDecoder í˜¸í™˜ì„± í•´ê²°\n",
    "- âœ… ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹ ë¥¸ ì‹¤í—˜\n",
    "\n",
    "### ğŸ”— GitHub ì €ì¥ì†Œ\n",
    "https://github.com/LeeSaeBom/GTM-Transformer-Jupyter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}