{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 GTM (Google Trends Transformer) - All-in-One Version\n",
    "\n",
    "이 노트북은 모든 코드가 포함된 독립실행형 버전입니다.\n",
    "- 외부 py 파일 없이 노트북 내에서 모든 클래스와 함수 정의\n",
    "- Google Drive의 작은 데이터셋만 필요\n",
    "- 빠른 테스트 및 실험 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 📦 패키지 설치 및 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Colab 환경에서 안정적인 패키지 설치\nimport subprocess\nimport sys\n\ndef install_package(package):\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n        print(f\"✅ {package} 설치 완료\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"❌ {package} 설치 실패: {e}\")\n        return False\n\n# tokenizers 문제 해결 - 미리 컴파일된 버전 사용\nprint(\"🔧 tokenizers 설치 중...\")\n!pip install tokenizers --no-build-isolation --quiet\n\n# transformers 호환 버전 설치\nprint(\"🔧 transformers 설치 중...\")\n!pip install transformers==4.21.0 --quiet\n\n# PyTorch Lightning 안정 버전\nprint(\"🔧 PyTorch Lightning 설치 중...\")\n!pip install pytorch-lightning==1.9.5 --quiet\n\n# 기타 패키지\nprint(\"🔧 기타 패키지 설치 중...\")\n!pip install scikit-learn pillow --quiet\n\nprint(\"\\n📦 패키지 설치 완료! import 시작...\")\n\n# 모든 import\nimport math\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image, ImageFile\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision.transforms import Resize, ToTensor, Normalize, Compose\nfrom torchvision import models\nfrom sklearn.preprocessing import MinMaxScaler\nfrom transformers import pipeline, Adafactor\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n# Google Drive 마운트\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nprint(f\"✅ PyTorch: {torch.__version__}\")\nprint(f\"✅ PyTorch Lightning: {pl.__version__}\")\nprint(f\"✅ CUDA 사용 가능: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 🧠 모델 클래스 정의 (GTM.py 내용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 기본 모듈들 정의\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=52):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass TimeDistributed(nn.Module):\n    def __init__(self, module, batch_first=True):\n        super(TimeDistributed, self).__init__()\n        self.module = module\n        self.batch_first = batch_first\n\n    def forward(self, x):\n        if len(x.size()) <= 2:\n            return self.module(x)\n\n        x_reshape = x.contiguous().view(-1, x.size(-1))  \n        y = self.module(x_reshape)\n\n        if self.batch_first:\n            y = y.contiguous().view(x.size(0), -1, y.size(-1))\n        else:\n            y = y.view(-1, x.size(1), y.size(-1))\n\n        return y\n\nprint(\"✅ 기본 모듈 정의 완료\")"
  },
  {
   "cell_type": "code",
   "source": "# 인코더 클래스들 정의\nclass FusionNetwork(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, use_img, use_text, dropout=0.2):\n        super(FusionNetwork, self).__init__()\n        \n        self.img_pool = nn.AdaptiveAvgPool2d((1,1))\n        self.img_linear = nn.Linear(2048, embedding_dim)\n        self.use_img = use_img\n        self.use_text = use_text\n        input_dim = embedding_dim + (embedding_dim*use_img) + (embedding_dim*use_text)\n        self.feature_fusion = nn.Sequential(\n            nn.BatchNorm1d(input_dim),\n            nn.Linear(input_dim, input_dim, bias=False),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(input_dim, hidden_dim)\n        )\n\n    def forward(self, img_encoding, text_encoding, dummy_encoding):\n        pooled_img = self.img_pool(img_encoding)\n        condensed_img = self.img_linear(pooled_img.flatten(1))\n\n        decoder_inputs = []\n        if self.use_img == 1:\n            decoder_inputs.append(condensed_img) \n        if self.use_text == 1:\n            decoder_inputs.append(text_encoding) \n        decoder_inputs.append(dummy_encoding)\n        concat_features = torch.cat(decoder_inputs, dim=1)\n\n        final = self.feature_fusion(concat_features)\n        return final\n\nclass GTrendEmbedder(nn.Module):\n    def __init__(self, forecast_horizon, embedding_dim, use_mask, trend_len, num_trends, gpu_num):\n        super().__init__()\n        self.forecast_horizon = forecast_horizon\n        self.input_linear = TimeDistributed(nn.Linear(num_trends, embedding_dim))\n        self.pos_embedding = PositionalEncoding(embedding_dim, max_len=trend_len)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dropout=0.2)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n        self.use_mask = use_mask\n        self.gpu_num = gpu_num\n\n    def _generate_encoder_mask(self, size, forecast_horizon):\n        mask = torch.zeros((size, size))\n        split = math.gcd(size, forecast_horizon)\n        for i in range(0, size, split):\n            mask[i:i+split, i:i+split] = 1\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def forward(self, gtrends):\n        gtrend_emb = self.input_linear(gtrends.permute(0,2,1))\n        gtrend_emb = self.pos_embedding(gtrend_emb.permute(1,0,2))\n        input_mask = self._generate_encoder_mask(gtrend_emb.shape[0], self.forecast_horizon).to(gtrend_emb.device)\n        if self.use_mask == 1:\n            gtrend_emb = self.encoder(gtrend_emb, input_mask)\n        else:\n            gtrend_emb = self.encoder(gtrend_emb)\n        return gtrend_emb\n\nclass TextEmbedder(nn.Module):\n    def __init__(self, embedding_dim, cat_dict, col_dict, fab_dict, gpu_num):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.cat_dict = {v: k for k, v in cat_dict.items()}\n        self.col_dict = {v: k for k, v in col_dict.items()}\n        self.fab_dict = {v: k for k, v in fab_dict.items()}\n        self.word_embedder = pipeline('feature-extraction', model='bert-base-uncased')\n        self.fc = nn.Linear(768, embedding_dim)\n        self.dropout = nn.Dropout(0.1)\n        self.gpu_num = gpu_num\n\n    def forward(self, category, color, fabric):\n        textual_description = [self.col_dict[color.detach().cpu().numpy().tolist()[i]] + ' ' \\\n                + self.fab_dict[fabric.detach().cpu().numpy().tolist()[i]] + ' ' \\\n                + self.cat_dict[category.detach().cpu().numpy().tolist()[i]] for i in range(len(category))]\n\n        word_embeddings = self.word_embedder(textual_description)\n        word_embeddings = [torch.tensor(x[0][1:-1], dtype=torch.float32).mean(axis=0) if len(x[0]) > 2 else torch.tensor(x[0], dtype=torch.float32).mean(axis=0) for x in word_embeddings] \n        word_embeddings = torch.stack(word_embeddings).to(self.fc.weight.device).requires_grad_()\n        \n        word_embeddings = self.dropout(self.fc(word_embeddings))\n        return word_embeddings\n\nclass ImageEmbedder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        resnet = models.resnet50(pretrained=True)\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        \n        # 모든 ResNet 파라미터를 trainable로 설정 (gradient 문제 해결)\n        for p in self.resnet.parameters():\n            p.requires_grad = True\n        \n    def forward(self, images):        \n        img_embeddings = self.resnet(images)  \n        size = img_embeddings.size()\n        out = img_embeddings.view(*size[:2],-1)\n        return out.view(*size).contiguous()\n\nclass DummyEmbedder(nn.Module):\n    def __init__(self, embedding_dim):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.day_embedding = nn.Linear(1, embedding_dim)\n        self.week_embedding = nn.Linear(1, embedding_dim)\n        self.month_embedding = nn.Linear(1, embedding_dim)\n        self.year_embedding = nn.Linear(1, embedding_dim)\n        self.dummy_fusion = nn.Linear(embedding_dim*4, embedding_dim)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, temporal_features):\n        d, w, m, y = temporal_features[:, 0].unsqueeze(1), temporal_features[:, 1].unsqueeze(1), \\\n            temporal_features[:, 2].unsqueeze(1), temporal_features[:, 3].unsqueeze(1)\n        d_emb, w_emb, m_emb, y_emb = self.day_embedding(d), self.week_embedding(w), self.month_embedding(m), self.year_embedding(y)\n        temporal_embeddings = self.dummy_fusion(torch.cat([d_emb, w_emb, m_emb, y_emb], dim=1))\n        temporal_embeddings = self.dropout(temporal_embeddings)\n        return temporal_embeddings\n\nclass TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n        super(TransformerDecoderLayer, self).__init__()\n        \n        # Add self_attn for compatibility with nn.TransformerDecoder\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = F.relu\n\n    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, \n            memory_key_padding_mask=None, tgt_is_causal=None, memory_is_causal=None):\n        \n        # Self-attention block\n        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n                              key_padding_mask=tgt_key_padding_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        \n        # Cross-attention block\n        tgt2, attn_weights = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n                                                  key_padding_mask=memory_key_padding_mask)\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        \n        # Feedforward block\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        \n        return tgt, attn_weights\n\nprint(\"✅ 모든 모델 컴포넌트 정의 완료 (PyTorch 2.x 호환, gradient 문제 해결)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 🎯 GTM 메인 모델 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class GTM(pl.LightningModule):\n    def __init__(self, embedding_dim, hidden_dim, output_dim, num_heads, num_layers, use_text, use_img, \\\n                cat_dict, col_dict, fab_dict, trend_len, num_trends, gpu_num, use_encoder_mask=1, autoregressive=False):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n        self.output_len = output_dim\n        self.use_encoder_mask = use_encoder_mask\n        self.autoregressive = autoregressive\n        self.gpu_num = gpu_num\n        self.save_hyperparameters()\n\n        # Encoder\n        self.dummy_encoder = DummyEmbedder(embedding_dim)\n        self.image_encoder = ImageEmbedder()\n        self.text_encoder = TextEmbedder(embedding_dim, cat_dict, col_dict, fab_dict, gpu_num)\n        self.gtrend_encoder = GTrendEmbedder(output_dim, hidden_dim, use_encoder_mask, trend_len, num_trends, gpu_num)\n        self.static_feature_encoder = FusionNetwork(embedding_dim, hidden_dim, use_img, use_text)\n\n        # Decoder - 단일 레이어만 사용\n        self.decoder_layer = TransformerDecoderLayer(d_model=self.hidden_dim, nhead=num_heads, \n                                                    dim_feedforward=self.hidden_dim * 4, dropout=0.1)\n        \n        if self.autoregressive: \n            self.pos_encoder = PositionalEncoding(hidden_dim, max_len=12)\n        \n        self.decoder_fc = nn.Sequential(\n            nn.Linear(hidden_dim, self.output_len if not self.autoregressive else 1),\n            nn.Dropout(0.2)\n        )\n        \n    def _generate_square_subsequent_mask(self, size):\n        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def forward(self, category, color, fabric, temporal_features, gtrends, images):\n        # Encode features and get inputs\n        img_encoding = self.image_encoder(images)\n        dummy_encoding = self.dummy_encoder(temporal_features)\n        text_encoding = self.text_encoder(category, color, fabric)\n        gtrend_encoding = self.gtrend_encoder(gtrends)\n\n        # Fuse static features together\n        static_feature_fusion = self.static_feature_encoder(img_encoding, text_encoding, dummy_encoding)\n\n        if self.autoregressive == 1:\n            # Decode\n            tgt = torch.zeros(self.output_len, gtrend_encoding.shape[1], gtrend_encoding.shape[-1]).to(gtrend_encoding.device)\n            tgt[0] = static_feature_fusion\n            tgt = self.pos_encoder(tgt)\n            tgt_mask = self._generate_square_subsequent_mask(self.output_len).to(tgt.device)\n            memory = gtrend_encoding\n            \n            # 단일 decoder layer 사용\n            decoder_out, attn_weights = self.decoder_layer(tgt, memory, tgt_mask)\n            forecast = self.decoder_fc(decoder_out)\n        else:\n            # Decode (generatively/non-autoregressively)\n            tgt = static_feature_fusion.unsqueeze(0)\n            memory = gtrend_encoding\n            \n            # 단일 decoder layer 사용\n            decoder_out, attn_weights = self.decoder_layer(tgt, memory)\n            forecast = self.decoder_fc(decoder_out)\n\n        return forecast.view(-1, self.output_len), attn_weights\n\n    def configure_optimizers(self):\n        optimizer = Adafactor(self.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n        return [optimizer]\n\n    def training_step(self, train_batch, batch_idx):\n        item_sales, category, color, fabric, temporal_features, gtrends, images = train_batch \n        \n        # 입력 텐서들의 gradient 활성화\n        temporal_features = temporal_features.requires_grad_(True)\n        gtrends = gtrends.requires_grad_(True)\n        images = images.requires_grad_(True)\n        \n        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n        loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, test_batch, batch_idx):\n        item_sales, category, color, fabric, temporal_features, gtrends, images = test_batch \n        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n        \n        if not hasattr(self, 'validation_step_outputs'):\n            self.validation_step_outputs = []\n        self.validation_step_outputs.append((item_sales.squeeze(), forecasted_sales.squeeze()))\n        \n        return item_sales.squeeze(), forecasted_sales.squeeze()\n\n    def on_validation_epoch_end(self):\n        if hasattr(self, 'validation_step_outputs'):\n            val_step_outputs = self.validation_step_outputs\n            item_sales, forecasted_sales = [x[0] for x in val_step_outputs], [x[1] for x in val_step_outputs]\n            item_sales, forecasted_sales = torch.stack(item_sales), torch.stack(forecasted_sales)\n            rescaled_item_sales, rescaled_forecasted_sales = item_sales*1065, forecasted_sales*1065\n            loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n            mae = F.l1_loss(rescaled_item_sales, rescaled_forecasted_sales)\n            self.log('val_mae', mae)\n            self.log('val_loss', loss)\n\n            print('Validation MAE:', mae.detach().cpu().numpy(), 'LR:', self.optimizers().param_groups[0]['lr'])\n            self.validation_step_outputs.clear()\n\nprint(\"✅ GTM 모델 클래스 정의 완료 (decoder layer 수정)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 📊 데이터셋 클래스 (data_multitrends.py 내용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotDataset():\n",
    "    def __init__(self, data_df, img_root, gtrends, cat_dict, col_dict, fab_dict, trend_len):\n",
    "        self.data_df = data_df\n",
    "        self.gtrends = gtrends\n",
    "        self.cat_dict = cat_dict\n",
    "        self.col_dict = col_dict\n",
    "        self.fab_dict = fab_dict\n",
    "        self.trend_len = trend_len\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_df.iloc[idx, :]\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        data = self.data_df\n",
    "\n",
    "        gtrends, image_features = [], []\n",
    "        img_transforms = Compose([Resize((256, 256)), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "        for (idx, row) in tqdm(data.iterrows(), total=len(data), ascii=True, desc=\"데이터 전처리\"):\n",
    "            cat, col, fab, fiq_attr, start_date, img_path = row['category'], row['color'], row['fabric'], row['extra'], \\\n",
    "                row['release_date'], row['image_path']\n",
    "\n",
    "            # Get the gtrend signal up to the previous year (52 weeks) of the release date\n",
    "            gtrend_start = start_date - pd.DateOffset(weeks=52)\n",
    "            cat_gtrend = self.gtrends.loc[gtrend_start:start_date][cat][-52:].values[:self.trend_len]\n",
    "            col_gtrend = self.gtrends.loc[gtrend_start:start_date][col][-52:].values[:self.trend_len]\n",
    "            fab_gtrend = self.gtrends.loc[gtrend_start:start_date][fab][-52:].values[:self.trend_len]\n",
    "\n",
    "            cat_gtrend = MinMaxScaler().fit_transform(cat_gtrend.reshape(-1,1)).flatten()\n",
    "            col_gtrend = MinMaxScaler().fit_transform(col_gtrend.reshape(-1,1)).flatten()\n",
    "            fab_gtrend = MinMaxScaler().fit_transform(fab_gtrend.reshape(-1,1)).flatten()\n",
    "            multitrends = np.vstack([cat_gtrend, col_gtrend, fab_gtrend])\n",
    "\n",
    "            # Read images\n",
    "            img = Image.open(os.path.join(self.img_root, img_path)).convert('RGB')\n",
    "\n",
    "            gtrends.append(multitrends)\n",
    "            image_features.append(img_transforms(img))\n",
    "\n",
    "        gtrends = np.array(gtrends)\n",
    "\n",
    "        # Remove non-numerical information\n",
    "        data = data.copy()  # 원본 데이터프레임 보존\n",
    "        data.drop(['external_code', 'season', 'release_date', 'image_path'], axis=1, inplace=True)\n",
    "\n",
    "        # Create tensors for each part of the input/output\n",
    "        item_sales, temporal_features = torch.FloatTensor(data.iloc[:, :12].values), torch.FloatTensor(\n",
    "            data.iloc[:, 13:17].values)\n",
    "        categories, colors, fabrics = [self.cat_dict[val] for val in data.iloc[:].category.values], \\\n",
    "                                       [self.col_dict[val] for val in data.iloc[:].color.values], \\\n",
    "                                       [self.fab_dict[val] for val in data.iloc[:].fabric.values]\n",
    "\n",
    "        categories, colors, fabrics = torch.LongTensor(categories), torch.LongTensor(colors), torch.LongTensor(fabrics)\n",
    "        gtrends = torch.FloatTensor(gtrends)\n",
    "        images = torch.stack(image_features)\n",
    "\n",
    "        return TensorDataset(item_sales, categories, colors, fabrics, temporal_features, gtrends, images)\n",
    "\n",
    "    def get_loader(self, batch_size, train=True):\n",
    "        print('데이터셋 생성 시작...')\n",
    "        data_with_gtrends = self.preprocess_data()\n",
    "        if train:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        else:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=1, shuffle=False, num_workers=2)\n",
    "        print('데이터셋 생성 완료.')\n",
    "        return data_loader\n",
    "\n",
    "print(\"✅ 데이터셋 클래스 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 📂 데이터 로딩 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로 설정\n",
    "dataset_path = Path('/content/drive/MyDrive/GTM-dataset-small/')\n",
    "\n",
    "# 필요한 파일들 확인\n",
    "required_files = ['train.csv', 'test.csv', 'gtrends.csv', 'category_labels.pt', 'color_labels.pt', 'fabric_labels.pt']\n",
    "print(\"📂 데이터 파일 확인:\")\n",
    "for file in required_files:\n",
    "    file_path = dataset_path / file\n",
    "    if file_path.exists():\n",
    "        size = file_path.stat().st_size\n",
    "        print(f\"  ✅ {file}: {size/1024:.1f} KB\")\n",
    "    else:\n",
    "        print(f\"  ❌ {file}: 파일 없음!\")\n",
    "\n",
    "# 이미지 폴더 확인\n",
    "image_path = dataset_path / 'images'\n",
    "if image_path.exists():\n",
    "    total_images = 0\n",
    "    print(f\"📁 이미지 폴더 구조:\")\n",
    "    for subdir in sorted(image_path.iterdir()):\n",
    "        if subdir.is_dir():\n",
    "            subdir_images = list(subdir.glob('*.png')) + list(subdir.glob('*.jpg'))\n",
    "            print(f\"  📂 {subdir.name}: {len(subdir_images)}개\")\n",
    "            total_images += len(subdir_images)\n",
    "    print(f\"  🖼️ 총 이미지: {total_images}개\")\n",
    "else:\n",
    "    print(f\"  ❌ images/ 폴더 없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "print(\"📊 데이터 로딩 중...\")\n",
    "train_df = pd.read_csv(dataset_path / 'train.csv', parse_dates=['release_date'])\n",
    "test_df = pd.read_csv(dataset_path / 'test.csv', parse_dates=['release_date'])\n",
    "gtrends = pd.read_csv(dataset_path / 'gtrends.csv', index_col=[0], parse_dates=True)\n",
    "\n",
    "print(f\"  - 훈련 데이터: {len(train_df):,}개\")\n",
    "print(f\"  - 테스트 데이터: {len(test_df):,}개\")\n",
    "print(f\"  - Google Trends: {len(gtrends):,}개 시점\")\n",
    "\n",
    "# 라벨 딕셔너리 로딩\n",
    "print(\"📋 라벨 딕셔너리 로딩 중...\")\n",
    "cat_dict = torch.load(dataset_path / 'category_labels.pt', weights_only=False)\n",
    "col_dict = torch.load(dataset_path / 'color_labels.pt', weights_only=False)\n",
    "fab_dict = torch.load(dataset_path / 'fabric_labels.pt', weights_only=False)\n",
    "\n",
    "print(f\"  - 카테고리: {len(cat_dict)}개\")\n",
    "print(f\"  - 색상: {len(col_dict)}개\")\n",
    "print(f\"  - 소재: {len(fab_dict)}개\")\n",
    "\n",
    "print(\"\\n✅ 모든 데이터 로딩 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 🔧 데이터셋 및 DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "print(\"🔄 훈련 데이터셋 생성 중...\")\n",
    "train_dataset = ZeroShotDataset(\n",
    "    train_df, \n",
    "    dataset_path / 'images',\n",
    "    gtrends, \n",
    "    cat_dict, \n",
    "    col_dict, \n",
    "    fab_dict, \n",
    "    trend_len=52\n",
    ")\n",
    "\n",
    "print(\"🔄 테스트 데이터셋 생성 중...\")\n",
    "test_dataset = ZeroShotDataset(\n",
    "    test_df, \n",
    "    dataset_path / 'images',\n",
    "    gtrends, \n",
    "    cat_dict, \n",
    "    col_dict, \n",
    "    fab_dict, \n",
    "    trend_len=52\n",
    ")\n",
    "\n",
    "# DataLoader 생성\n",
    "BATCH_SIZE = 8 if torch.cuda.is_available() else 4\n",
    "\n",
    "print(f\"🔄 DataLoader 생성 중... (배치 크기: {BATCH_SIZE})\")\n",
    "train_loader = train_dataset.get_loader(batch_size=BATCH_SIZE, train=True)\n",
    "test_loader = test_dataset.get_loader(batch_size=1, train=False)\n",
    "\n",
    "print(f\"✅ 데이터 준비 완료!\")\n",
    "print(f\"  - 훈련 배치 수: {len(train_loader)}\")\n",
    "print(f\"  - 테스트 배치 수: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 🤖 GTM 모델 생성 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTM 모델 생성\n",
    "print(\"🎯 GTM 모델 생성 중...\")\n",
    "\n",
    "model = GTM(\n",
    "    embedding_dim=32,\n",
    "    hidden_dim=64,\n",
    "    output_dim=12,\n",
    "    num_heads=4,\n",
    "    num_layers=1,\n",
    "    use_text=True,\n",
    "    use_img=True,\n",
    "    cat_dict=cat_dict,\n",
    "    col_dict=col_dict,\n",
    "    fab_dict=fab_dict,\n",
    "    trend_len=52,\n",
    "    num_trends=3,\n",
    "    gpu_num=0,\n",
    "    use_encoder_mask=1,\n",
    "    autoregressive=False\n",
    ")\n",
    "\n",
    "print(f\"✅ GTM 모델 생성 완료!\")\n",
    "print(f\"📊 모델 파라미터: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 첫 번째 배치로 포워드 패스 테스트\n",
    "print(\"\\n🔬 포워드 패스 테스트...\")\n",
    "try:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    item_sales, category, color, fabric, temporal_features, gtrends_batch, images = sample_batch\n",
    "    \n",
    "    print(f\"  입력 shape:\")\n",
    "    print(f\"    - item_sales: {item_sales.shape}\")\n",
    "    print(f\"    - images: {images.shape}\")\n",
    "    print(f\"    - gtrends: {gtrends_batch.shape}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output, attn = model(category, color, fabric, temporal_features, gtrends_batch, images)\n",
    "        \n",
    "    print(f\"  출력 shape: {output.shape}\")\n",
    "    print(f\"  ✅ 포워드 패스 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 포워드 패스 실패: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ⚡ PyTorch Lightning Trainer 설정 및 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 설정\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "EPOCHS = 5  # 빠른 실험을 위해\n",
    "ACCELERATOR = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 체크포인트 콜백\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='./checkpoints/',\n",
    "    filename='gtm-all-in-one-{epoch:02d}-{val_mae:.2f}',\n",
    "    monitor='val_mae',\n",
    "    mode='min',\n",
    "    save_top_k=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# CSV 로거\n",
    "csv_logger = CSVLogger(\n",
    "    save_dir='./logs/',\n",
    "    name='gtm_all_in_one'\n",
    ")\n",
    "\n",
    "# Trainer 생성\n",
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=ACCELERATOR,\n",
    "    max_epochs=EPOCHS,\n",
    "    check_val_every_n_epoch=1,\n",
    "    logger=csv_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=20\n",
    ")\n",
    "\n",
    "print(f\"🚀 Trainer 설정 완료!\")\n",
    "print(f\"  - 가속기: {ACCELERATOR}\")\n",
    "print(f\"  - 에포크: {EPOCHS}\")\n",
    "print(f\"  - 배치 크기: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 실행\n",
    "print(\"🚀 GTM 모델 훈련 시작!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    trainer.fit(\n",
    "        model, \n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=test_loader\n",
    "    )\n",
    "    \n",
    "    print(\"\\n🎉 훈련 완료!\")\n",
    "    print(f\"💾 최고 모델: {checkpoint_callback.best_model_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 훈련 실패: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 📊 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 훈련 메트릭 시각화\n",
    "log_dir = './logs/gtm_all_in_one/'\n",
    "version_dirs = [d for d in os.listdir(log_dir) if d.startswith('version_')]\n",
    "\n",
    "if version_dirs:\n",
    "    latest_version = max(version_dirs, key=lambda x: int(x.split('_')[1]))\n",
    "    metrics_path = os.path.join(log_dir, latest_version, 'metrics.csv')\n",
    "    \n",
    "    if os.path.exists(metrics_path):\n",
    "        metrics_df = pd.read_csv(metrics_path)\n",
    "        \n",
    "        # 메트릭 플롯\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss 플롯\n",
    "        train_loss = metrics_df.dropna(subset=['train_loss'])\n",
    "        val_loss = metrics_df.dropna(subset=['val_loss'])\n",
    "        \n",
    "        if len(train_loss) > 0:\n",
    "            axes[0].plot(train_loss['step'], train_loss['train_loss'], label='Training Loss', alpha=0.7)\n",
    "        if len(val_loss) > 0:\n",
    "            axes[0].plot(val_loss['step'], val_loss['val_loss'], label='Validation Loss', marker='o')\n",
    "        axes[0].set_title('📉 Training/Validation Loss')\n",
    "        axes[0].set_xlabel('Steps')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # MAE 플롯\n",
    "        val_mae = metrics_df.dropna(subset=['val_mae'])\n",
    "        if len(val_mae) > 0:\n",
    "            axes[1].plot(val_mae['step'], val_mae['val_mae'], label='Validation MAE', marker='s', color='red')\n",
    "            final_mae = val_mae['val_mae'].iloc[-1]\n",
    "            print(f\"🎯 최종 Validation MAE: {final_mae:.2f}\")\n",
    "        \n",
    "        axes[1].set_title('📊 Validation MAE')\n",
    "        axes[1].set_xlabel('Steps')\n",
    "        axes[1].set_ylabel('MAE')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ 메트릭 파일을 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(\"⚠️ 로그 디렉토리를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 🔮 예측 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측 테스트\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(test_loader))\n",
    "    item_sales, category, color, fabric, temporal_features, gtrends_batch, images = sample_batch\n",
    "    \n",
    "    # 예측 수행\n",
    "    predictions, attention_weights = model(category, color, fabric, temporal_features, gtrends_batch, images)\n",
    "    \n",
    "    # 정규화 해제 (1065는 정규화 팩터)\n",
    "    actual_sales = item_sales * 1065\n",
    "    predicted_sales = predictions * 1065\n",
    "\n",
    "# 예측 시각화\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(4, len(predictions))):\n",
    "    actual = actual_sales[i].cpu().numpy()\n",
    "    predicted = predicted_sales[i].cpu().numpy()\n",
    "    \n",
    "    axes[i].plot(months, actual, label='실제 매출', marker='o', linewidth=2)\n",
    "    axes[i].plot(months, predicted, label='예측 매출', marker='s', linewidth=2, alpha=0.8)\n",
    "    axes[i].set_title(f'All-in-One 예측 결과 {i+1}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    mae = np.mean(np.abs(actual - predicted))\n",
    "    axes[i].text(0.02, 0.98, f'MAE: {mae:.1f}', transform=axes[i].transAxes, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7),\n",
    "                verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 전체 성능\n",
    "overall_mae = np.mean(np.abs(actual_sales.cpu().numpy() - predicted_sales.cpu().numpy()))\n",
    "print(f\"🔮 All-in-One 전체 예측 MAE: {overall_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 💾 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive에 결과 저장\n",
    "drive_save_path = '/content/drive/MyDrive/GTM-Results-All-In-One/'\n",
    "os.makedirs(drive_save_path, exist_ok=True)\n",
    "\n",
    "# 최고 모델을 Google Drive에 복사\n",
    "if checkpoint_callback.best_model_path:\n",
    "    import shutil\n",
    "    best_model_name = f\"gtm_all_in_one_best_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.ckpt\"\n",
    "    shutil.copy2(checkpoint_callback.best_model_path, drive_save_path + best_model_name)\n",
    "    print(f\"💾 최고 모델 저장: {drive_save_path + best_model_name}\")\n",
    "\n",
    "# 메트릭 CSV도 저장\n",
    "if 'metrics_path' in locals() and os.path.exists(metrics_path):\n",
    "    shutil.copy2(metrics_path, drive_save_path + 'training_metrics.csv')\n",
    "    print(f\"📊 훈련 메트릭 저장: {drive_save_path}training_metrics.csv\")\n",
    "\n",
    "print(\"\\n✅ 모든 결과가 Google Drive에 저장되었습니다!\")\n",
    "print(f\"📂 저장 위치: {drive_save_path}\")\n",
    "\n",
    "print(\"\\n🎉 All-in-One GTM 모델 훈련 및 테스트 완료!\")\n",
    "print(\"📝 이 노트북은 외부 파일 없이 독립적으로 실행됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 사용 가이드\n",
    "\n",
    "### ✅ 실행 전 준비사항\n",
    "1. Google Drive에 `GTM-dataset-small` 폴더 업로드\n",
    "2. GPU 런타임 설정 (런타임 → 런타임 유형 변경 → GPU)\n",
    "\n",
    "### 🚀 실행 방법\n",
    "1. 모든 셀을 순서대로 실행\n",
    "2. 첫 번째 셀에서 Google Drive 마운트 허용\n",
    "3. 자동으로 모든 과정 진행\n",
    "\n",
    "### 📊 예상 실행 시간\n",
    "- 데이터 로딩: ~2-3분\n",
    "- 모델 훈련 (5 에포크): ~5-10분\n",
    "- 전체: ~10-15분\n",
    "\n",
    "### 💡 특징\n",
    "- ✅ 외부 .py 파일 불필요\n",
    "- ✅ 모든 코드가 노트북 내 포함\n",
    "- ✅ Gradient 문제 해결됨\n",
    "- ✅ TransformerDecoder 호환성 해결\n",
    "- ✅ 작은 데이터셋으로 빠른 실험\n",
    "\n",
    "### 🔗 GitHub 저장소\n",
    "https://github.com/LeeSaeBom/GTM-Transformer-Jupyter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}