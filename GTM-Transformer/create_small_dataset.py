"""
GTM ë°ì´í„°ì…‹ì„ Colabìš©ìœ¼ë¡œ ì¶•ì†Œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
- ì›ë³¸ì˜ 1/10 í¬ê¸°ë¡œ ìƒ˜í”Œë§
- ì¹´í…Œê³ ë¦¬/ìƒ‰ìƒ/ì†Œì¬ ë¶„í¬ ìœ ì§€
- ì‹œì¦Œë³„ ê· ë“± ìƒ˜í”Œë§
"""

import pandas as pd
import numpy as np
import torch
import shutil
import os
from pathlib import Path
from collections import Counter

def create_small_dataset(
    data_folder='./dataset/',
    output_folder='./dataset_small/',
    train_samples=500,
    test_samples=50
):
    """
    ì‘ì€ ë°ì´í„°ì…‹ ìƒì„±
    
    Args:
        data_folder: ì›ë³¸ ë°ì´í„° í´ë”
        output_folder: ì¶œë ¥ í´ë”
        train_samples: í›ˆë ¨ ìƒ˜í”Œ ìˆ˜
        test_samples: í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜
    """
    
    print(f"ğŸ”„ GTM ë°ì´í„°ì…‹ ì¶•ì†Œ ì‹œì‘...")
    print(f"ğŸ“Š ëª©í‘œ: í›ˆë ¨ {train_samples}ê°œ, í…ŒìŠ¤íŠ¸ {test_samples}ê°œ")
    
    # ì¶œë ¥ í´ë” ìƒì„±
    output_path = Path(output_folder)
    output_path.mkdir(exist_ok=True)
    (output_path / 'images').mkdir(exist_ok=True)
    
    # ì›ë³¸ ë°ì´í„° ë¡œë“œ
    train_df = pd.read_csv(Path(data_folder) / 'train.csv', parse_dates=['release_date'])
    test_df = pd.read_csv(Path(data_folder) / 'test.csv', parse_dates=['release_date'])
    gtrends = pd.read_csv(Path(data_folder) / 'gtrends.csv', index_col=[0], parse_dates=True)
    
    print(f"ğŸ“¥ ì›ë³¸ ë°ì´í„° í¬ê¸°:")
    print(f"  - í›ˆë ¨: {len(train_df):,}ê°œ")
    print(f"  - í…ŒìŠ¤íŠ¸: {len(test_df):,}ê°œ")
    
    # 1. í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œë§ (stratified sampling)
    print(f"\nğŸ¯ í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œë§...")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê· ë“± ìƒ˜í”Œë§
    train_sampled_list = []
    categories = train_df['category'].unique()
    
    samples_per_category = train_samples // len(categories)
    remaining_samples = train_samples % len(categories)
    
    for i, category in enumerate(categories):
        category_data = train_df[train_df['category'] == category]
        
        # ì¼ë¶€ ì¹´í…Œê³ ë¦¬ì— ë‚¨ì€ ìƒ˜í”Œ ì¶”ê°€ ë°°ë¶„
        n_samples = samples_per_category + (1 if i < remaining_samples else 0)
        n_samples = min(n_samples, len(category_data))
        
        sampled = category_data.sample(n=n_samples, random_state=42)
        train_sampled_list.append(sampled)
        
        print(f"  ğŸ“‚ {category}: {len(category_data)} â†’ {n_samples}ê°œ")
    
    train_sampled = pd.concat(train_sampled_list, ignore_index=True)
    
    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒ˜í”Œë§
    print(f"\nğŸ¯ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒ˜í”Œë§...")
    test_sampled = test_df.sample(n=min(test_samples, len(test_df)), random_state=42)
    
    print(f"  ğŸ“Š í…ŒìŠ¤íŠ¸: {len(test_df)} â†’ {len(test_sampled)}ê°œ")
    
    # 3. ì‚¬ìš©ëœ ì´ë¯¸ì§€ë“¤ë§Œ ë³µì‚¬
    print(f"\nğŸ–¼ï¸ ì´ë¯¸ì§€ íŒŒì¼ ë³µì‚¬...")
    
    all_sampled = pd.concat([train_sampled, test_sampled])
    image_paths = all_sampled['image_path'].unique()
    
    copied_count = 0
    missing_count = 0
    
    for img_path in image_paths:
        src_path = Path(data_folder) / 'images' / img_path
        dst_path = output_path / 'images' / img_path
        
        # ëŒ€ìƒ í´ë” ìƒì„±
        dst_path.parent.mkdir(parents=True, exist_ok=True)
        
        if src_path.exists():
            shutil.copy2(src_path, dst_path)
            copied_count += 1
        else:
            print(f"  âš ï¸ ì—†ëŠ” íŒŒì¼: {img_path}")
            missing_count += 1
    
    print(f"  âœ… ë³µì‚¬: {copied_count}ê°œ")
    if missing_count > 0:
        print(f"  âŒ ëˆ„ë½: {missing_count}ê°œ")
    
    # 4. CSV íŒŒì¼ ì €ì¥
    print(f"\nğŸ’¾ CSV íŒŒì¼ ì €ì¥...")
    train_sampled.to_csv(output_path / 'train.csv', index=False)
    test_sampled.to_csv(output_path / 'test.csv', index=False)
    
    # Google TrendsëŠ” ê·¸ëŒ€ë¡œ ë³µì‚¬ (ì‹œê°„ ì‹œë¦¬ì¦ˆì´ë¯€ë¡œ)
    gtrends.to_csv(output_path / 'gtrends.csv')
    
    # 5. ë¼ë²¨ ë”•ì…”ë„ˆë¦¬ ì—…ë°ì´íŠ¸
    print(f"\nğŸ·ï¸ ë¼ë²¨ ë”•ì…”ë„ˆë¦¬ ì—…ë°ì´íŠ¸...")
    
    # ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ë¼ë²¨ë§Œ ì¶”ì¶œ
    used_categories = set(all_sampled['category'].unique())
    used_colors = set(all_sampled['color'].unique()) 
    used_fabrics = set(all_sampled['fabric'].unique())
    
    # ìƒˆë¡œìš´ ë¼ë²¨ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    new_cat_dict = {cat: i for i, cat in enumerate(sorted(used_categories))}
    new_col_dict = {col: i for i, col in enumerate(sorted(used_colors))}
    new_fab_dict = {fab: i for i, fab in enumerate(sorted(used_fabrics))}
    
    # ì €ì¥
    torch.save(new_cat_dict, output_path / 'category_labels.pt')
    torch.save(new_col_dict, output_path / 'color_labels.pt') 
    torch.save(new_fab_dict, output_path / 'fabric_labels.pt')
    
    print(f"  ğŸ“‹ ì¹´í…Œê³ ë¦¬: {len(new_cat_dict)}ê°œ")
    print(f"  ğŸ¨ ìƒ‰ìƒ: {len(new_col_dict)}ê°œ")
    print(f"  ğŸ§µ ì†Œì¬: {len(new_fab_dict)}ê°œ")
    
    # 6. í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ìµœì¢… ë°ì´í„°ì…‹ í†µê³„:")
    print(f"  ğŸš‚ í›ˆë ¨: {len(train_sampled)}ê°œ")
    print(f"  ğŸ§ª í…ŒìŠ¤íŠ¸: {len(test_sampled)}ê°œ") 
    print(f"  ğŸ–¼ï¸ ì´ë¯¸ì§€: {copied_count}ê°œ")
    print(f"  ğŸ’¾ í´ë” í¬ê¸° ì¶”ì •: ~{copied_count * 0.1:.0f} MB")
    
    print(f"\nâœ… ì¶•ì†Œëœ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {output_path}")
    print(f"\nğŸš€ Google Driveì— '{output_path.name}' í´ë”ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”!")
    
    return {
        'train_size': len(train_sampled),
        'test_size': len(test_sampled),
        'images': copied_count,
        'categories': len(new_cat_dict),
        'colors': len(new_col_dict), 
        'fabrics': len(new_fab_dict)
    }

if __name__ == "__main__":
    # ì‹¤í–‰
    stats = create_small_dataset(
        data_folder='./dataset/',
        output_folder='./dataset_small/',
        train_samples=500,  # í›ˆë ¨ 500ê°œ
        test_samples=50     # í…ŒìŠ¤íŠ¸ 50ê°œ
    )
    
    print("\n" + "="*50)
    print("ğŸ‰ ì™„ë£Œ! ë‹¤ìŒ ë‹¨ê³„:")
    print("1. 'dataset_small' í´ë”ë¥¼ Google Driveì— ì—…ë¡œë“œ")
    print("2. Colabì—ì„œ ê²½ë¡œë¥¼ 'GTM-dataset-small'ë¡œ ë³€ê²½")
    print("3. ë¹ ë¥¸ ì‹¤í–‰ìœ¼ë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸!")