{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ğŸ“Š GTM (Google Trends Transformer) íŒ¨ì…˜ ë§¤ì¶œ ì˜ˆì¸¡ - Google Colab\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Google Colabì—ì„œ GTM ëª¨ë¸ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ¯ ëª¨ë¸ ê°œìš”\n",
    "- **ë©€í‹°ëª¨ë‹¬ ì…ë ¥**: ì´ë¯¸ì§€, í…ìŠ¤íŠ¸(ì¹´í…Œê³ ë¦¬/ìƒ‰ìƒ/ì†Œì¬), Google Trends, ì‹œê°„ì  íŠ¹ì„±\n",
    "- **ì¶œë ¥**: í–¥í›„ 12ê°œì›” ë§¤ì¶œ ì˜ˆì¸¡\n",
    "- **ì•„í‚¤í…ì²˜**: Transformer ê¸°ë°˜ + ResNet50 + BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. ğŸ”§ í™˜ê²½ ì„¤ì • ë° Google Drive ë§ˆìš´íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "import os\n",
    "os.chdir('/content')\n",
    "\n",
    "# GPU í™•ì¸\n",
    "import torch\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": "# ë‹¨ê³„ì  íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ì—ëŸ¬ ë°©ì§€)\nimport os\nimport subprocess\nimport sys\n\ndef install_package(package):\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n        print(f\"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"âŒ {package} ì„¤ì¹˜ ì‹¤íŒ¨: {e}\")\n        return False\n\n# 1. ê¸°ë³¸ ì˜ì¡´ì„±ë¶€í„° ì„¤ì¹˜\nprint(\"ğŸ”§ ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\ninstall_package(\"wheel\")\ninstall_package(\"setuptools\")\n\n# 2. tokenizers ë¬¸ì œ í•´ê²° - ì‚¬ì „ ì»´íŒŒì¼ëœ ë²„ì „ ì‚¬ìš©\nprint(\"ğŸ”§ tokenizers ì„¤ì¹˜ ì¤‘...\")\n!pip install tokenizers --no-build-isolation --quiet\n\n# 3. transformers ì„¤ì¹˜\nprint(\"ğŸ”§ transformers ì„¤ì¹˜ ì¤‘...\")\ninstall_package(\"transformers==4.21.0\")\n\n# 4. PyTorch Lightning ì„¤ì¹˜ (Colab ê¸°ë³¸ PyTorchì™€ í˜¸í™˜ë˜ëŠ” ë²„ì „)\nprint(\"ğŸ”§ PyTorch Lightning ì„¤ì¹˜ ì¤‘...\")\ninstall_package(\"pytorch-lightning==1.9.5\")  # ë” ì•ˆì •ì ì¸ ë²„ì „ ì‚¬ìš©\n\n# 5. ê¸°íƒ€ í•„ìš” íŒ¨í‚¤ì§€\nprint(\"ğŸ”§ ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\ninstall_package(\"wandb\")\ninstall_package(\"scikit-learn\")\n\nprint(\"ğŸ“¦ ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")\n\n# ì´ì œ import í…ŒìŠ¤íŠ¸\ntry:\n    import torch\n    import pytorch_lightning as pl\n    import transformers\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from pathlib import Path\n    import warnings\n    warnings.filterwarnings('ignore')\n    \n    from pytorch_lightning.callbacks import ModelCheckpoint\n    from pytorch_lightning.loggers import CSVLogger\n    \n    print(f\"âœ… PyTorch: {torch.__version__}\")\n    print(f\"âœ… PyTorch Lightning: {pl.__version__}\")\n    print(f\"âœ… Transformers: {transformers.__version__}\")\n    print(f\"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n    \nexcept ImportError as e:\n    print(f\"âŒ Import ì‹¤íŒ¨: {e}\")\n    print(\"âš ï¸ ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•œ í›„ ë‹¤ìŒ ëŒ€ì•ˆ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\")\n    print(\"!pip uninstall -y tokenizers transformers pytorch-lightning\")\n    print(\"!pip install pytorch-lightning==1.9.5 transformers==4.21.0 --no-cache-dir\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "## 2. ğŸ”„ GTM-Transformer ì½”ë“œ ë³µì‚¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_code"
   },
   "outputs": [],
   "source": "# ìˆ˜ì •ëœ GTM-Transformer ì½”ë“œ ì‚¬ìš© (Google Driveì—ì„œ ë³µì‚¬)\n# git clone ëŒ€ì‹  ì´ë¯¸ ìˆ˜ì •ëœ ì½”ë“œë¥¼ Google Driveì—ì„œ ë³µì‚¬\n\nprint(\"ğŸ” Google Driveì—ì„œ ìˆ˜ì •ëœ GTM-Transformer ì½”ë“œ í™•ì¸...\")\n\n# Google Driveì˜ GTM-Transformer ê²½ë¡œ (ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ê²½ë¡œ)\nDRIVE_CODE_PATH = '/content/drive/MyDrive/GTM-Transformer/'  # ìˆ˜ì •ëœ ì½”ë“œ ê²½ë¡œ\nDRIVE_DATASET_PATH = '/content/drive/MyDrive/GTM-dataset/'   # ë°ì´í„°ì…‹ ê²½ë¡œ\n\n# GTM-Transformer í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸\nif os.path.exists(DRIVE_CODE_PATH):\n    print(\"âœ… ìˆ˜ì •ëœ GTM-Transformer ì½”ë“œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!\")\n    print(\"ğŸ“‚ ì½”ë“œ êµ¬ì¡°:\")\n    !ls -la \"/content/drive/MyDrive/GTM-Transformer/\"\n    \n    # ì½”ë“œë¥¼ Colab ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬\n    print(\"ğŸ”„ ì½”ë“œ ë³µì‚¬ ì¤‘...\")\n    !cp -r \"/content/drive/MyDrive/GTM-Transformer\" \"/content/\"\n    os.chdir('/content/GTM-Transformer')\n    \n    print(\"âœ… ì½”ë“œ ë³µì‚¬ ì™„ë£Œ!\")\n    \n    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±\n    !mkdir -p checkpoints logs saved_models\n    \nelse:\n    print(\"âŒ GTM-Transformer í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n    print(\"ğŸ“‹ ì—…ë¡œë“œ ë°©ë²•:\")\n    print(\"1. ë¡œì»¬ì˜ ìˆ˜ì •ëœ GTM-Transformer í´ë” ì „ì²´ë¥¼ Google Driveì— ì—…ë¡œë“œ\")\n    print(\"2. Google Drive ê²½ë¡œ: /content/drive/MyDrive/GTM-Transformer/\")\n    print(\"3. í´ë” êµ¬ì¡° í™•ì¸:\")\n    print(\"   GTM-Transformer/\")\n    print(\"   â”œâ”€â”€ models/\")\n    print(\"   â”œâ”€â”€ utils/\") \n    print(\"   â”œâ”€â”€ train.py\")\n    print(\"   â””â”€â”€ ...\")\n\n# ë°ì´í„°ì…‹ ê²½ë¡œë„ í™•ì¸\nif os.path.exists(DRIVE_DATASET_PATH):\n    print(\"âœ… ë°ì´í„°ì…‹ë„ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤!\")\nelse:\n    print(\"âš ï¸ ë°ì´í„°ì…‹ í´ë”ë„ í™•ì¸í•´ì£¼ì„¸ìš”: /content/drive/MyDrive/GTM-dataset/\")\n\nprint(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fix_code"
   },
   "source": "## 3. âœ… ì½”ë“œ ì¤€ë¹„ ì™„ë£Œ (ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ìˆ˜ì •ë¨)\n\nGoogle Driveì—ì„œ ë³µì‚¬í•œ ì½”ë“œëŠ” ì´ë¯¸ í˜¸í™˜ì„± ë¬¸ì œê°€ ëª¨ë‘ ìˆ˜ì •ëœ ë²„ì „ì…ë‹ˆë‹¤:\n- âœ… fairseq â†’ transformers (Adafactor)\n- âœ… validation_epoch_end â†’ on_validation_epoch_end  \n- âœ… CUDA â†’ CPU/GPU ìë™ ì„ íƒ\n- âœ… torch.load weights_only íŒŒë¼ë¯¸í„°\n- âœ… PyTorch Lightning 2.x í˜¸í™˜ì„±\n- âœ… **GPU/CPU ë””ë°”ì´ìŠ¤ ë¶ˆì¼ì¹˜ í•´ê²°** (NEW!)\n\n### ğŸ”§ ìµœì‹  ìˆ˜ì •ì‚¬í•­ (GPU/CPU ë””ë°”ì´ìŠ¤ ì˜¤ë¥˜ í•´ê²°)\n- GTM ëª¨ë¸ ë‚´ ëª¨ë“  `.to('cpu')` í˜¸ì¶œì„ ë™ì  ë””ë°”ì´ìŠ¤ í• ë‹¹ìœ¼ë¡œ ë³€ê²½\n- TextEmbedderì˜ word_embeddingsë¥¼ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ë°°ì¹˜\n- ë§ˆìŠ¤í¬ ìƒì„± ì‹œ ë””ë°”ì´ìŠ¤ ì¼ì¹˜ì„± ë³´ì¥\n- autoregressive ë””ì½”ë”© ì‹œ í…ì„œ ë””ë°”ì´ìŠ¤ í†µì¼"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "patch_gtm"
   },
   "outputs": [],
   "source": "# ì½”ë“œ í™•ì¸ (ì„ íƒì )\n# ìˆ˜ì •ëœ ì½”ë“œê°€ ì œëŒ€ë¡œ ë³µì‚¬ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‹¶ë‹¤ë©´ ì‹¤í–‰\n\nprint(\"ğŸ” ì£¼ìš” íŒŒì¼ í™•ì¸:\")\n!ls -la models/\nprint(\"\\nğŸ“„ GTM.pyì—ì„œ transformers import í™•ì¸:\")\n!head -10 models/GTM.py | grep -E \"(transformers|Adafactor)\"\nprint(\"\\nğŸ“„ train.pyì—ì„œ weights_only í™•ì¸:\")\n!grep -n \"weights_only\" train.py | head -3\n\nprint(\"\\nâœ… ëª¨ë“  ìˆ˜ì •ì‚¬í•­ì´ ì ìš©ë˜ì–´ ìˆìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_check"
   },
   "source": [
    "## 4. ğŸ“Š ë°ì´í„°ì…‹ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_dataset"
   },
   "outputs": [],
   "source": "# ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸ (ì¶•ì†Œëœ ë°ì´í„°ì…‹ ì‚¬ìš©)\ndataset_path = Path('/content/drive/MyDrive/GTM-dataset-small/')  # ì¶•ì†Œëœ ë°ì´í„°ì…‹ ê²½ë¡œ\nrequired_files = ['train.csv', 'test.csv', 'gtrends.csv', 'category_labels.pt', 'color_labels.pt', 'fabric_labels.pt']\n\nprint(\"ğŸ“‚ ì¶•ì†Œëœ ë°ì´í„°ì…‹ íŒŒì¼ í™•ì¸:\")\nfor file in required_files:\n    file_path = dataset_path / file\n    exists = file_path.exists()\n    if exists:\n        size = file_path.stat().st_size\n        print(f\"  âœ… {file}: {size/1024:.1f} KB\")\n    else:\n        print(f\"  âŒ {file}: íŒŒì¼ ì—†ìŒ\")\n\n# ì´ë¯¸ì§€ í´ë” í™•ì¸\nimage_path = dataset_path / 'images'\nif image_path.exists():\n    # í•˜ìœ„ í´ë”ë³„ ì´ë¯¸ì§€ ìˆ˜ í™•ì¸\n    total_images = 0\n    print(f\"  ğŸ“ ì´ë¯¸ì§€ í´ë” êµ¬ì¡°:\")\n    for subdir in sorted(image_path.iterdir()):\n        if subdir.is_dir():\n            subdir_images = list(subdir.glob('*.png')) + list(subdir.glob('*.jpg'))\n            print(f\"    ğŸ“‚ {subdir.name}: {len(subdir_images)}ê°œ\")\n            total_images += len(subdir_images)\n    print(f\"  ğŸ–¼ï¸ ì´ ì´ë¯¸ì§€: {total_images}ê°œ\")\nelse:\n    print(f\"  âŒ images/ í´ë” ì—†ìŒ\")\n\n# ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸ (parse_dates íŒŒë¼ë¯¸í„° ì¶”ê°€)\ntry:\n    print(\"\\nğŸ”„ ì¶•ì†Œëœ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸...\")\n    train_df = pd.read_csv(dataset_path / 'train.csv', parse_dates=['release_date'])\n    test_df = pd.read_csv(dataset_path / 'test.csv', parse_dates=['release_date'])\n    gtrends = pd.read_csv(dataset_path / 'gtrends.csv', index_col=[0], parse_dates=True)\n    \n    print(f\"\\nğŸ“Š ì¶•ì†Œëœ ë°ì´í„° í†µê³„:\")\n    print(f\"  - í›ˆë ¨ ë°ì´í„°: {len(train_df):,}ê°œ ìƒ˜í”Œ (ì›ë³¸ì˜ ~1/10)\")\n    print(f\"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ ìƒ˜í”Œ (ì›ë³¸ì˜ ~1/10)\")\n    print(f\"  - Google Trends: {len(gtrends):,}ê°œ ì‹œì  (ë™ì¼)\")\n    print(f\"  - ì´ ì´ë¯¸ì§€: {total_images}ê°œ (ì›ë³¸ì˜ ~1/10)\")\n    \n    # release_date íƒ€ì… í™•ì¸\n    print(f\"\\nğŸ“… release_date íƒ€ì…: {type(train_df['release_date'].iloc[0])}\")\n    print(f\"ğŸ’¾ ì˜ˆìƒ ë°ì´í„°ì…‹ í¬ê¸°: ~{total_images * 0.1:.0f} MB\")\n    \n    # ì¹´í…Œê³ ë¦¬ ë¶„í¬ í™•ì¸\n    print(f\"\\nğŸ·ï¸ ì¹´í…Œê³ ë¦¬ ë¶„í¬:\")\n    category_counts = train_df['category'].value_counts()\n    for cat, count in category_counts.head(10).items():\n        print(f\"  - {cat}: {count}ê°œ\")\n    \n    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n    print(f\"\\nğŸ‘€ ì¶•ì†Œëœ í›ˆë ¨ ë°ì´í„° ì²« 3í–‰:\")\n    print(train_df[['category', 'color', 'fabric', 'release_date', 'image_path']].head(3))\n    \nexcept Exception as e:\n    print(f\"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n    print(\"\\nğŸ”§ í•´ê²° ë°©ë²•:\")\n    print(\"1. ë¡œì»¬ì—ì„œ 'dataset_small' í´ë”ë¥¼ 'GTM-dataset-small'ë¡œ ì´ë¦„ ë³€ê²½\")\n    print(\"2. Google Driveì— 'GTM-dataset-small' í´ë” ì—…ë¡œë“œ\")\n    print(\"3. í´ë” êµ¬ì¡° í™•ì¸:\")\n    print(\"   GTM-dataset-small/\")\n    print(\"   â”œâ”€â”€ train.csv (508 ìƒ˜í”Œ)\")\n    print(\"   â”œâ”€â”€ test.csv (50 ìƒ˜í”Œ)\")\n    print(\"   â”œâ”€â”€ images/ (~558ê°œ ì´ë¯¸ì§€)\")\n    print(\"   â””â”€â”€ *.pt ë¼ë²¨ íŒŒì¼ë“¤\")\n    print(\"4. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ ì‹¤ì œ í´ë” í™•ì¸:\")\n    print(\"   !ls -la '/content/drive/MyDrive/GTM-dataset-small/'\")\n\nprint(f\"\\nğŸš€ ì¶•ì†Œëœ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹ ë¥¸ ì‹¤í—˜ ì¤€ë¹„ ì™„ë£Œ!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": "# ì´ë¯¸ì§€ í´ë” êµ¬ì¡° í™•ì¸ ë° ì¬êµ¬ì„±\nimport shutil\nfrom pathlib import Path\n\nprint(\"ğŸ–¼ï¸ ì´ë¯¸ì§€ í´ë” êµ¬ì¡° í™•ì¸ ë° ìˆ˜ì •...\")\n\ndataset_path = Path('/content/drive/MyDrive/GTM-dataset/')\nimage_path = dataset_path / 'images'\n\n# í˜„ì¬ ì´ë¯¸ì§€ í´ë” êµ¬ì¡° í™•ì¸\nprint(f\"ğŸ“‚ ì´ë¯¸ì§€ í´ë”: {image_path}\")\n\n# í•˜ìœ„ í´ë”ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸\nsubdirs = [d for d in image_path.iterdir() if d.is_dir()]\nimage_files = list(image_path.glob('*.png')) + list(image_path.glob('*.jpg'))\n\nprint(f\"  ğŸ“ í•˜ìœ„ í´ë” ìˆ˜: {len(subdirs)}\")\nprint(f\"  ğŸ–¼ï¸ ì§ì ‘ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: {len(image_files)}\")\n\nif subdirs:\n    print(\"âœ… ì˜¬ë°”ë¥¸ í´ë” êµ¬ì¡°ê°€ ìˆìŠµë‹ˆë‹¤:\")\n    for subdir in subdirs:\n        subdir_images = list(subdir.glob('*.png')) + list(subdir.glob('*.jpg'))\n        print(f\"  ğŸ“ {subdir.name}: {len(subdir_images)}ê°œ ì´ë¯¸ì§€\")\nelse:\n    print(\"âš ï¸ í•˜ìœ„ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ê²½ë¡œë¥¼ í™•ì¸í•©ë‹ˆë‹¤...\")\n    \n    # CSVì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ” ê²½ë¡œ íŒ¨í„´ í™•ì¸\n    sample_paths = train_df['image_path'].head(10).tolist()\n    print(\"ğŸ“‹ CSVì˜ ì´ë¯¸ì§€ ê²½ë¡œ ì˜ˆì‹œ:\")\n    for path in sample_paths:\n        print(f\"  - {path}\")\n    \n    # í•„ìš”í•œ í•˜ìœ„ í´ë”ë“¤ ìƒì„± (CSV ê²½ë¡œ ê¸°ë°˜)\n    required_subdirs = set()\n    for path in train_df['image_path']:\n        if '/' in path:\n            subdir = path.split('/')[0]\n            required_subdirs.add(subdir)\n    \n    print(f\"\\nğŸ“ í•„ìš”í•œ í•˜ìœ„ í´ë”ë“¤: {list(required_subdirs)}\")\n    \n    if len(image_files) > 0 and len(required_subdirs) > 0:\n        print(\"ğŸ”§ í´ë” êµ¬ì¡° ì¬êµ¬ì„± ì¤‘...\")\n        \n        # í•˜ìœ„ í´ë”ë“¤ ìƒì„±\n        for subdir in required_subdirs:\n            (image_path / subdir).mkdir(exist_ok=True)\n            print(f\"  ğŸ“ ìƒì„±: {subdir}/\")\n        \n        # ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ ì ì ˆí•œ í•˜ìœ„ í´ë”ë¡œ ì´ë™\n        # íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ í´ë” ì¶”ì • (ì˜ˆ: PE17/00001.png -> 00001.pngëŠ” PE17 í´ë”ì—)\n        moved_count = 0\n        for img_file in image_files:\n            # CSVì—ì„œ ì´ íŒŒì¼ì„ ì°¸ì¡°í•˜ëŠ” ê²½ë¡œ ì°¾ê¸°\n            matching_paths = train_df[train_df['image_path'].str.contains(img_file.name)]\n            if len(matching_paths) > 0:\n                csv_path = matching_paths.iloc[0]['image_path']\n                if '/' in csv_path:\n                    target_subdir = csv_path.split('/')[0]\n                    target_path = image_path / target_subdir / img_file.name\n                    \n                    if not target_path.exists():\n                        shutil.move(str(img_file), str(target_path))\n                        moved_count += 1\n        \n        print(f\"âœ… {moved_count}ê°œ ì´ë¯¸ì§€ íŒŒì¼ ì´ë™ ì™„ë£Œ!\")\n        \n        # ìµœì¢… êµ¬ì¡° í™•ì¸\n        print(\"\\nğŸ“Š ìµœì¢… í´ë” êµ¬ì¡°:\")\n        for subdir in (image_path).iterdir():\n            if subdir.is_dir():\n                subdir_images = list(subdir.glob('*.png')) + list(subdir.glob('*.jpg'))\n                print(f\"  ğŸ“ {subdir.name}: {len(subdir_images)}ê°œ ì´ë¯¸ì§€\")\n\nprint(\"\\nâœ… ì´ë¯¸ì§€ í´ë” êµ¬ì¡° ì¤€ë¹„ ì™„ë£Œ!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_model"
   },
   "outputs": [],
   "source": "# ëª¨ë¸ import ë° ì„¤ì •\nsys.path.append('./models')\nfrom models.GTM import GTM\nfrom models.FCN import FCN\nfrom utils.data_multitrends import ZeroShotDataset\n\n# í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì¶•ì†Œëœ ë°ì´í„°ì…‹ìš©ìœ¼ë¡œ ì¡°ì •)\nBATCH_SIZE = 8 if torch.cuda.is_available() else 4  # ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°\nEPOCHS = 5  # ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•´ ì—í¬í¬ ìˆ˜ ê°ì†Œ\nACCELERATOR = 'gpu' if torch.cuda.is_available() else 'cpu'\nDEVICES = 1\n\n# ì¶•ì†Œëœ ë°ì´í„°ì…‹ ê²½ë¡œ\ndataset_path = Path('/content/drive/MyDrive/GTM-dataset-small/')\n\n# ë¼ë²¨ ë”•ì…”ë„ˆë¦¬ ë¡œë”© (ì¶•ì†Œëœ ë°ì´í„°ì…‹ì—ì„œ)\ncat_dict = torch.load(dataset_path / 'category_labels.pt', weights_only=False)\ncol_dict = torch.load(dataset_path / 'color_labels.pt', weights_only=False)\nfab_dict = torch.load(dataset_path / 'fabric_labels.pt', weights_only=False)\n\nprint(f\"ğŸ“‹ ì¶•ì†Œëœ ë¼ë²¨ ë”•ì…”ë„ˆë¦¬:\")\nprint(f\"  - ì¹´í…Œê³ ë¦¬: {len(cat_dict)}ê°œ\")\nprint(f\"  - ìƒ‰ìƒ: {len(col_dict)}ê°œ\") \nprint(f\"  - ì†Œì¬: {len(fab_dict)}ê°œ\")\n\n# ì¶•ì†Œëœ ë°ì´í„°ì…‹ ìƒì„±\nprint(\"\\nğŸ”„ ì¶•ì†Œëœ í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\ntrain_dataset = ZeroShotDataset(\n    train_df, \n    dataset_path / 'images',  # ì¶•ì†Œëœ ì´ë¯¸ì§€ ê²½ë¡œ\n    gtrends, \n    cat_dict, \n    col_dict, \n    fab_dict, \n    trend_len=52\n)\n\nprint(\"ğŸ”„ ì¶•ì†Œëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\ntest_dataset = ZeroShotDataset(\n    test_df, \n    dataset_path / 'images',  # ì¶•ì†Œëœ ì´ë¯¸ì§€ ê²½ë¡œ\n    gtrends, \n    cat_dict, \n    col_dict, \n    fab_dict, \n    trend_len=52\n)\n\n# DataLoader ìƒì„± (ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°)\nprint(\"ğŸ”„ DataLoader ìƒì„± ì¤‘...\")\ntrain_loader = train_dataset.get_loader(batch_size=BATCH_SIZE, train=True)\ntest_loader = test_dataset.get_loader(batch_size=1, train=False)\n\nprint(f\"âœ… ì¶•ì†Œëœ ë°ì´í„° ë¡œë”© ì™„ë£Œ!\")\nprint(f\"  - í›ˆë ¨ ë°ì´í„°: {len(train_df)}ê°œ\")\nprint(f\"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ\")  \nprint(f\"  - í›ˆë ¨ ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\nprint(f\"  - í…ŒìŠ¤íŠ¸ ë°°ì¹˜ í¬ê¸°: 1\")\nprint(f\"  - ì—í¬í¬: {EPOCHS}ê°œ (ë¹ ë¥¸ ì‹¤í—˜ìš©)\")\nprint(f\"  - ê°€ì†ê¸°: {ACCELERATOR}\")\n\n# ì²« ë²ˆì§¸ ë°°ì¹˜ êµ¬ì¡° í™•ì¸\nprint(\"\\nğŸ” ì²« ë²ˆì§¸ ë°°ì¹˜ êµ¬ì¡° í™•ì¸:\")\ntry:\n    sample_batch = next(iter(train_loader))\n    print(f\"  - ë°°ì¹˜ ìš”ì†Œ ìˆ˜: {len(sample_batch)}\")\n    for i, item in enumerate(sample_batch):\n        if hasattr(item, 'shape'):\n            print(f\"  - ìš”ì†Œ {i}: {item.shape}\")\n        else:\n            print(f\"  - ìš”ì†Œ {i}: {type(item)}\")\n    print(\"âœ… ì¶•ì†Œëœ ë°ì´í„°ì…‹ ë°°ì¹˜ êµ¬ì¡° í™•ì¸ ì™„ë£Œ!\")\n    print(f\"ğŸ’¡ ì˜ˆìƒ í›ˆë ¨ ì‹œê°„: ~{len(train_loader) * EPOCHS // 10} ë¶„\")\nexcept Exception as e:\n    print(f\"âŒ ë°°ì¹˜ ë¡œë”© ì‹¤íŒ¨: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_model"
   },
   "outputs": [],
   "source": "# GTM ëª¨ë¸ ìƒì„± ë° ì„¤ì •\nprint(\"ğŸ¯ GTM ëª¨ë¸ ìƒì„± ì¤‘...\")\n\n# GTM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\nmodel = GTM(\n    embedding_dim=32,\n    hidden_dim=64,\n    output_dim=12,\n    num_heads=4,\n    num_layers=1,\n    use_text=True,\n    use_img=True,\n    cat_dict=cat_dict,\n    col_dict=col_dict,\n    fab_dict=fab_dict,\n    trend_len=52,\n    num_trends=3,\n    gpu_num=0,\n    use_encoder_mask=1,\n    autoregressive=False\n)\n\nprint(f\"âœ… GTM ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\nprint(f\"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}\")\n\n# ì²´í¬í¬ì¸íŠ¸ ë° ë¡œê±° ì„¤ì •\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='./checkpoints/',\n    filename='gtm-colab-{epoch:02d}-{val_mae:.2f}',\n    monitor='val_mae',\n    mode='min',\n    save_top_k=2,\n    verbose=True\n)\n\ncsv_logger = CSVLogger(\n    save_dir='./logs/',\n    name='gtm_colab'\n)\n\n# Trainer ì„¤ì •\ntrainer = pl.Trainer(\n    devices=DEVICES,\n    accelerator=ACCELERATOR,\n    max_epochs=EPOCHS,\n    check_val_every_n_epoch=2,\n    logger=csv_logger,\n    callbacks=[checkpoint_callback],\n    enable_progress_bar=True,\n    log_every_n_steps=20\n)\n\nprint(f\"ğŸš€ Trainer ì„¤ì • ì™„ë£Œ!\")\n\n# ğŸ” Gradient ë””ë²„ê¹…ì„ ìœ„í•œ í…ì„œ ìƒíƒœ í™•ì¸\nprint(\"\\nğŸ” ì…ë ¥ í…ì„œë“¤ì˜ gradient ìƒíƒœ í™•ì¸...\")\n\n# ì²« ë²ˆì§¸ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°\nsample_batch = next(iter(train_loader))\nitem_sales, category, color, fabric, temporal_features, gtrends, images = sample_batch\n\nprint(\"ğŸ“Š ì…ë ¥ í…ì„œ gradient ìƒíƒœ:\")\nprint(f\"  - item_sales: requires_grad={item_sales.requires_grad}, device={item_sales.device}\")\nprint(f\"  - category: requires_grad={category.requires_grad}, device={category.device}\")\nprint(f\"  - color: requires_grad={color.requires_grad}, device={color.device}\")\nprint(f\"  - fabric: requires_grad={fabric.requires_grad}, device={fabric.device}\")\nprint(f\"  - temporal_features: requires_grad={temporal_features.requires_grad}, device={temporal_features.device}\")\nprint(f\"  - gtrends: requires_grad={gtrends.requires_grad}, device={gtrends.device}\")\nprint(f\"  - images: requires_grad={images.requires_grad}, device={images.device}\")\n\n# ëª¨ë¸ì˜ ê° ì¸ì½”ë”ë³„ ì¶œë ¥ í™•ì¸\nprint(\"\\nğŸ”§ ê° ì¸ì½”ë” ì¶œë ¥ì˜ gradient ìƒíƒœ:\")\nwith torch.no_grad():  # ì¼ì‹œì ìœ¼ë¡œ gradient ê³„ì‚° ë¹„í™œì„±í™”\n    img_encoding = model.image_encoder(images)\n    dummy_encoding = model.dummy_encoder(temporal_features)\n    text_encoding = model.text_encoder(category, color, fabric)\n    gtrend_encoding = model.gtrend_encoder(gtrends)\n    \n    print(f\"  - img_encoding: requires_grad={img_encoding.requires_grad}\")\n    print(f\"  - dummy_encoding: requires_grad={dummy_encoding.requires_grad}\")\n    print(f\"  - text_encoding: requires_grad={text_encoding.requires_grad}\")\n    print(f\"  - gtrend_encoding: requires_grad={gtrend_encoding.requires_grad}\")\n\n# ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¤‘ requires_grad=Falseì¸ ê²ƒ ì°¾ê¸°\nprint(\"\\nâš ï¸ requires_grad=Falseì¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°:\")\nno_grad_params = []\nfor name, param in model.named_parameters():\n    if not param.requires_grad:\n        no_grad_params.append(name)\n\nif no_grad_params:\n    print(\"  ë‹¤ìŒ íŒŒë¼ë¯¸í„°ë“¤ì´ requires_grad=False:\")\n    for name in no_grad_params[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥\n        print(f\"    - {name}\")\n    if len(no_grad_params) > 10:\n        print(f\"    ... ì´ {len(no_grad_params)}ê°œ\")\nelse:\n    print(\"  âœ… ëª¨ë“  ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ requires_grad=True\")\n\nprint(\"\\nğŸ’¡ Gradient ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì¡°ì¹˜ê°€ ì ìš©ë¨:\")\nprint(\"  âœ… training_stepì—ì„œ ì…ë ¥ í…ì„œì— requires_grad ì„¤ì •\")\nprint(\"  âœ… ResNet ì¼ë¶€ ë ˆì´ì–´ freeze í•´ì œ\")\nprint(\"  âœ… TextEmbedderì— gradient í™œì„±í™”\")\nprint(\"\\nğŸ¯ ëª¨ë¸ê³¼ Trainer ì¤€ë¹„ ì™„ë£Œ! ë‹¤ìŒ ì…€ì—ì„œ í›ˆë ¨ì„ ì‹œì‘í•˜ì„¸ìš”.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰\n",
    "print(\"ğŸš€ GTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer.fit(\n",
    "    model, \n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=test_loader\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ’¾ ìµœê³  ëª¨ë¸: {checkpoint_callback.best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## 6. ğŸ“Š ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_results"
   },
   "outputs": [],
   "source": [
    "# í›ˆë ¨ ë©”íŠ¸ë¦­ ì‹œê°í™”\n",
    "log_dir = './logs/gtm_colab/'\n",
    "version_dirs = [d for d in os.listdir(log_dir) if d.startswith('version_')]\n",
    "if version_dirs:\n",
    "    latest_version = max(version_dirs, key=lambda x: int(x.split('_')[1]))\n",
    "    metrics_path = os.path.join(log_dir, latest_version, 'metrics.csv')\n",
    "    \n",
    "    if os.path.exists(metrics_path):\n",
    "        metrics_df = pd.read_csv(metrics_path)\n",
    "        \n",
    "        # ë©”íŠ¸ë¦­ í”Œë¡¯\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss í”Œë¡¯\n",
    "        train_loss = metrics_df.dropna(subset=['train_loss'])\n",
    "        val_loss = metrics_df.dropna(subset=['val_loss'])\n",
    "        \n",
    "        axes[0].plot(train_loss['step'], train_loss['train_loss'], label='Training Loss', alpha=0.7)\n",
    "        axes[0].plot(val_loss['step'], val_loss['val_loss'], label='Validation Loss', marker='o')\n",
    "        axes[0].set_title('ğŸ“‰ Training/Validation Loss')\n",
    "        axes[0].set_xlabel('Steps')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # MAE í”Œë¡¯\n",
    "        val_mae = metrics_df.dropna(subset=['val_mae'])\n",
    "        axes[1].plot(val_mae['step'], val_mae['val_mae'], label='Validation MAE', marker='s', color='red')\n",
    "        axes[1].set_title('ğŸ“Š Validation MAE')\n",
    "        axes[1].set_xlabel('Steps')\n",
    "        axes[1].set_ylabel('MAE')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ìµœì¢… ì„±ëŠ¥ ì¶œë ¥\n",
    "        if not val_mae.empty:\n",
    "            final_mae = val_mae['val_mae'].iloc[-1]\n",
    "            print(f\"ğŸ¯ ìµœì¢… Validation MAE: {final_mae:.2f}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ë©”íŠ¸ë¦­ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_predictions"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(test_loader))\n",
    "    item_sales, category, color, fabric, temporal_features, gtrends_batch, images = sample_batch\n",
    "    \n",
    "    # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    predictions, attention_weights = model(category, color, fabric, temporal_features, gtrends_batch, images)\n",
    "    \n",
    "    # ì •ê·œí™” í•´ì œ\n",
    "    actual_sales = item_sales * 1065\n",
    "    predicted_sales = predictions * 1065\n",
    "\n",
    "# ì˜ˆì¸¡ ì‹œê°í™”\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(4, len(predictions))):\n",
    "    actual = actual_sales[i].cpu().numpy()\n",
    "    predicted = predicted_sales[i].cpu().numpy()\n",
    "    \n",
    "    axes[i].plot(months, actual, label='ì‹¤ì œ ë§¤ì¶œ', marker='o', linewidth=2)\n",
    "    axes[i].plot(months, predicted, label='ì˜ˆì¸¡ ë§¤ì¶œ', marker='s', linewidth=2, alpha=0.8)\n",
    "    axes[i].set_title(f'Colab ì˜ˆì¸¡ ê²°ê³¼ {i+1}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    mae = np.mean(np.abs(actual - predicted))\n",
    "    axes[i].text(0.02, 0.98, f'MAE: {mae:.1f}', transform=axes[i].transAxes, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7),\n",
    "                verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ì „ì²´ ì„±ëŠ¥\n",
    "overall_mae = np.mean(np.abs(actual_sales.cpu().numpy() - predicted_sales.cpu().numpy()))\n",
    "print(f\"ğŸ”® Colab ì „ì²´ ì˜ˆì¸¡ MAE: {overall_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_model"
   },
   "source": [
    "## 7. ğŸ’¾ ëª¨ë¸ ì €ì¥ (Google Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_to_drive"
   },
   "outputs": [],
   "source": [
    "# Google Driveì— ê²°ê³¼ ì €ì¥\n",
    "drive_save_path = '/content/drive/MyDrive/GTM-Results/'\n",
    "os.makedirs(drive_save_path, exist_ok=True)\n",
    "\n",
    "# ìµœê³  ëª¨ë¸ì„ Google Driveì— ë³µì‚¬\n",
    "if checkpoint_callback.best_model_path:\n",
    "    import shutil\n",
    "    best_model_name = f\"gtm_colab_best_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.ckpt\"\n",
    "    shutil.copy2(checkpoint_callback.best_model_path, drive_save_path + best_model_name)\n",
    "    print(f\"ğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥: {drive_save_path + best_model_name}\")\n",
    "\n",
    "# ë©”íŠ¸ë¦­ CSVë„ ì €ì¥\n",
    "if os.path.exists(metrics_path):\n",
    "    shutil.copy2(metrics_path, drive_save_path + 'training_metrics.csv')\n",
    "    print(f\"ğŸ“Š í›ˆë ¨ ë©”íŠ¸ë¦­ ì €ì¥: {drive_save_path}training_metrics.csv\")\n",
    "\n",
    "# ë…¸íŠ¸ë¶ë„ ì €ì¥\n",
    "!cp /content/colabtools/notebook.ipynb \"{drive_save_path}GTM_Colab_Executed.ipynb\"\n",
    "print(f\"ğŸ““ ì‹¤í–‰ëœ ë…¸íŠ¸ë¶ ì €ì¥: {drive_save_path}GTM_Colab_Executed.ipynb\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ê²°ê³¼ê°€ Google Driveì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {drive_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": "## ğŸ“‹ Colab ì‹¤í–‰ ê°€ì´ë“œ (ì¶•ì†Œëœ ë°ì´í„°ì…‹ ë²„ì „)\n\n### âœ… ì‹¤í–‰ ì „ ì¤€ë¹„ì‚¬í•­\n\n**1. ë¡œì»¬ì—ì„œ ì¶•ì†Œëœ ë°ì´í„°ì…‹ ìƒì„±:**\n```bash\n# GTM-Transformer í´ë”ì—ì„œ ì‹¤í–‰\npython simple_sample.py    # CSV íŒŒì¼ 1/10ë¡œ ìƒ˜í”Œë§  \npython copy_images.py      # í•„ìš”í•œ ì´ë¯¸ì§€ë§Œ ë³µì‚¬\n```\n\n**2. Google Driveì— ì—…ë¡œë“œí•  í´ë”:**\n```\n/content/drive/MyDrive/\nâ”œâ”€â”€ GTM-Transformer/          # ë¡œì»¬ì—ì„œ ìˆ˜ì •ëœ ì „ì²´ ì½”ë“œ\nâ””â”€â”€ GTM-dataset-small/        # ì¶•ì†Œëœ ë°ì´í„°ì…‹ (NEW!)\n    â”œâ”€â”€ train.csv            # 508ê°œ ìƒ˜í”Œ (ì›ë³¸ì˜ 1/10)\n    â”œâ”€â”€ test.csv             # 50ê°œ ìƒ˜í”Œ (ì›ë³¸ì˜ 1/10)  \n    â”œâ”€â”€ gtrends.csv          # ë™ì¼\n    â”œâ”€â”€ category_labels.pt\n    â”œâ”€â”€ color_labels.pt\n    â”œâ”€â”€ fabric_labels.pt\n    â””â”€â”€ images/              # 558ê°œ ì´ë¯¸ì§€ (ì›ë³¸ì˜ 1/10)\n        â”œâ”€â”€ AI17/ (~88ê°œ)\n        â”œâ”€â”€ AI18/ (~101ê°œ)\n        â”œâ”€â”€ AI19/ (~108ê°œ)\n        â”œâ”€â”€ PE17/ (~96ê°œ)\n        â”œâ”€â”€ PE18/ (~76ê°œ)\n        â””â”€â”€ PE19/ (~89ê°œ)\n```\n\n**3. GPU ì„¤ì •:** \n- Colabì—ì„œ ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ GPU ì„ íƒ\n\n### ğŸš€ ì‹¤í–‰ ìˆœì„œ (ì¶•ì†Œëœ ë°ì´í„°ì…‹)\n1. **íŒ¨í‚¤ì§€ ì„¤ì¹˜** (ìë™ ì—ëŸ¬ ì²˜ë¦¬)\n2. **Google Drive ë§ˆìš´íŠ¸ ë° ì½”ë“œ/ë°ì´í„°ì…‹ í™•ì¸**\n3. **ìˆ˜ì •ëœ ì½”ë“œ ë³µì‚¬** \n4. **ì¶•ì†Œëœ ë°ì´í„° ë¡œë”© ë° ë¹ ë¥¸ ëª¨ë¸ í›ˆë ¨**\n5. **ê²°ê³¼ ë¶„ì„ ë° Google Drive ì €ì¥**\n\n### ğŸ’¡ ì¶•ì†Œëœ ë°ì´í„°ì…‹ì˜ ì¥ì \n- âœ… **10ë°° ë¹ ë¥¸ ì‹¤í–‰** (~5-10ë¶„ ë‚´ ì™„ë£Œ)\n- âœ… **GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°** \n- âœ… **Colab ì„¸ì…˜ ì‹œê°„ ì œí•œ ì—¬ìœ **\n- âœ… **ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘** ê°€ëŠ¥\n- âœ… **ë™ì¼í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜** í…ŒìŠ¤íŠ¸\n\n### âš™ï¸ ìˆ˜ì •ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n- **ë°°ì¹˜ í¬ê¸°**: 8 (GPU) / 4 (CPU)\n- **ì—í¬í¬**: 5ê°œ (ë¹ ë¥¸ ì‹¤í—˜ìš©)\n- **ë°ì´í„°**: 558ê°œ (ì›ë³¸ 5,577ê°œì˜ 1/10)\n- **ì˜ˆìƒ í›ˆë ¨ ì‹œê°„**: ~5-10ë¶„\n\n### âš ï¸ ì£¼ì˜ì‚¬í•­\n- ì¶•ì†Œëœ ë°ì´í„°ì…‹ì´ë¯€ë¡œ ì„±ëŠ¥ì€ ì›ë³¸ë³´ë‹¤ ë‚®ì„ ìˆ˜ ìˆìŒ\n- ì‹¤í—˜ ë° ë””ë²„ê¹… ëª©ì ìœ¼ë¡œ ìµœì í™”ë¨\n- ì‹¤ì œ ìš´ì˜ì‹œì—” ì›ë³¸ ë°ì´í„°ì…‹ ê¶Œì¥\n- ëª¨ë“  ê²°ê³¼ëŠ” Google Driveì— ìë™ ì €ì¥\n\n### ğŸ”„ ì›ë³¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ëŒì•„ê°€ë ¤ë©´\nê²½ë¡œë§Œ ë³€ê²½: `GTM-dataset-small` â†’ `GTM-dataset`"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}