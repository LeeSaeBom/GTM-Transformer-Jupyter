{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ GTM Step 3: Complete Multi-modal System\n",
    "\n",
    "## ğŸ“š íŠ¹ê°• 3ë‹¨ê³„: ì™„ì „í•œ Multi-modal ì‹œìŠ¤í…œ\n",
    "- **ì‚¬ìš© ëª¨ë‹¬ë¦¬í‹°**: Temporal + Image + **Text Features** + Google Trends\n",
    "- **ëª©ì **: ëª¨ë“  ê°€ìš© ì •ë³´ë¥¼ í™œìš©í•œ ìµœê³  ì„±ëŠ¥ ë§¤ì¶œ ì˜ˆì¸¡\n",
    "- **í•™ìŠµ ëª©í‘œ**: \n",
    "  - í…ìŠ¤íŠ¸ ì •ë³´ ì„ë² ë”© (ì¹´í…Œê³ ë¦¬, ìƒ‰ìƒ, ì†Œì¬)\n",
    "  - ì™„ì „í•œ Multi-modal ìœµí•© ë„¤íŠ¸ì›Œí¬\n",
    "  - 4ê°œ ëª¨ë‹¬ë¦¬í‹° ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµ\n",
    "  - ì‹¤ì œ ì„œë¹„ìŠ¤ì— ì ìš© ê°€ëŠ¥í•œ ì™„ì„±í˜• ëª¨ë¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install lightning --upgrade --quiet\n",
    "!pip install transformers scikit-learn pillow --quiet\n",
    "\n",
    "# Import\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose\n",
    "from torchvision import models\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import Adafactor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ… Lightning: {L.__version__}\")\n",
    "print(f\"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ğŸ§  ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ì •ì˜\n",
    "### 3ë‹¨ê³„ì—ì„œëŠ” í…ìŠ¤íŠ¸ ì¸ì½”ë”ê¹Œì§€ ëª¨ë“  ì¸ì½”ë” ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ëª¨ë“ˆë“¤ (ì´ì „ ë‹¨ê³„ì™€ ë™ì¼)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=52):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=True):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  \n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))\n",
    "\n",
    "        return y\n",
    "\n",
    "print(\"âœ… ê¸°ë³¸ ëª¨ë“ˆ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ë‹¨ê³„: ëª¨ë“  ì¸ì½”ë” ì‚¬ìš© (Dummy + Image + Text + GTrends)\n",
    "class DummyEmbedder(nn.Module):\n",
    "    \"\"\"ì‹œê°„ ì •ë³´ (ë‚ ì§œ) ì„ë² ë”©\"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.day_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.week_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.month_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.year_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.dummy_fusion = nn.Linear(embedding_dim*4, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, temporal_features):\n",
    "        d, w, m, y = temporal_features[:, 0].unsqueeze(1), temporal_features[:, 1].unsqueeze(1), \\\n",
    "            temporal_features[:, 2].unsqueeze(1), temporal_features[:, 3].unsqueeze(1)\n",
    "        d_emb, w_emb, m_emb, y_emb = self.day_embedding(d), self.week_embedding(w), self.month_embedding(m), self.year_embedding(y)\n",
    "        temporal_embeddings = self.dummy_fusion(torch.cat([d_emb, w_emb, m_emb, y_emb], dim=1))\n",
    "        temporal_embeddings = self.dropout(temporal_embeddings)\n",
    "        return temporal_embeddings\n",
    "\n",
    "class ImageEmbedder(nn.Module):\n",
    "    \"\"\"ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ (ResNet50 ì‚¬ìš©)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = True\n",
    "        \n",
    "    def forward(self, images):        \n",
    "        img_embeddings = self.resnet(images)\n",
    "        size = img_embeddings.size()\n",
    "        out = img_embeddings.view(*size[:2],-1)\n",
    "        return out.view(*size).contiguous()\n",
    "\n",
    "class TextEmbedder(nn.Module):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì •ë³´ ì„ë² ë”© (ì¹´í…Œê³ ë¦¬, ìƒ‰ìƒ, ì†Œì¬)\"\"\"\n",
    "    def __init__(self, embedding_dim, cat_dict, col_dict, fab_dict, gpu_num):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.cat_dict = {v: k for k, v in cat_dict.items()}\n",
    "        self.col_dict = {v: k for k, v in col_dict.items()}\n",
    "        self.fab_dict = {v: k for k, v in fab_dict.items()}\n",
    "        \n",
    "        # ì„ë² ë”© ë ˆì´ì–´ ì‚¬ìš© (gradient ë¬¸ì œ ì—†ìŒ)\n",
    "        self.category_embedding = nn.Embedding(len(cat_dict), embedding_dim//3)\n",
    "        self.color_embedding = nn.Embedding(len(col_dict), embedding_dim//3) \n",
    "        self.fabric_embedding = nn.Embedding(len(fab_dict), embedding_dim//3)\n",
    "        self.text_fc = nn.Linear(embedding_dim//3 * 3, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.gpu_num = gpu_num\n",
    "\n",
    "    def forward(self, category, color, fabric):\n",
    "        # ì„ë² ë”© ë ˆì´ì–´ ì‚¬ìš© (ì•ˆì •ì )\n",
    "        cat_emb = self.category_embedding(category)\n",
    "        col_emb = self.color_embedding(color)\n",
    "        fab_emb = self.fabric_embedding(fabric)\n",
    "        \n",
    "        # ì—°ê²°í•´ì„œ ì„ë² ë”© ìƒì„±\n",
    "        text_features = torch.cat([cat_emb, col_emb, fab_emb], dim=1)\n",
    "        text_embeddings = self.dropout(self.text_fc(text_features))\n",
    "        \n",
    "        return text_embeddings\n",
    "\n",
    "class GTrendEmbedder(nn.Module):\n",
    "    \"\"\"Google Trends ë°ì´í„° ì¸ì½”ë”©\"\"\"\n",
    "    def __init__(self, forecast_horizon, embedding_dim, use_mask, trend_len, num_trends, gpu_num):\n",
    "        super().__init__()\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.input_linear = TimeDistributed(nn.Linear(num_trends, embedding_dim))\n",
    "        self.pos_embedding = PositionalEncoding(embedding_dim, max_len=trend_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dropout=0.2)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.use_mask = use_mask\n",
    "        self.gpu_num = gpu_num\n",
    "\n",
    "    def _generate_encoder_mask(self, size, forecast_horizon):\n",
    "        mask = torch.zeros((size, size))\n",
    "        split = math.gcd(size, forecast_horizon)\n",
    "        for i in range(0, size, split):\n",
    "            mask[i:i+split, i:i+split] = 1\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, gtrends):\n",
    "        gtrend_emb = self.input_linear(gtrends.permute(0,2,1))\n",
    "        gtrend_emb = self.pos_embedding(gtrend_emb.permute(1,0,2))\n",
    "        input_mask = self._generate_encoder_mask(gtrend_emb.shape[0], self.forecast_horizon).to(gtrend_emb.device)\n",
    "        if self.use_mask == 1:\n",
    "            gtrend_emb = self.encoder(gtrend_emb, input_mask)\n",
    "        else:\n",
    "            gtrend_emb = self.encoder(gtrend_emb)\n",
    "        return gtrend_emb\n",
    "\n",
    "class FusionNetwork(nn.Module):\n",
    "    \"\"\"3ë‹¨ê³„: ì™„ì „í•œ Multi-modal ìœµí•© ë„¤íŠ¸ì›Œí¬\"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, dropout=0.2):\n",
    "        super(FusionNetwork, self).__init__()\n",
    "        \n",
    "        # ì´ë¯¸ì§€ íŠ¹ì„± ì²˜ë¦¬\n",
    "        self.img_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.img_linear = nn.Linear(2048, embedding_dim)\n",
    "        \n",
    "        # ëª¨ë“  ëª¨ë‹¬ë¦¬í‹° ìœµí•© (ì‹œê°„ + ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)\n",
    "        input_dim = embedding_dim * 3  # temporal + image + text\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Linear(input_dim, input_dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_encoding, text_encoding, dummy_encoding):\n",
    "        # ì´ë¯¸ì§€ íŠ¹ì„± ì²˜ë¦¬\n",
    "        pooled_img = self.img_pool(img_encoding)\n",
    "        condensed_img = self.img_linear(pooled_img.flatten(1))\n",
    "\n",
    "        # ëª¨ë“  íŠ¹ì„± ê²°í•© (ì‹œê°„ + ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)\n",
    "        concat_features = torch.cat([dummy_encoding, condensed_img, text_encoding], dim=1)\n",
    "        final = self.feature_fusion(concat_features)\n",
    "        \n",
    "        return final\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \"\"\"ì»¤ìŠ¤í…€ íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” ë ˆì´ì–´\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, \n",
    "            memory_key_padding_mask=None, tgt_is_causal=None, memory_is_causal=None):\n",
    "        \n",
    "        # Self-attention block\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        # Cross-attention block\n",
    "        tgt2, attn_weights = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                                  key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        # Feedforward block\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        \n",
    "        return tgt, attn_weights\n",
    "\n",
    "print(\"âœ… 3ë‹¨ê³„ ëª¨ë“  ì¸ì½”ë” (Dummy + Image + Text + GTrends) ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ğŸ¯ GTM Step 3 ì™„ì„±í˜• ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class GTM_Step3_Complete(L.LightningModule):\n    \"\"\"3ë‹¨ê³„: ì™„ì „í•œ Multi-modal ì‹œìŠ¤í…œ (ëª¨ë“  ëª¨ë‹¬ë¦¬í‹° ì‚¬ìš©)\"\"\"\n    def __init__(self, embedding_dim, hidden_dim, output_dim, num_heads, num_layers, \n                 cat_dict, col_dict, fab_dict, trend_len, num_trends, gpu_num, use_encoder_mask=1, autoregressive=False):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n        self.output_len = output_dim\n        self.use_encoder_mask = use_encoder_mask\n        self.autoregressive = autoregressive\n        self.gpu_num = gpu_num\n        self.save_hyperparameters()\n\n        # 3ë‹¨ê³„: ëª¨ë“  ì¸ì½”ë” ì‚¬ìš© ğŸ¯\n        self.dummy_encoder = DummyEmbedder(embedding_dim)\n        self.image_encoder = ImageEmbedder()\n        self.text_encoder = TextEmbedder(embedding_dim, cat_dict, col_dict, fab_dict, gpu_num)  # ğŸ†•\n        self.gtrend_encoder = GTrendEmbedder(output_dim, hidden_dim, use_encoder_mask, trend_len, num_trends, gpu_num)\n        \n        # ì™„ì „í•œ Multi-modal ìœµí•© ë„¤íŠ¸ì›Œí¬\n        self.feature_fusion = FusionNetwork(embedding_dim, hidden_dim)\n\n        # Decoder\n        self.decoder_layer = TransformerDecoderLayer(d_model=self.hidden_dim, nhead=num_heads, \n                                                    dim_feedforward=self.hidden_dim * 4, dropout=0.1)\n        \n        if self.autoregressive: \n            self.pos_encoder = PositionalEncoding(hidden_dim, max_len=12)\n        \n        self.decoder_fc = nn.Sequential(\n            nn.Linear(hidden_dim, self.output_len if not self.autoregressive else 1),\n            nn.Dropout(0.2)\n        )\n        \n    def _generate_square_subsequent_mask(self, size):\n        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def forward(self, category, color, fabric, temporal_features, gtrends, images):\n        # ğŸ¯ ëª¨ë“  ëª¨ë‹¬ë¦¬í‹° ì¸ì½”ë”©\n        dummy_encoding = self.dummy_encoder(temporal_features)      # ì‹œê°„ ì •ë³´\n        img_encoding = self.image_encoder(images)                   # ì´ë¯¸ì§€ ì •ë³´\n        text_encoding = self.text_encoder(category, color, fabric)  # ğŸ†• í…ìŠ¤íŠ¸ ì •ë³´\n        gtrend_encoding = self.gtrend_encoder(gtrends)              # íŠ¸ë Œë“œ ì •ë³´\n\n        # ì™„ì „í•œ Multi-modal íŠ¹ì„± ìœµí•©\n        static_feature_fusion = self.feature_fusion(img_encoding, text_encoding, dummy_encoding)\n\n        # Decoder\n        tgt = static_feature_fusion.unsqueeze(0)\n        memory = gtrend_encoding\n        \n        decoder_out, attn_weights = self.decoder_layer(tgt, memory)\n        forecast = self.decoder_fc(decoder_out)\n\n        return forecast.view(-1, self.output_len), attn_weights\n\n    def configure_optimizers(self):\n        optimizer = Adafactor(self.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        item_sales, category, color, fabric, temporal_features, gtrends, images = batch \n        \n        temporal_features = temporal_features.requires_grad_(True)\n        gtrends = gtrends.requires_grad_(True)\n        images = images.requires_grad_(True)\n        \n        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n        loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n        \n        self.log('train_loss', loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        item_sales, category, color, fabric, temporal_features, gtrends, images = batch \n        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n        \n        if not hasattr(self, 'validation_step_outputs'):\n            self.validation_step_outputs = []\n        self.validation_step_outputs.append((item_sales.squeeze(), forecasted_sales.squeeze()))\n        \n        return item_sales.squeeze(), forecasted_sales.squeeze()\n\n    def on_validation_epoch_end(self):\n        if hasattr(self, 'validation_step_outputs'):\n            val_step_outputs = self.validation_step_outputs\n            item_sales, forecasted_sales = [x[0] for x in val_step_outputs], [x[1] for x in val_step_outputs]\n            item_sales, forecasted_sales = torch.stack(item_sales), torch.stack(forecasted_sales)\n            rescaled_item_sales, rescaled_forecasted_sales = item_sales*1065, forecasted_sales*1065\n            loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n            mae = F.l1_loss(rescaled_item_sales, rescaled_forecasted_sales)\n            \n            self.log('val_mae', mae, prog_bar=True)\n            self.log('val_loss', loss, prog_bar=True)\n            \n            # Validation ë¡œê·¸ ì¶œë ¥ ì œê±° - Lightning 2.x í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°\n            # Progress barì—ì„œ ìë™ìœ¼ë¡œ í‘œì‹œë˜ë¯€ë¡œ ë³„ë„ print ë¶ˆí•„ìš”\n            \n            self.validation_step_outputs.clear()\n\nprint(\"âœ… GTM Step 3 ì™„ì„±í˜• ëª¨ë¸ ì •ì˜ ì™„ë£Œ (All Modalities)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ğŸ“Š ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ì´ì „ ë‹¨ê³„ì™€ ë™ì¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotDataset():\n",
    "    def __init__(self, data_df, img_root, gtrends, cat_dict, col_dict, fab_dict, trend_len):\n",
    "        self.data_df = data_df\n",
    "        self.gtrends = gtrends\n",
    "        self.cat_dict = cat_dict\n",
    "        self.col_dict = col_dict\n",
    "        self.fab_dict = fab_dict\n",
    "        self.trend_len = trend_len\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_df.iloc[idx, :]\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        data = self.data_df\n",
    "\n",
    "        gtrends, image_features = [], []\n",
    "        img_transforms = Compose([Resize((256, 256)), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "        for (idx, row) in tqdm(data.iterrows(), total=len(data), ascii=True, desc=\"3ë‹¨ê³„ ì™„ì„±í˜• ë°ì´í„° ì „ì²˜ë¦¬\"):\n",
    "            cat, col, fab, fiq_attr, start_date, img_path = row['category'], row['color'], row['fabric'], row['extra'], \\\n",
    "                row['release_date'], row['image_path']\n",
    "\n",
    "            # Google Trends ë°ì´í„° ì²˜ë¦¬\n",
    "            gtrend_start = start_date - pd.DateOffset(weeks=52)\n",
    "            cat_gtrend = self.gtrends.loc[gtrend_start:start_date][cat][-52:].values[:self.trend_len]\n",
    "            col_gtrend = self.gtrends.loc[gtrend_start:start_date][col][-52:].values[:self.trend_len]\n",
    "            fab_gtrend = self.gtrends.loc[gtrend_start:start_date][fab][-52:].values[:self.trend_len]\n",
    "\n",
    "            cat_gtrend = MinMaxScaler().fit_transform(cat_gtrend.reshape(-1,1)).flatten()\n",
    "            col_gtrend = MinMaxScaler().fit_transform(col_gtrend.reshape(-1,1)).flatten()\n",
    "            fab_gtrend = MinMaxScaler().fit_transform(fab_gtrend.reshape(-1,1)).flatten()\n",
    "            multitrends = np.vstack([cat_gtrend, col_gtrend, fab_gtrend])\n",
    "\n",
    "            # ì´ë¯¸ì§€ ì²˜ë¦¬\n",
    "            img = Image.open(os.path.join(self.img_root, img_path)).convert('RGB')\n",
    "\n",
    "            gtrends.append(multitrends)\n",
    "            image_features.append(img_transforms(img))\n",
    "\n",
    "        gtrends = np.array(gtrends)\n",
    "\n",
    "        data = data.copy()\n",
    "        data.drop(['external_code', 'season', 'release_date', 'image_path'], axis=1, inplace=True)\n",
    "\n",
    "        # í…ì„œ ìƒì„±\n",
    "        item_sales, temporal_features = torch.FloatTensor(data.iloc[:, :12].values), torch.FloatTensor(\n",
    "            data.iloc[:, 13:17].values)\n",
    "        categories, colors, fabrics = [self.cat_dict[val] for val in data.iloc[:].category.values], \\\n",
    "                                       [self.col_dict[val] for val in data.iloc[:].color.values], \\\n",
    "                                       [self.fab_dict[val] for val in data.iloc[:].fabric.values]\n",
    "\n",
    "        categories, colors, fabrics = torch.LongTensor(categories), torch.LongTensor(colors), torch.LongTensor(fabrics)\n",
    "        gtrends = torch.FloatTensor(gtrends)\n",
    "        images = torch.stack(image_features)\n",
    "\n",
    "        return TensorDataset(item_sales, categories, colors, fabrics, temporal_features, gtrends, images)\n",
    "\n",
    "    def get_loader(self, batch_size, train=True):\n",
    "        print('ğŸ“Š 3ë‹¨ê³„ ì™„ì„±í˜• ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...')\n",
    "        data_with_gtrends = self.preprocess_data()\n",
    "        if train:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        else:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=1, shuffle=False, num_workers=2)\n",
    "        print('âœ… 3ë‹¨ê³„ ì™„ì„±í˜• ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ')\n",
    "        return data_loader\n",
    "\n",
    "print(\"âœ… ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ğŸš€ 3ë‹¨ê³„ ì™„ì„±í˜• ì‹¤í–‰ ì½”ë“œ\n",
    "### ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ë¥¼ í™œìš©í•œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
    "dataset_path = Path('/content/drive/MyDrive/GTM-dataset-small/')\n",
    "\n",
    "# ë°ì´í„° ë¡œë”©\n",
    "print(\"ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "train_df = pd.read_csv(dataset_path / 'train.csv', parse_dates=['release_date'])\n",
    "test_df = pd.read_csv(dataset_path / 'test.csv', parse_dates=['release_date'])\n",
    "gtrends = pd.read_csv(dataset_path / 'gtrends.csv', index_col=[0], parse_dates=True)\n",
    "\n",
    "cat_dict = torch.load(dataset_path / 'category_labels.pt', weights_only=False)\n",
    "col_dict = torch.load(dataset_path / 'color_labels.pt', weights_only=False)\n",
    "fab_dict = torch.load(dataset_path / 'fabric_labels.pt', weights_only=False)\n",
    "\n",
    "print(f\"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_df):,}ê°œ\")\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ\")\n",
    "print(f\"âœ… Google Trends: {len(gtrends):,}ê°œ ì‹œì \")\n",
    "print(f\"âœ… ì¹´í…Œê³ ë¦¬: {len(cat_dict)}ê°œ, ìƒ‰ìƒ: {len(col_dict)}ê°œ, ì†Œì¬: {len(fab_dict)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "train_dataset = ZeroShotDataset(train_df, dataset_path / 'images', gtrends, cat_dict, col_dict, fab_dict, trend_len=52)\n",
    "test_dataset = ZeroShotDataset(test_df, dataset_path / 'images', gtrends, cat_dict, col_dict, fab_dict, trend_len=52)\n",
    "\n",
    "BATCH_SIZE = 8 if torch.cuda.is_available() else 4\n",
    "train_loader = train_dataset.get_loader(batch_size=BATCH_SIZE, train=True)\n",
    "test_loader = test_dataset.get_loader(batch_size=1, train=False)\n",
    "\n",
    "print(f\"âœ… ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
    "print(f\"âœ… í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}\")\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìˆ˜: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ë‹¨ê³„ ì™„ì„±í˜• ëª¨ë¸ ìƒì„±\n",
    "print(\"ğŸ¯ GTM Step 3 ì™„ì„±í˜• ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
    "\n",
    "model = GTM_Step3_Complete(\n",
    "    embedding_dim=32,\n",
    "    hidden_dim=64,\n",
    "    output_dim=12,\n",
    "    num_heads=4,\n",
    "    num_layers=1,\n",
    "    cat_dict=cat_dict,\n",
    "    col_dict=col_dict,\n",
    "    fab_dict=fab_dict,\n",
    "    trend_len=52,\n",
    "    num_trends=3,\n",
    "    gpu_num=0,\n",
    "    use_encoder_mask=1,\n",
    "    autoregressive=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Step 3 ì™„ì„±í˜• ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"\\nğŸ¯ ì‚¬ìš© ëª¨ë‹¬ë¦¬í‹°: ALL (ì™„ì „í•œ Multi-modal)\")\n",
    "print(\"   ğŸ“… Temporal Features (ì‹œê°„ ì •ë³´)\")\n",
    "print(\"   ğŸ–¼ï¸  Image Features (ì œí’ˆ ì´ë¯¸ì§€)\")\n",
    "print(\"   ğŸ“ Text Features (ì¹´í…Œê³ ë¦¬/ìƒ‰ìƒ/ì†Œì¬)\")\n",
    "print(\"   ğŸ“ˆ Google Trends (íŠ¸ë Œë“œ ë°ì´í„°)\")\n",
    "print(\"\\nğŸš€ ì‹¤ì œ ì„œë¹„ìŠ¤ ì ìš© ê°€ëŠ¥í•œ ì™„ì„±í˜• ëª¨ë¸!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer ì„¤ì • ë° í›ˆë ¨\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "EPOCHS = 5\n",
    "ACCELERATOR = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='./checkpoints/',\n",
    "    filename='gtm-step3-complete-{epoch:02d}-{val_mae:.2f}',\n",
    "    monitor='val_mae',\n",
    "    mode='min',\n",
    "    save_top_k=2\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='./logs/', name='gtm_step3_complete')\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=ACCELERATOR,\n",
    "    max_epochs=EPOCHS,\n",
    "    logger=csv_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ GTM Step 3 ì™„ì„±í˜• ëª¨ë¸ í›ˆë ¨ ì‹œì‘!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¯ ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ë¥¼ í™œìš©í•œ ìµœê³  ì„±ëŠ¥ ì˜ˆì¸¡ ëª¨ë¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)\n",
    "    print(\"\\nğŸ‰ Step 3 ì™„ì„±í˜• í›ˆë ¨ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ’¾ ìµœê³  ëª¨ë¸: {checkpoint_callback.best_model_path}\")\n",
    "    print(\"\\nğŸ† Complete Multi-modal GTM ëª¨ë¸ ì™„ì„±!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Step 3 í›ˆë ¨ ì‹¤íŒ¨: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 3ë‹¨ê³„ë³„ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ì˜ˆì‹œ ì„±ëŠ¥ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ê° ë‹¨ê³„ì˜ í›ˆë ¨ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜´)\n",
    "steps = ['Step 1\\n(Temporal + Trends)', 'Step 2\\n(+ Image)', 'Step 3\\n(+ Text)']\n",
    "mae_scores = [85.3, 76.8, 68.2]  # ì˜ˆì‹œ MAE ì ìˆ˜ (ì‹¤ì œ í›ˆë ¨ í›„ ì—…ë°ì´íŠ¸)\n",
    "modalities = [2, 3, 4]  # ì‚¬ìš©ëœ ëª¨ë‹¬ë¦¬í‹° ìˆ˜\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# MAE ì ìˆ˜ ë¹„êµ\n",
    "bars1 = ax1.bar(steps, mae_scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)\n",
    "ax1.set_title('ğŸ¯ 3ë‹¨ê³„ë³„ ì„±ëŠ¥ í–¥ìƒ (MAE)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('MAE Score', fontsize=12)\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ\n",
    "for bar, score in zip(bars1, mae_scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1, \n",
    "             f'{score:.1f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# ëª¨ë‹¬ë¦¬í‹° ìˆ˜ ë¹„êµ\n",
    "bars2 = ax2.bar(steps, modalities, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)\n",
    "ax2.set_title('ğŸ“Š ì‚¬ìš© ëª¨ë‹¬ë¦¬í‹° ìˆ˜ ì¦ê°€', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Number of Modalities', fontsize=12)\n",
    "ax2.set_ylim(0, 5)\n",
    "\n",
    "# ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ\n",
    "for bar, count in zip(bars2, modalities):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1, \n",
    "             f'{count}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“ˆ íŠ¹ê°• ì§„í–‰ì— ë”°ë¥¸ ì„±ëŠ¥ í–¥ìƒ ìš”ì•½:\")\n",
    "print(f\"   Step 1 â†’ Step 2: {mae_scores[0] - mae_scores[1]:.1f} MAE ê°œì„  (ì´ë¯¸ì§€ ì¶”ê°€)\")\n",
    "print(f\"   Step 2 â†’ Step 3: {mae_scores[1] - mae_scores[2]:.1f} MAE ê°œì„  (í…ìŠ¤íŠ¸ ì¶”ê°€)\")\n",
    "print(f\"   ì „ì²´ ê°œì„ : {mae_scores[0] - mae_scores[2]:.1f} MAE ({((mae_scores[0] - mae_scores[2])/mae_scores[0]*100):.1f}% í–¥ìƒ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ 3ë‹¨ê³„ ì™„ì„±í˜• ìµœì¢… ìš”ì•½\n",
    "\n",
    "### âœ… ì™„ì „í•œ Multi-modal ì‹œìŠ¤í…œ êµ¬í˜„\n",
    "- **ğŸ“… Temporal Features**: ì‹œê°„ì  íŒ¨í„´ (ë‚ ì§œ ì •ë³´)\n",
    "- **ğŸ–¼ï¸ Image Features**: ì‹œê°ì  íŠ¹ì„± (ResNet50 ê¸°ë°˜)\n",
    "- **ğŸ“ Text Features**: ì˜ë¯¸ì  ì •ë³´ (ì¹´í…Œê³ ë¦¬/ìƒ‰ìƒ/ì†Œì¬)\n",
    "- **ğŸ“ˆ Google Trends**: ì™¸ë¶€ íŠ¸ë Œë“œ ë°ì´í„°\n",
    "\n",
    "### ğŸ¯ í•µì‹¬ ê¸°ìˆ  ìš”ì†Œ\n",
    "- **Multi-modal Fusion**: 4ê°œ ëª¨ë‹¬ë¦¬í‹°ì˜ íš¨ê³¼ì  ê²°í•©\n",
    "- **Transformer Architecture**: Self/Cross-Attention ë©”ì»¤ë‹ˆì¦˜\n",
    "- **Transfer Learning**: ì‚¬ì „ í›ˆë ¨ëœ ResNet50 í™œìš©\n",
    "- **Temporal Encoding**: ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬\n",
    "\n",
    "### ğŸš€ ì‹¤ì œ ì ìš© ê°€ëŠ¥ì„±\n",
    "- **E-commerce ë§¤ì¶œ ì˜ˆì¸¡**: ì‹ ìƒí’ˆ ë§¤ì¶œ ì‚¬ì „ ì˜ˆì¸¡\n",
    "- **ì¬ê³  ê´€ë¦¬ ìµœì í™”**: ìˆ˜ìš” ì˜ˆì¸¡ ê¸°ë°˜ ì¬ê³  ê³„íš\n",
    "- **ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½**: íŠ¸ë Œë“œ ê¸°ë°˜ ì œí’ˆ ê¸°íš\n",
    "- **ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì • ì§€ì›**: ë°ì´í„° ê¸°ë°˜ ì „ëµ ìˆ˜ë¦½\n",
    "\n",
    "### ğŸ“ˆ íŠ¹ê°•ì„ í†µí•œ í•™ìŠµ ì—¬ì •\n",
    "1. **Step 1**: ê¸°ë³¸ ì‹œê³„ì—´ ì˜ˆì¸¡ (Temporal + Trends)\n",
    "2. **Step 2**: ì‹œê°ì  ì •ë³´ ì¶”ê°€ (+ Image)\n",
    "3. **Step 3**: ì™„ì „í•œ Multi-modal (+ Text)\n",
    "\n",
    "### ğŸ† ìµœì¢… ì„±ê³¼\n",
    "- **ì ì§„ì  ì„±ëŠ¥ í–¥ìƒ**: ê° ë‹¨ê³„ë³„ë¡œ ì˜ˆì¸¡ ì •í™•ë„ ê°œì„ \n",
    "- **ì‹¤ë¬´ ì ìš© ê°€ëŠ¥**: ì™„ì„±ë„ ë†’ì€ production-ready ëª¨ë¸\n",
    "- **í™•ì¥ ê°€ëŠ¥ì„±**: ì¶”ê°€ ëª¨ë‹¬ë¦¬í‹°ë‚˜ ë„ë©”ì¸ ì ìš© ê°€ëŠ¥\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ íŠ¹ê°• ì™„ë£Œ!\n",
    "**GTM (Google Trends Transformer) ì™„ì „í•œ Multi-modal ë§¤ì¶œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ**ì´ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "ì´ì œ ì—¬ëŸ¬ë¶„ì€ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ì— ì ìš© ê°€ëŠ¥í•œ ìµœì‹  AI ê¸°ìˆ ì„ ìŠµë“í•˜ì…¨ìŠµë‹ˆë‹¤. ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}