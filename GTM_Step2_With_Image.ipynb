{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 GTM Step 2: Temporal + Image + Google Trends\n",
    "\n",
    "## 📚 특강 2단계: 이미지 정보 추가\n",
    "- **사용 모달리티**: Temporal Features + **Image Features** + Google Trends\n",
    "- **목적**: 시각적 정보를 활용한 매출 예측 성능 향상\n",
    "- **학습 목표**: \n",
    "  - CNN을 통한 이미지 특성 추출\n",
    "  - Multi-modal 특성 융합 (Temporal + Visual)\n",
    "  - ResNet50을 사용한 전이학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 📦 패키지 설치 및 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 설치\n",
    "!pip install lightning --upgrade --quiet\n",
    "!pip install transformers scikit-learn pillow --quiet\n",
    "\n",
    "# Import\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose\n",
    "from torchvision import models  # ResNet50 사용\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import Adafactor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Google Drive 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(f\"✅ PyTorch: {torch.__version__}\")\n",
    "print(f\"✅ Lightning: {L.__version__}\")\n",
    "print(f\"✅ CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 🧠 모델 컴포넌트 정의\n",
    "### 2단계에서는 이미지 인코더 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 모듈들 (1단계와 동일)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=52):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=True):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  \n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))\n",
    "\n",
    "        return y\n",
    "\n",
    "print(\"✅ 기본 모듈 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2단계: Dummy + Image + GTrends 인코더\n",
    "class DummyEmbedder(nn.Module):\n",
    "    \"\"\"시간 정보 (날짜) 임베딩\"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.day_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.week_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.month_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.year_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.dummy_fusion = nn.Linear(embedding_dim*4, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, temporal_features):\n",
    "        d, w, m, y = temporal_features[:, 0].unsqueeze(1), temporal_features[:, 1].unsqueeze(1), \\\n",
    "            temporal_features[:, 2].unsqueeze(1), temporal_features[:, 3].unsqueeze(1)\n",
    "        d_emb, w_emb, m_emb, y_emb = self.day_embedding(d), self.week_embedding(w), self.month_embedding(m), self.year_embedding(y)\n",
    "        temporal_embeddings = self.dummy_fusion(torch.cat([d_emb, w_emb, m_emb, y_emb], dim=1))\n",
    "        temporal_embeddings = self.dropout(temporal_embeddings)\n",
    "        return temporal_embeddings\n",
    "\n",
    "class ImageEmbedder(nn.Module):\n",
    "    \"\"\"이미지 특성 추출 (ResNet50 사용)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ResNet50 사전 훈련된 모델 사용\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]  # 마지막 2개 레이어 제거\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        # 모든 ResNet 파라미터를 trainable로 설정\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = True\n",
    "        \n",
    "    def forward(self, images):        \n",
    "        img_embeddings = self.resnet(images)  # [batch_size, 2048, 8, 8]\n",
    "        size = img_embeddings.size()\n",
    "        out = img_embeddings.view(*size[:2],-1)\n",
    "        return out.view(*size).contiguous()  # [batch_size, 2048, 8, 8]\n",
    "\n",
    "class GTrendEmbedder(nn.Module):\n",
    "    \"\"\"Google Trends 데이터 인코딩\"\"\"\n",
    "    def __init__(self, forecast_horizon, embedding_dim, use_mask, trend_len, num_trends, gpu_num):\n",
    "        super().__init__()\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.input_linear = TimeDistributed(nn.Linear(num_trends, embedding_dim))\n",
    "        self.pos_embedding = PositionalEncoding(embedding_dim, max_len=trend_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dropout=0.2)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.use_mask = use_mask\n",
    "        self.gpu_num = gpu_num\n",
    "\n",
    "    def _generate_encoder_mask(self, size, forecast_horizon):\n",
    "        mask = torch.zeros((size, size))\n",
    "        split = math.gcd(size, forecast_horizon)\n",
    "        for i in range(0, size, split):\n",
    "            mask[i:i+split, i:i+split] = 1\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, gtrends):\n",
    "        gtrend_emb = self.input_linear(gtrends.permute(0,2,1))\n",
    "        gtrend_emb = self.pos_embedding(gtrend_emb.permute(1,0,2))\n",
    "        input_mask = self._generate_encoder_mask(gtrend_emb.shape[0], self.forecast_horizon).to(gtrend_emb.device)\n",
    "        if self.use_mask == 1:\n",
    "            gtrend_emb = self.encoder(gtrend_emb, input_mask)\n",
    "        else:\n",
    "            gtrend_emb = self.encoder(gtrend_emb)\n",
    "        return gtrend_emb\n",
    "\n",
    "class FusionNetwork(nn.Module):\n",
    "    \"\"\"2단계: 시간 + 이미지 특성 융합\"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, dropout=0.2):\n",
    "        super(FusionNetwork, self).__init__()\n",
    "        \n",
    "        # 이미지 특성 처리\n",
    "        self.img_pool = nn.AdaptiveAvgPool2d((1,1))  # Global Average Pooling\n",
    "        self.img_linear = nn.Linear(2048, embedding_dim)  # ResNet50 출력 크기: 2048\n",
    "        \n",
    "        # 시간 + 이미지 융합\n",
    "        input_dim = embedding_dim + embedding_dim  # temporal + image\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Linear(input_dim, input_dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_encoding, dummy_encoding):\n",
    "        # 이미지 특성 처리\n",
    "        pooled_img = self.img_pool(img_encoding)  # [batch, 2048, 1, 1]\n",
    "        condensed_img = self.img_linear(pooled_img.flatten(1))  # [batch, embedding_dim]\n",
    "\n",
    "        # 시간 + 이미지 특성 결합\n",
    "        concat_features = torch.cat([dummy_encoding, condensed_img], dim=1)\n",
    "        final = self.feature_fusion(concat_features)\n",
    "        \n",
    "        return final\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \"\"\"커스텀 트랜스포머 디코더 레이어\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, \n",
    "            memory_key_padding_mask=None, tgt_is_causal=None, memory_is_causal=None):\n",
    "        \n",
    "        # Self-attention block\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        # Cross-attention block\n",
    "        tgt2, attn_weights = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                                  key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        # Feedforward block\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        \n",
    "        return tgt, attn_weights\n",
    "\n",
    "print(\"✅ 2단계 인코더 (Dummy + Image + GTrends) 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 🎯 GTM Step 2 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTM_Step2(L.LightningModule):\n",
    "    \"\"\"2단계: Temporal Features + Image Features + Google Trends 사용\"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_heads, num_layers, \n",
    "                 cat_dict, col_dict, fab_dict, trend_len, num_trends, gpu_num, use_encoder_mask=1, autoregressive=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_len = output_dim\n",
    "        self.use_encoder_mask = use_encoder_mask\n",
    "        self.autoregressive = autoregressive\n",
    "        self.gpu_num = gpu_num\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # 2단계에서는 Dummy + Image + GTrend 사용\n",
    "        self.dummy_encoder = DummyEmbedder(embedding_dim)\n",
    "        self.image_encoder = ImageEmbedder()  # 🆕 이미지 인코더 추가\n",
    "        self.gtrend_encoder = GTrendEmbedder(output_dim, hidden_dim, use_encoder_mask, trend_len, num_trends, gpu_num)\n",
    "        \n",
    "        # 2단계 융합 네트워크 (시간 + 이미지)\n",
    "        self.feature_fusion = FusionNetwork(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_layer = TransformerDecoderLayer(d_model=self.hidden_dim, nhead=num_heads, \n",
    "                                                    dim_feedforward=self.hidden_dim * 4, dropout=0.1)\n",
    "        \n",
    "        if self.autoregressive: \n",
    "            self.pos_encoder = PositionalEncoding(hidden_dim, max_len=12)\n",
    "        \n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, self.output_len if not self.autoregressive else 1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, category, color, fabric, temporal_features, gtrends, images):\n",
    "        # 2단계: 시간 정보 + 이미지 정보 사용\n",
    "        dummy_encoding = self.dummy_encoder(temporal_features)\n",
    "        img_encoding = self.image_encoder(images)  # 🆕 이미지 인코딩\n",
    "        gtrend_encoding = self.gtrend_encoder(gtrends)\n",
    "\n",
    "        # Multi-modal 특성 융합 (시간 + 이미지)\n",
    "        static_feature_fusion = self.feature_fusion(img_encoding, dummy_encoding)\n",
    "\n",
    "        # Decoder\n",
    "        tgt = static_feature_fusion.unsqueeze(0)\n",
    "        memory = gtrend_encoding\n",
    "        \n",
    "        decoder_out, attn_weights = self.decoder_layer(tgt, memory)\n",
    "        forecast = self.decoder_fc(decoder_out)\n",
    "\n",
    "        return forecast.view(-1, self.output_len), attn_weights\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adafactor(self.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        item_sales, category, color, fabric, temporal_features, gtrends, images = batch \n",
    "        \n",
    "        temporal_features = temporal_features.requires_grad_(True)\n",
    "        gtrends = gtrends.requires_grad_(True)\n",
    "        images = images.requires_grad_(True)  # 🆕 이미지 gradient 활성화\n",
    "        \n",
    "        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n",
    "        loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        item_sales, category, color, fabric, temporal_features, gtrends, images = batch \n",
    "        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n",
    "        \n",
    "        if not hasattr(self, 'validation_step_outputs'):\n",
    "            self.validation_step_outputs = []\n",
    "        self.validation_step_outputs.append((item_sales.squeeze(), forecasted_sales.squeeze()))\n",
    "        \n",
    "        return item_sales.squeeze(), forecasted_sales.squeeze()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if hasattr(self, 'validation_step_outputs'):\n",
    "            val_step_outputs = self.validation_step_outputs\n",
    "            item_sales, forecasted_sales = [x[0] for x in val_step_outputs], [x[1] for x in val_step_outputs]\n",
    "            item_sales, forecasted_sales = torch.stack(item_sales), torch.stack(forecasted_sales)\n",
    "            rescaled_item_sales, rescaled_forecasted_sales = item_sales*1065, forecasted_sales*1065\n",
    "            loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
    "            mae = F.l1_loss(rescaled_item_sales, rescaled_forecasted_sales)\n",
    "            \n",
    "            self.log('val_mae', mae, prog_bar=True)\n",
    "            self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "            print(f'Step 2 - Validation MAE: {mae.detach().cpu().numpy():.2f}, LR: {self.optimizers().param_groups[0][\"lr\"]:.2e}')\n",
    "            self.validation_step_outputs.clear()\n",
    "\n",
    "print(\"✅ GTM Step 2 모델 정의 완료 (Temporal + Image + GTrends)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 📊 데이터셋 클래스 (1단계와 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotDataset():\n",
    "    def __init__(self, data_df, img_root, gtrends, cat_dict, col_dict, fab_dict, trend_len):\n",
    "        self.data_df = data_df\n",
    "        self.gtrends = gtrends\n",
    "        self.cat_dict = cat_dict\n",
    "        self.col_dict = col_dict\n",
    "        self.fab_dict = fab_dict\n",
    "        self.trend_len = trend_len\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_df.iloc[idx, :]\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        data = self.data_df\n",
    "\n",
    "        gtrends, image_features = [], []\n",
    "        img_transforms = Compose([Resize((256, 256)), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "        for (idx, row) in tqdm(data.iterrows(), total=len(data), ascii=True, desc=\"2단계 데이터 전처리\"):\n",
    "            cat, col, fab, fiq_attr, start_date, img_path = row['category'], row['color'], row['fabric'], row['extra'], \\\n",
    "                row['release_date'], row['image_path']\n",
    "\n",
    "            # Google Trends 데이터 처리\n",
    "            gtrend_start = start_date - pd.DateOffset(weeks=52)\n",
    "            cat_gtrend = self.gtrends.loc[gtrend_start:start_date][cat][-52:].values[:self.trend_len]\n",
    "            col_gtrend = self.gtrends.loc[gtrend_start:start_date][col][-52:].values[:self.trend_len]\n",
    "            fab_gtrend = self.gtrends.loc[gtrend_start:start_date][fab][-52:].values[:self.trend_len]\n",
    "\n",
    "            cat_gtrend = MinMaxScaler().fit_transform(cat_gtrend.reshape(-1,1)).flatten()\n",
    "            col_gtrend = MinMaxScaler().fit_transform(col_gtrend.reshape(-1,1)).flatten()\n",
    "            fab_gtrend = MinMaxScaler().fit_transform(fab_gtrend.reshape(-1,1)).flatten()\n",
    "            multitrends = np.vstack([cat_gtrend, col_gtrend, fab_gtrend])\n",
    "\n",
    "            # 이미지 처리 (2단계에서는 실제로 사용)\n",
    "            img = Image.open(os.path.join(self.img_root, img_path)).convert('RGB')\n",
    "\n",
    "            gtrends.append(multitrends)\n",
    "            image_features.append(img_transforms(img))\n",
    "\n",
    "        gtrends = np.array(gtrends)\n",
    "\n",
    "        data = data.copy()\n",
    "        data.drop(['external_code', 'season', 'release_date', 'image_path'], axis=1, inplace=True)\n",
    "\n",
    "        # 텐서 생성\n",
    "        item_sales, temporal_features = torch.FloatTensor(data.iloc[:, :12].values), torch.FloatTensor(\n",
    "            data.iloc[:, 13:17].values)\n",
    "        categories, colors, fabrics = [self.cat_dict[val] for val in data.iloc[:].category.values], \\\n",
    "                                       [self.col_dict[val] for val in data.iloc[:].color.values], \\\n",
    "                                       [self.fab_dict[val] for val in data.iloc[:].fabric.values]\n",
    "\n",
    "        categories, colors, fabrics = torch.LongTensor(categories), torch.LongTensor(colors), torch.LongTensor(fabrics)\n",
    "        gtrends = torch.FloatTensor(gtrends)\n",
    "        images = torch.stack(image_features)\n",
    "\n",
    "        return TensorDataset(item_sales, categories, colors, fabrics, temporal_features, gtrends, images)\n",
    "\n",
    "    def get_loader(self, batch_size, train=True):\n",
    "        print('📊 2단계 데이터셋 생성 시작...')\n",
    "        data_with_gtrends = self.preprocess_data()\n",
    "        if train:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        else:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=1, shuffle=False, num_workers=2)\n",
    "        print('✅ 2단계 데이터셋 생성 완료')\n",
    "        return data_loader\n",
    "\n",
    "print(\"✅ 데이터셋 클래스 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 🚀 2단계 실행 코드\n",
    "### 이미지 정보까지 포함한 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로 설정\n",
    "dataset_path = Path('/content/drive/MyDrive/GTM-dataset-small/')\n",
    "\n",
    "# 데이터 로딩\n",
    "print(\"📊 데이터 로딩 중...\")\n",
    "train_df = pd.read_csv(dataset_path / 'train.csv', parse_dates=['release_date'])\n",
    "test_df = pd.read_csv(dataset_path / 'test.csv', parse_dates=['release_date'])\n",
    "gtrends = pd.read_csv(dataset_path / 'gtrends.csv', index_col=[0], parse_dates=True)\n",
    "\n",
    "cat_dict = torch.load(dataset_path / 'category_labels.pt', weights_only=False)\n",
    "col_dict = torch.load(dataset_path / 'color_labels.pt', weights_only=False)\n",
    "fab_dict = torch.load(dataset_path / 'fabric_labels.pt', weights_only=False)\n",
    "\n",
    "print(f\"✅ 훈련 데이터: {len(train_df):,}개\")\n",
    "print(f\"✅ 테스트 데이터: {len(test_df):,}개\")\n",
    "print(f\"✅ Google Trends: {len(gtrends):,}개 시점\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "train_dataset = ZeroShotDataset(train_df, dataset_path / 'images', gtrends, cat_dict, col_dict, fab_dict, trend_len=52)\n",
    "test_dataset = ZeroShotDataset(test_df, dataset_path / 'images', gtrends, cat_dict, col_dict, fab_dict, trend_len=52)\n",
    "\n",
    "BATCH_SIZE = 8 if torch.cuda.is_available() else 4\n",
    "train_loader = train_dataset.get_loader(batch_size=BATCH_SIZE, train=True)\n",
    "test_loader = test_dataset.get_loader(batch_size=1, train=False)\n",
    "\n",
    "print(f\"✅ 배치 크기: {BATCH_SIZE}\")\n",
    "print(f\"✅ 훈련 배치 수: {len(train_loader)}\")\n",
    "print(f\"✅ 테스트 배치 수: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2단계 모델 생성\n",
    "print(\"🎯 GTM Step 2 모델 생성 중...\")\n",
    "\n",
    "model = GTM_Step2(\n",
    "    embedding_dim=32,\n",
    "    hidden_dim=64,\n",
    "    output_dim=12,\n",
    "    num_heads=4,\n",
    "    num_layers=1,\n",
    "    cat_dict=cat_dict,\n",
    "    col_dict=col_dict,\n",
    "    fab_dict=fab_dict,\n",
    "    trend_len=52,\n",
    "    num_trends=3,\n",
    "    gpu_num=0,\n",
    "    use_encoder_mask=1,\n",
    "    autoregressive=False\n",
    ")\n",
    "\n",
    "print(f\"✅ Step 2 모델 생성 완료!\")\n",
    "print(f\"📊 모델 파라미터: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"\\n🔍 사용 모달리티: Temporal Features + Image Features + Google Trends\")\n",
    "print(\"🆕 추가된 기능: ResNet50 기반 이미지 특성 추출\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 설정 및 훈련\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "EPOCHS = 5\n",
    "ACCELERATOR = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='./checkpoints/',\n",
    "    filename='gtm-step2-{epoch:02d}-{val_mae:.2f}',\n",
    "    monitor='val_mae',\n",
    "    mode='min',\n",
    "    save_top_k=2\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='./logs/', name='gtm_step2')\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=ACCELERATOR,\n",
    "    max_epochs=EPOCHS,\n",
    "    logger=csv_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "print(\"🚀 GTM Step 2 훈련 시작!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)\n",
    "    print(\"\\n🎉 Step 2 훈련 완료!\")\n",
    "    print(f\"💾 최고 모델: {checkpoint_callback.best_model_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Step 2 훈련 실패: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 2단계 요약\n",
    "\n",
    "### ✅ 구현 완료\n",
    "- **시간적 특성 임베딩**: 1단계와 동일 (날짜 정보)\n",
    "- **🆕 이미지 특성 추출**: ResNet50으로 제품 이미지 분석\n",
    "- **Multi-modal 융합**: 시간 + 시각적 정보 결합\n",
    "- **Google Trends 인코딩**: 1단계와 동일\n",
    "- **Cross-Attention**: 융합된 특성과 트렌드 데이터 간 관계 학습\n",
    "\n",
    "### 🎯 학습 목표 달성\n",
    "- CNN을 통한 이미지 특성 추출 이해\n",
    "- Multi-modal 데이터 융합 기법 학습\n",
    "- 전이학습 (Transfer Learning) 활용\n",
    "- 시각적 정보가 매출 예측에 미치는 영향 분석\n",
    "\n",
    "### 📈 성능 향상 포인트\n",
    "- **1단계 대비**: 이미지 정보 추가로 예측 정확도 향상 기대\n",
    "- **ResNet50 활용**: 사전 훈련된 모델로 강력한 시각적 특성 추출\n",
    "- **Feature Fusion**: 다양한 모달리티 정보의 효과적 결합\n",
    "\n",
    "### 🔜 다음 단계 예고\n",
    "**Step 3**에서는 **텍스트 정보**까지 추가하여 완전한 Multi-modal 시스템을 완성합니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}