{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeSaeBom/GTM-Transformer-Jupyter/blob/main/GTM_Step1_Dummy_Only_%EA%B0%95%EC%9D%98%EC%9A%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOdUqdq8HNQB"
      },
      "source": [
        "# ğŸš€ GTM Step 1: Temporal Features + Google Trends\n",
        "\n",
        "## ğŸ“š íŠ¹ê°• 1ë‹¨ê³„: ê¸°ë³¸ ì‹œê³„ì—´ ì˜ˆì¸¡\n",
        "- **ì‚¬ìš© ëª¨ë‹¬ë¦¬í‹°**: Temporal Features (ë‚ ì§œ ì •ë³´) + Google Trends\n",
        "- **ëª©ì **: ì‹œê³„ì—´ ë°ì´í„°ë§Œìœ¼ë¡œ ë§¤ì¶œ ì˜ˆì¸¡ì˜ ê¸°ì´ˆ êµ¬í˜„\n",
        "- **í•™ìŠµ ëª©í‘œ**:\n",
        "  - Transformer ê¸°ë³¸ êµ¬ì¡° ì´í•´\n",
        "  - ì‹œê³„ì—´ ì¸ì½”ë”© (Positional Encoding)\n",
        "  - Google Trends ë°ì´í„° í™œìš©"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gl57um5HNQC"
      },
      "source": [
        "## 1. ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR9ZTTFOHNQC",
        "outputId": "48bbb795-fee5-4443-a1d4-2b94210e7049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.3/363.4 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
          ]
        }
      ],
      "source": [
        "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install lightning --upgrade --quiet\n",
        "!pip install transformers scikit-learn pillow --quiet\n",
        "\n",
        "# Import\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import lightning as L\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageFile\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.transforms import Resize, ToTensor, Normalize, Compose\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from transformers import Adafactor\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# Google Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(f\" PyTorch: {torch.__version__}\")\n",
        "print(f\" Lightning: {L.__version__}\")\n",
        "print(f\" CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\" GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kn6FjsVHNQC"
      },
      "source": [
        "## 2. ğŸ§  ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ì •ì˜\n",
        "### 1ë‹¨ê³„ì—ì„œëŠ” ìµœì†Œí•œì˜ ì»´í¬ë„ŒíŠ¸ë§Œ ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¸°ë³¸ ëª¨ë“ˆë“¤\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=52):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # ë“œë¡­ì•„ì›ƒ: ìœ„ì¹˜ ì„ë² ë”©ì´ ì¶”ê°€ëœ í›„ ê³¼ì í•©ì„ ì¤„ì´ê¸° ìœ„í•œ ì •ê·œí™”\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # pe: [max_len, d_model] í¬ê¸°ì˜ ìœ„ì¹˜ ì„ë² ë”© í…Œì´ë¸”(ê³ ì •ê°’, í•™ìŠµë˜ì§€ ì•ŠìŒ)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # position: [max_len, 1] í˜•íƒœë¡œ 0 ~ max_len-1ê¹Œì§€ì˜ ì •ìˆ˜ ìœ„ì¹˜\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # div_term: ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼(ì§€ìˆ˜ì ìœ¼ë¡œ ì»¤ì§€ëŠ” ê°„ê²©)\n",
        "        #   - ë…¼ë¬¸ \"Attention Is All You Need\"ì˜ ì‚¬ì¸/ì½”ì‚¬ì¸ ìœ„ì¹˜ ì¸ì½”ë”© ê³µì‹ì„ ê·¸ëŒ€ë¡œ êµ¬í˜„\n",
        "        #   - d_modelì˜ ì§ìˆ˜ ì¸ë±ìŠ¤ì— ëŒ€ì‘í•˜ëŠ” ì£¼íŒŒìˆ˜ë“¤ë§Œ ê³„ì‚°\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # ì§ìˆ˜ ì±„ë„(0,2,4,...)ì—ëŠ” sin, í™€ìˆ˜ ì±„ë„(1,3,5,...)ì—ëŠ” cosë¥¼ ì±„ìš´ë‹¤.\n",
        "        # position * div_termì˜ ë¸Œë¡œë“œìºìŠ¤íŒ…ìœ¼ë¡œ [max_len, d_model/2]ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ë§Œë“  ë’¤ í• ë‹¹\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd  indices\n",
        "\n",
        "        # Transformerì˜ ê¸°ë³¸ ì…ë ¥ í˜•íƒœ(S, N, E = seq_len, batch, embed)ì— ë§ì¶”ê¸° ìœ„í•´\n",
        "        # peë¥¼ [max_len, 1, d_model]ë¡œ ë°”ê¿” ë°°ì¹˜ ì°¨ì›ìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŒ…ë˜ê²Œ ë§Œë“ ë‹¤.\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)  # [1, max_len, d_model] -> [max_len, 1, d_model]\n",
        "\n",
        "        # register_buffer: í•™ìŠµ íŒŒë¼ë¯¸í„°ëŠ” ì•„ë‹ˆì§€ë§Œ ëª¨ë¸ê³¼ í•¨ê»˜ ë””ë°”ì´ìŠ¤ ì´ë™/ì €ì¥ë˜ë„ë¡ ë“±ë¡\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [seq_len, batch_size, d_model] í˜•íƒœë¥¼ ê¸°ëŒ€ (PyTorch nn.Transformer í‘œì¤€)\n",
        "        # í˜„ì¬ ì‹œí€€ìŠ¤ ê¸¸ì´(seq_len)ì— í•´ë‹¹í•˜ëŠ” ìœ„ì¹˜ ì„ë² ë”©ì„ ì˜ë¼ ë”í•œë‹¤.\n",
        "        # self.pe[:x.size(0), :] -> [seq_len, 1, d_model] ì´ê³  ë°°ì¹˜ ì°¨ì›ìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŒ…ë¨\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)  # ìœ„ì¹˜ ì¸ì½”ë”©ì´ ë”í•´ì§„ í›„ ë“œë¡­ì•„ì›ƒ ì ìš©"
      ],
      "metadata": {
        "id": "i-ZYkybguPpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeDistributed(nn.Module):\n",
        "    def __init__(self, module, batch_first=True):\n",
        "        super(TimeDistributed, self).__init__()\n",
        "        # module: ì‹œê°„ì¶• ê° ìŠ¤í…ì— ë˜‘ê°™ì´ ì ìš©í•  í•˜ìœ„ ëª¨ë“ˆ\n",
        "        #   ì˜ˆ: nn.Linear, ì‘ì€ CNN, MLP ë“±\n",
        "        self.module = module\n",
        "        # batch_first:\n",
        "        #   True  â†’ ì…ë ¥ í˜•íƒœê°€ [batch, time, feature]\n",
        "        #   False â†’ ì…ë ¥ í˜•íƒœê°€ [time, batch, feature]\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (1) ì…ë ¥ì´ 2ì°¨ì› ì´í•˜ì´ë©´ (ì‹œê°„ì¶•ì´ ì—†ìœ¼ë©´)\n",
        "        # ì˜ˆ: [batch, feature] ë˜ëŠ” [time, feature]\n",
        "        # ê·¸ëƒ¥ moduleì— ë°”ë¡œ ë„£ì–´ì„œ ì²˜ë¦¬\n",
        "        if len(x.size()) <= 2:\n",
        "            return self.module(x)\n",
        "\n",
        "        # (2) ì‹œê°„ì¶•ì´ ìˆëŠ” ê²½ìš° â†’ ì‹œê°„ì¶•ê³¼ ë°°ì¹˜ì¶•ì„ í•©ì³ì„œ í•œ ë²ˆì— ì²˜ë¦¬\n",
        "        # ì˜ˆ: batch_first=True  -> [B, T, F] â†’ [B*T, F]\n",
        "        #     batch_first=False -> [T, B, F] â†’ [T*B, F]\n",
        "        x_reshape = x.contiguous().view(-1, x.size(-1))\n",
        "        # contiguous(): ë©”ëª¨ë¦¬ ìƒì—ì„œ ì—°ì†ì ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜ì„œ view()ê°€ ì•ˆì „í•˜ê²Œ ë™ì‘í•˜ë„ë¡ í•¨\n",
        "\n",
        "        # (3) í‰íƒ„í™”ëœ ë°ì´í„°ë¥¼ moduleì— í†µê³¼\n",
        "        # ì´ë ‡ê²Œ í•˜ë©´ forë¬¸ ì—†ì´ í•œ ë²ˆì— ëª¨ë“  ì‹œì  ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥\n",
        "        y = self.module(x_reshape)  # shape: [B*T, F_out] ë˜ëŠ” [T*B, F_out]\n",
        "\n",
        "        # (4) ì›ë˜ ëª¨ì–‘ìœ¼ë¡œ ë³µì›\n",
        "        if self.batch_first:\n",
        "            # batch_first=True â†’ [B, T, F_out] í˜•íƒœë¡œ ë³µì›\n",
        "            y = y.contiguous().view(x.size(0), -1, y.size(-1))\n",
        "        else:\n",
        "            # batch_first=False â†’ [T, B, F_out] í˜•íƒœë¡œ ë³µì›\n",
        "            y = y.view(-1, x.size(1), y.size(-1))\n",
        "\n",
        "        # (5) ìµœì¢… ê²°ê³¼ ë°˜í™˜\n",
        "        return y\n",
        "\n",
        "\n",
        "print(\" ê¸°ë³¸ ëª¨ë“ˆ ì •ì˜ ì™„ë£Œ\")\n"
      ],
      "metadata": {
        "id": "xlTLPA4EuRrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1ë‹¨ê³„: Dummy (ì‹œê°„) + GTrends ì¸ì½”ë”ë§Œ ì‚¬ìš©\n",
        "class DummyEmbedder(nn.Module):\n",
        "    \"\"\"ì‹œê°„ ì •ë³´(ì¼/ì£¼/ì›”/ì—°)ë¥¼ ê°ê° ì„ë² ë”©í•œ ë’¤ í•˜ë‚˜ë¡œ í•©ì³ ë‹¨ì¼ ì„ë² ë”©ìœ¼ë¡œ íˆ¬ì˜\"\"\"\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        # ê° ìŠ¤ì¹¼ë¼ ì‹œê°„ê°’(ì¼/ì£¼/ì›”/ì—°: ëª¨ë‘ 1ì°¨ì›)ì„ ì„ë² ë”© ì°¨ì›ìœ¼ë¡œ ì„ í˜• ì‚¬ìƒ\n",
        "        self.day_embedding   = nn.Linear(1, embedding_dim)\n",
        "        self.week_embedding  = nn.Linear(1, embedding_dim)\n",
        "        self.month_embedding = nn.Linear(1, embedding_dim)\n",
        "        self.year_embedding  = nn.Linear(1, embedding_dim)\n",
        "        # 4ê°œ ì„ë² ë”©ì„ concatí•˜ì—¬(embedding_dim*4) ë‹¤ì‹œ embedding_dimìœ¼ë¡œ ì¶•ì†Œ\n",
        "        self.dummy_fusion = nn.Linear(embedding_dim*4, embedding_dim)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, temporal_features):\n",
        "        \"\"\"\n",
        "        temporal_features: [B, 4] ê°€ì • (ì»¬ëŸ¼: day, week, month, year ìˆœì„œ)\n",
        "          - ê° ì»¬ëŸ¼ì€ ìŠ¤ì¹¼ë¼ì´ë¯€ë¡œ Linear ì…ë ¥ì„ ìœ„í•´ [B, 1]ë¡œ í™•ì¥\n",
        "        ë°˜í™˜: temporal_embeddings âˆˆ â„[B, embedding_dim]\n",
        "        \"\"\"\n",
        "        # ê° ì‹œê°„ ì„±ë¶„ ë¶„ë¦¬ + [B, 1]ë¡œ ì°¨ì› í™•ì¥\n",
        "        d, w, m, y = temporal_features[:, 0].unsqueeze(1), temporal_features[:, 1].unsqueeze(1), \\\n",
        "                     temporal_features[:, 2].unsqueeze(1), temporal_features[:, 3].unsqueeze(1)\n",
        "\n",
        "        # ê°ê° ì„ í˜• ì„ë² ë”©\n",
        "        d_emb = self.day_embedding(d)     # [B, D]\n",
        "        w_emb = self.week_embedding(w)    # [B, D]\n",
        "        m_emb = self.month_embedding(m)   # [B, D]\n",
        "        y_emb = self.year_embedding(y)    # [B, D]\n",
        "\n",
        "        # concat í›„ ì°¨ì› ì¶•ì†Œ(ìœµí•©)\n",
        "        temporal_embeddings = self.dummy_fusion(torch.cat([d_emb, w_emb, m_emb, y_emb], dim=1))  # [B, D]\n",
        "        temporal_embeddings = self.dropout(temporal_embeddings)\n",
        "        return temporal_embeddings"
      ],
      "metadata": {
        "id": "wo8Ijl_nzL52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GTrendEmbedder(nn.Module):\n",
        "    \"\"\"Google Trends ë°ì´í„° ì¸ì½”ë”©\"\"\"\n",
        "    def __init__(self, forecast_horizon, embedding_dim, use_mask, trend_len, num_trends, gpu_num):\n",
        "        super().__init__()\n",
        "        # ì˜ˆì¸¡ ì§€í‰(horizon): ë§ˆìŠ¤í¬ ìƒì„± ì‹œ ë¸”ë¡ í¬ê¸° ê²°ì •ì— ì‚¬ìš©\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "\n",
        "        # ì‹œì ë³„ ì…ë ¥(ê¸¸ì´ num_trends ë²¡í„°)ì„ embedding_dimìœ¼ë¡œ íˆ¬ì˜\n",
        "        # ê¸°ëŒ€ ì…ë ¥: [B, T, num_trends]  â†’ ì¶œë ¥: [B, T, embedding_dim]\n",
        "        # (forwardì—ì„œ permuteë¡œ [B, num_trends, T]ë¥¼ [B, T, num_trends]ë¡œ ë°”ê¾¼ ë’¤ ì‚¬ìš©)\n",
        "        self.input_linear = TimeDistributed(nn.Linear(num_trends, embedding_dim))\n",
        "\n",
        "        # ìœ„ì¹˜ ì¸ì½”ë”©: Transformerê°€ ìˆœì„œë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ ì¶”ê°€\n",
        "        # PositionalEncodingì€ [S, N, E] (seq, batch, embed) í˜•íƒœë¥¼ ê¸°ëŒ€í•¨\n",
        "        # trend_lenì€ ì‚¬ìš©í•  ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´(â‰¥ ì‹¤ì œ T)ë¡œ ì„¤ì •í•´ì•¼ í•¨\n",
        "        self.pos_embedding = PositionalEncoding(embedding_dim, max_len=trend_len)\n",
        "\n",
        "        # ì‹œê³„ì—´ ë‚´ ìƒê´€ê´€ê³„ í•™ìŠµì„ ìœ„í•œ Transformer Encoder (2ì¸µ)\n",
        "        # d_model=embedding_dim, nhead=4, dropout=0.2\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dropout=0.2)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "\n",
        "        # ë§ˆìŠ¤í¬ ì‚¬ìš© ì—¬ë¶€(1: ì‚¬ìš©, ê·¸ ì™¸: ë¯¸ì‚¬ìš©)\n",
        "        self.use_mask = use_mask\n",
        "\n",
        "        # (ì°¸ê³ ) í˜„ì¬ ì½”ë“œì—ì„œ gpu_numì€ ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ\n",
        "        self.gpu_num = gpu_num\n",
        "\n",
        "    def _generate_encoder_mask(self, size, forecast_horizon):\n",
        "        # ì–´í…ì…˜ ë§ˆìŠ¤í¬(ì¶”ê°€ ê°€ì¤‘ì¹˜ ë°©ì‹, additive mask) ìƒì„±\n",
        "        # - ì…ë ¥/ì¶œë ¥ shape: [size, size] (size = ì‹œí€€ìŠ¤ ê¸¸ì´ T)\n",
        "        # - split = gcd(T, horizon)ë¡œ ëŒ€ê°ì„  ë¸”ë¡ì„ ë§Œë“¤ê³  ë¸”ë¡ ë‚´ë¶€ë§Œ ì–´í…ì…˜ í—ˆìš©\n",
        "        # - PyTorch ê·œì•½: í—ˆìš©=0.0, ì°¨ë‹¨=-inf (ê°€ì¤‘ì¹˜ì— ë”í•´ì ¸ softmaxì—ì„œ ë¬´ì‹œë¨)\n",
        "        mask = torch.zeros((size, size))\n",
        "        split = math.gcd(size, forecast_horizon)\n",
        "        for i in range(0, size, split):\n",
        "            mask[i:i+split, i:i+split] = 1\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, gtrends):\n",
        "        # gtrends ì˜ˆìƒ ì…ë ¥: [B, num_trends, T]  (ì±„ë„ ìš°ì„ )\n",
        "        # TimeDistributed ì ìš©ì„ ìœ„í•´ [B, T, num_trends]ë¡œ ë³€í™˜\n",
        "        gtrend_emb = self.input_linear(gtrends.permute(0,2,1))   # â†’ [B, T, D]\n",
        "\n",
        "        # PositionalEncodingì€ [S, N, E] í˜•íƒœë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ [T, B, D]ë¡œ ë³€í™˜í•˜ì—¬ ì ìš©\n",
        "        gtrend_emb = self.pos_embedding(gtrend_emb.permute(1,0,2))  # â†’ [T, B, D]\n",
        "\n",
        "        # ì‹œí€€ìŠ¤ ê¸¸ì´ Tì— ë§ì¶° ë¸”ë¡ ëŒ€ê°ì„  ë§ˆìŠ¤í¬ ìƒì„± (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€/ì£¼ê¸° ë°˜ì˜)\n",
        "        input_mask = self._generate_encoder_mask(gtrend_emb.shape[0], self.forecast_horizon).to(gtrend_emb.device)\n",
        "\n",
        "        # ë§ˆìŠ¤í¬ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ Encoderì— ì „ë‹¬\n",
        "        if self.use_mask == 1:\n",
        "            gtrend_emb = self.encoder(gtrend_emb, input_mask)   # [T, B, D] (ë§ˆìŠ¤í¬ ì ìš©)\n",
        "        else:\n",
        "            gtrend_emb = self.encoder(gtrend_emb)               # [T, B, D] (ì „ì²´ ì–´í…ì…˜)\n",
        "\n",
        "        # ë°˜í™˜: ì‹œí€€ìŠ¤ ìš°ì„  í…ì„œ [T, B, D]\n",
        "        return gtrend_emb"
      ],
      "metadata": {
        "id": "AGGPSQw2z38X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"ì»¤ìŠ¤í…€ íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” ë ˆì´ì–´\n",
        "    - êµ¬ì„±: (1) ë””ì½”ë” ìê¸°ì–´í…ì…˜ â†’ (2) ì¸ì½”ë”-ë””ì½”ë”(í¬ë¡œìŠ¤) ì–´í…ì…˜ â†’ (3) ìœ„ì¹˜ë³„ FFN\n",
        "    - ê° ë¸”ë¡ë§ˆë‹¤: ì”ì°¨ ì—°ê²°(Residual) + LayerNorm + Dropout\n",
        "    - ê¸°ë³¸ í…ì„œ í˜•ìƒ(ê¸°ë³¸ê°’ batch_first=False ê°€ì •):\n",
        "        tgt    : [T_tgt, B, D]   (ë””ì½”ë” ì…ë ¥/ì´ì „ ë‹¨ê³„ ì¶œë ¥)\n",
        "        memory : [S_src, B, D]   (ì¸ì½”ë” ì¶œë ¥)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "\n",
        "        # (1) ë””ì½”ë” ìê¸°ì–´í…ì…˜: ë””ì½”ë”ì˜ í˜„ì¬ í† í°ë“¤ì´ ìê¸° ìì‹  ì‹œí€€ìŠ¤ë¥¼ ì°¸ì¡°\n",
        "        #  - ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ(ë¯¸ë˜ ì°¨ë‹¨)ë¡œ ì“°ë ¤ë©´ forwardì—ì„œ tgt_mask(í˜¹ì€ causal) ì „ë‹¬\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        # (2) ì¸ì½”ë”-ë””ì½”ë”(í¬ë¡œìŠ¤) ì–´í…ì…˜:\n",
        "        #  - ë””ì½”ë”ê°€ ì¸ì½”ë”ì˜ ì •ë³´ë¥¼ ëŒì–´ì™€ í˜„ì¬ í† í°ì„ ë” ì •í™•íˆ ì˜ˆì¸¡\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        # (3) ìœ„ì¹˜ë³„ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬(FFN): ê° ì‹œì ì˜ í”¼ì²˜ë¥¼ ë¹„ì„ í˜• ë³€í™˜\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ (ê° ì„œë¸Œë ˆì´ì–´ ë’¤ì— ì‚¬ìš©)\n",
        "        self.norm1 = nn.LayerNorm(d_model)  # self-attn ë’¤\n",
        "        self.norm2 = nn.LayerNorm(d_model)  # cross-attn ë’¤\n",
        "        self.norm3 = nn.LayerNorm(d_model)  # FFN ë’¤\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        # í™œì„±í•¨ìˆ˜ (í•„ìš”ì‹œ GELU ë“±ìœ¼ë¡œ êµì²´ ê°€ëŠ¥)\n",
        "        self.activation = F.relu\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None,\n",
        "            memory_key_padding_mask=None, tgt_is_causal=None, memory_is_causal=None):\n",
        "        \"\"\"\n",
        "        ì¸ì ì„¤ëª…:\n",
        "          - tgt : ë””ì½”ë” ì…ë ¥/ì´ì „ ë””ì½”ë” ë ˆì´ì–´ ì¶œë ¥  (shape: [T_tgt, B, D])\n",
        "          - memory : ì¸ì½”ë” ì¶œë ¥(ì†ŒìŠ¤ ì‹œí€€ìŠ¤ ì¸ì½”ë”© ê²°ê³¼) (shape: [S_src, B, D])\n",
        "          - tgt_mask : ë””ì½”ë” ìê¸°ì–´í…ì…˜ìš© attn mask (ì˜ˆ: ë¯¸ë˜ ì°¨ë‹¨ìš© causal mask)\n",
        "          - memory_mask : ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ìš© mask (íŠ¹ì • ì†ŒìŠ¤ ìœ„ì¹˜ ì°¨ë‹¨ ë“±)\n",
        "          - *_key_padding_mask : íŒ¨ë”© í† í°(True=ë¬´ì‹œ) ê°€ë¦¬ê¸° ìœ„í•œ ë§ˆìŠ¤í¬ (batch ì°¨ì› ê¸°ì¤€)\n",
        "\n",
        "        ë°˜í™˜:\n",
        "          - tgt : í˜„ì¬ ë””ì½”ë” ë ˆì´ì–´ì˜ ì¶œë ¥ (shape: [T_tgt, B, D])\n",
        "          - attn_weights : í¬ë¡œìŠ¤ ì–´í…ì…˜ ê°€ì¤‘ì¹˜(ë””ë²„ê¹…/ê°€ì‹œí™” ìš©)\n",
        "        \"\"\"\n",
        "\n",
        "        # ---------------------------\n",
        "        # (A) ë””ì½”ë” ìê¸°ì–´í…ì…˜ ë¸”ë¡\n",
        "        #  - Query/Key/Value ëª¨ë‘ tgt\n",
        "        #  - ì¼ë°˜ì ìœ¼ë¡œ tgt_maskë¡œ ë¯¸ë˜ í† í° ì°¨ë‹¨(causal)í•˜ê±°ë‚˜,\n",
        "        #    tgt_key_padding_maskë¡œ íŒ¨ë”© ìœ„ì¹˜ ë¬´ì‹œ\n",
        "        # ---------------------------\n",
        "        tgt2 = self.self_attn(\n",
        "            tgt, tgt, tgt,\n",
        "            attn_mask=tgt_mask,\n",
        "            key_padding_mask=tgt_key_padding_mask\n",
        "        )[0]  # ë°˜í™˜: (attn_output, attn_weights); ì—¬ê¸°ì„  ì¶œë ¥ë§Œ ì‚¬ìš©\n",
        "        tgt = tgt + self.dropout1(tgt2)  # ì”ì°¨ ì—°ê²°\n",
        "        tgt = self.norm1(tgt)            # ì •ê·œí™”\n",
        "\n",
        "        # ---------------------------\n",
        "        # (B) ì¸ì½”ë”-ë””ì½”ë”(í¬ë¡œìŠ¤) ì–´í…ì…˜ ë¸”ë¡\n",
        "        #  - Query=tgt, Key/Value=memory(ì¸ì½”ë” ì¶œë ¥)\n",
        "        #  - memory_maskë¡œ íŠ¹ì • ì†ŒìŠ¤ ìœ„ì¹˜ ì°¨ë‹¨ ê°€ëŠ¥,\n",
        "        #    memory_key_padding_maskë¡œ ì†ŒìŠ¤ íŒ¨ë”© ë¬´ì‹œ\n",
        "        # ---------------------------\n",
        "        tgt2, attn_weights = self.multihead_attn(\n",
        "            tgt, memory, memory,\n",
        "            attn_mask=memory_mask,\n",
        "            key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "        tgt = tgt + self.dropout2(tgt2)  # ì”ì°¨ ì—°ê²°\n",
        "        tgt = self.norm2(tgt)            # ì •ê·œí™”\n",
        "\n",
        "        # ---------------------------\n",
        "        # (C) ìœ„ì¹˜ë³„ FFN ë¸”ë¡\n",
        "        #  - ê° ì‹œì (feature ë²¡í„°)ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë˜ëŠ” 2ì¸µ MLP\n",
        "        # ---------------------------\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)  # ì”ì°¨ ì—°ê²°\n",
        "        tgt = self.norm3(tgt)            # ì •ê·œí™”\n",
        "\n",
        "        # ë””ì½”ë” ë ˆì´ì–´ ì¶œë ¥ê³¼, í¬ë¡œìŠ¤ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ë°˜í™˜\n",
        "        return tgt, attn_weights\n",
        "\n",
        "print(\"1ë‹¨ê³„ ì¸ì½”ë” (Dummy + GTrends) ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "q3nHSo2y-K4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5gkl4dMHNQD"
      },
      "source": [
        "## 3. ğŸ¯ GTM Step 1 ëª¨ë¸"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GTM_Step1(L.LightningModule):\n",
        "    \"\"\"1ë‹¨ê³„: Temporal Features(ì‹œê°„ íŠ¹ì„±) + Google Trends ë°ì´í„°ë§Œ ì‚¬ìš©í•´ì„œ íŒë§¤ ì˜ˆì¸¡\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_heads, num_layers,\n",
        "                 cat_dict, col_dict, fab_dict, trend_len, num_trends, gpu_num,\n",
        "                 use_encoder_mask=1, autoregressive=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥\n",
        "        self.hidden_dim = hidden_dim            # ë‚´ë¶€ ì²˜ë¦¬ ì°¨ì› (ì€ë‹‰ ë²¡í„° í¬ê¸°)\n",
        "        self.embedding_dim = embedding_dim      # ì„ë² ë”© ì°¨ì› (ì‹œê°„ íŠ¹ì„± ì„ë² ë”© í¬ê¸°)\n",
        "        self.output_len = output_dim            # ì˜ˆì¸¡í•  ì‹œì  ê°œìˆ˜\n",
        "        self.use_encoder_mask = use_encoder_mask# GTrends ì¸ì½”ë” ë§ˆìŠ¤í¬ ì‚¬ìš© ì—¬ë¶€\n",
        "        self.autoregressive = autoregressive    # ìê¸°íšŒê·€ ì˜ˆì¸¡ ëª¨ë“œ ì—¬ë¶€\n",
        "        self.gpu_num = gpu_num                  # ì‚¬ìš© GPU ë²ˆí˜¸ (ë°ì´í„° ì´ë™ ì‹œ ì‚¬ìš©)\n",
        "        self.save_hyperparameters()             # Lightningì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ì €ì¥\n",
        "\n",
        "        # (1) ì‹œê°„ íŠ¹ì„± ì¸ì½”ë” â€” ë‚ ì§œ ê´€ë ¨ ìŠ¤ì¹¼ë¼(day/week/month/year)ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
        "        self.dummy_encoder = DummyEmbedder(embedding_dim)\n",
        "\n",
        "        # (2) Google Trends ì¸ì½”ë” â€” ì‹œê³„ì—´ ë°ì´í„°ë¥¼ Transformer ì¸ì½”ë”ë¡œ ë³€í™˜\n",
        "        #    - output_dim: ì˜ˆì¸¡ ê¸¸ì´\n",
        "        #    - hidden_dim: ë‚´ë¶€ ì²˜ë¦¬ ì°¨ì›\n",
        "        #    - trend_len: ì‹œê³„ì—´ ê¸¸ì´\n",
        "        #    - num_trends: íŠ¸ë Œë“œ ì±„ë„ ìˆ˜\n",
        "        self.gtrend_encoder = GTrendEmbedder(output_dim, hidden_dim,\n",
        "                                             use_encoder_mask, trend_len,\n",
        "                                             num_trends, gpu_num)\n",
        "\n",
        "        # (3) ì‹œê°„ íŠ¹ì„±ê³¼ íŠ¸ë Œë“œ ì •ë³´ë¥¼ ê²°í•©í•˜ê¸° ì „ì—,\n",
        "        #     ì‹œê°„ íŠ¹ì„± ë²¡í„°ë¥¼ ì€ë‹‰ ì°¨ì› í¬ê¸°ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì€ ë„¤íŠ¸ì›Œí¬\n",
        "        self.feature_fusion = nn.Sequential(\n",
        "            nn.BatchNorm1d(embedding_dim),      # ë°°ì¹˜ ì •ê·œí™” (í•™ìŠµ ì•ˆì •í™”)\n",
        "            nn.Linear(embedding_dim, hidden_dim),# ì°¨ì› ë³€í™˜\n",
        "            nn.ReLU(),                          # ë¹„ì„ í˜• í™œì„±í™”\n",
        "            nn.Dropout(0.2)                     # ê³¼ì í•© ë°©ì§€\n",
        "        )\n",
        "\n",
        "        # (4) Transformer ë””ì½”ë” ë ˆì´ì–´\n",
        "        #     - Cross-Attentionì„ ì‚¬ìš©í•´ ì‹œê°„ íŠ¹ì„±(tgt)ì™€ íŠ¸ë Œë“œ ì¸ì½”ë”©(memory) ì—°ê²°\n",
        "        self.decoder_layer = TransformerDecoderLayer(\n",
        "            d_model=self.hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=self.hidden_dim * 4,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        # (5) ìê¸°íšŒê·€ ëª¨ë“œì¸ ê²½ìš°, ë””ì½”ë” ì…ë ¥ì— Positional Encoding ì¶”ê°€\n",
        "        if self.autoregressive:\n",
        "            self.pos_encoder = PositionalEncoding(hidden_dim, max_len=12)\n",
        "\n",
        "        # (6) ë””ì½”ë” ì¶œë ¥ â†’ ìµœì¢… ì˜ˆì¸¡ê°’ ë³€í™˜í•˜ëŠ” FC ë ˆì´ì–´\n",
        "        #     - ë¹„ìê°€íšŒê·€: í•œ ë²ˆì— output_len ê¸¸ì´ ì˜ˆì¸¡\n",
        "        #     - ìê¸°íšŒê·€: í•œ ë²ˆì— 1 ì‹œì ë§Œ ì˜ˆì¸¡\n",
        "        self.decoder_fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim,\n",
        "                      self.output_len if not self.autoregressive else 1),\n",
        "            nn.Dropout(0.2)\n",
        "        )"
      ],
      "metadata": {
        "id": "vUebnojPEAVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def _generate_square_subsequent_mask(self, size):\n",
        "        \"\"\"\n",
        "        ìê¸°íšŒê·€(Autoregressive) ì˜ˆì¸¡ì—ì„œ ë¯¸ë˜ ì‹œì  ì •ë³´ë¥¼ ë³´ì§€ ì•Šë„ë¡ ë§Œë“œëŠ” ë§ˆìŠ¤í¬ ìƒì„± í•¨ìˆ˜\n",
        "        - size: ì‹œí€€ìŠ¤ ê¸¸ì´\n",
        "        - torch.triu(...): ìƒì‚¼ê°í–‰ë ¬ì„ ë§Œë“¤ì–´, í˜„ì¬ ì‹œì  ì´í›„(ë¯¸ë˜) ê°’ì€ ì°¨ë‹¨\n",
        "        - transpose(0,1): Transformer ê·œì¹™ì— ë§ê²Œ ì°¨ì› ì „í™˜\n",
        "        - masked_fill: 0ì¸ ë¶€ë¶„(ë¯¸ë˜ ì‹œì )ì€ -infë¡œ ì±„ì›Œì„œ attentionì—ì„œ ë¬´ì‹œ\n",
        "                       1ì¸ ë¶€ë¶„(í˜„ì¬ ë° ê³¼ê±° ì‹œì )ì€ 0.0ìœ¼ë¡œ ì±„ì›Œì„œ attention í—ˆìš©\n",
        "        \"\"\"\n",
        "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, category, color, fabric, temporal_features, gtrends, images):\n",
        "        \"\"\"\n",
        "        ëª¨ë¸ ìˆœì „íŒŒ(forward) ê³¼ì •\n",
        "        category, color, fabric: (í˜„ì¬ ë‹¨ê³„ì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨)\n",
        "        temporal_features: ë‚ ì§œ/ì‹œê°„ ê´€ë ¨ íŠ¹ì„± (day/week/month/year ë“±)\n",
        "        gtrends: Google Trends ì‹œê³„ì—´ ë°ì´í„°\n",
        "        images: (í˜„ì¬ ë‹¨ê³„ì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨)\n",
        "        \"\"\"\n",
        "\n",
        "        # (1) ë‚ ì§œ/ì‹œê°„ íŠ¹ì„± â†’ ì„ë² ë”©\n",
        "        dummy_encoding = self.dummy_encoder(temporal_features)\n",
        "\n",
        "        # (2) Google Trends ì‹œê³„ì—´ ë°ì´í„° â†’ Transformer ì¸ì½”ë”©\n",
        "        gtrend_encoding = self.gtrend_encoder(gtrends)\n",
        "\n",
        "        # (3) ì‹œê°„ íŠ¹ì„± ì„ë² ë”©ì„ ì€ë‹‰ ì°¨ì› í¬ê¸°ë¡œ ë³€í™˜ (BatchNorm â†’ Linear â†’ ReLU â†’ Dropout)\n",
        "        static_feature_fusion = self.feature_fusion(dummy_encoding)\n",
        "\n",
        "        # (4) ë””ì½”ë” ì…ë ¥ ì¤€ë¹„\n",
        "        #     - Transformer ë””ì½”ë”ëŠ” [ì‹œí€€ìŠ¤ ê¸¸ì´, ë°°ì¹˜ í¬ê¸°, íŠ¹ì„± ì°¨ì›] í˜•íƒœë¥¼ ê¸°ëŒ€\n",
        "        #     - ì—¬ê¸°ì„œëŠ” í•œ ì‹œì  ì •ë³´ë§Œ ë„£ìœ¼ë¯€ë¡œ unsqueeze(0)ìœ¼ë¡œ seq_len = 1 ì¶”ê°€\n",
        "        tgt = static_feature_fusion.unsqueeze(0)\n",
        "\n",
        "        # (5) ë””ì½”ë”ì˜ 'memory'ëŠ” ì¸ì½”ë”ì—ì„œ ë‚˜ì˜¨ Google Trends ì¸ì½”ë”© ë²¡í„°\n",
        "        memory = gtrend_encoding\n",
        "\n",
        "        # (6) Transformer ë””ì½”ë” ë ˆì´ì–´ í†µê³¼\n",
        "        #     - tgt: ì‹œê°„ íŠ¹ì„±\n",
        "        #     - memory: íŠ¸ë Œë“œ ì¸ì½”ë”© (Cross-Attention)\n",
        "        decoder_out, attn_weights = self.decoder_layer(tgt, memory)\n",
        "\n",
        "        # (7) ë””ì½”ë” ì¶œë ¥ â†’ ìµœì¢… ì˜ˆì¸¡ê°’ ë³€í™˜\n",
        "        forecast = self.decoder_fc(decoder_out)\n",
        "\n",
        "        # (8) [seq_len, batch, output_len] â†’ [batch, output_len] í˜•íƒœë¡œ ë³€í™˜\n",
        "        return forecast.view(-1, self.output_len), attn_weights"
      ],
      "metadata": {
        "id": "HfEG0TxKE3ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        í•™ìŠµì— ì‚¬ìš©í•  ì˜µí‹°ë§ˆì´ì €(Optimizer) ì„¤ì •\n",
        "        - Adafactor: Adamì˜ ë³€í˜•ìœ¼ë¡œ, ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê³  í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜\n",
        "        - scale_parameter, relative_step, warmup_init: í•™ìŠµë¥  ìë™ ì¡°ì • ì˜µì…˜\n",
        "        - lr=None: í•™ìŠµë¥ ì€ Adafactor ë‚´ë¶€ì—ì„œ ìë™ ê²°ì •\n",
        "        \"\"\"\n",
        "        optimizer = Adafactor(self.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        í•™ìŠµ ë‹¨ê³„ì—ì„œ í•œ ë°°ì¹˜(batch)ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
        "        - batch: (item_sales, category, color, fabric, temporal_features, gtrends, images) í˜•íƒœ\n",
        "        - batch_idx: í˜„ì¬ ë°°ì¹˜ ë²ˆí˜¸\n",
        "        \"\"\"\n",
        "\n",
        "        # ë°°ì¹˜ì—ì„œ ê° ë°ì´í„° êº¼ë‚´ê¸°\n",
        "        item_sales, category, color, fabric, temporal_features, gtrends, images = batch\n",
        "\n",
        "        # temporal_featuresì™€ gtrendsê°€ í•™ìŠµ ì¤‘ì— ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •\n",
        "        temporal_features = temporal_features.requires_grad_(True)\n",
        "        gtrends = gtrends.requires_grad_(True)\n",
        "\n",
        "        # ëª¨ë¸ì„ í†µê³¼ì‹œì¼œ ì˜ˆì¸¡ê°’(forecasted_sales) êµ¬í•˜ê¸°\n",
        "        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n",
        "\n",
        "        # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ íŒë§¤ëŸ‰(item_sales)ì˜ ì°¨ì´ë¥¼ MSE(í‰ê· ì œê³±ì˜¤ì°¨)ë¡œ ê³„ì‚°\n",
        "        loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
        "\n",
        "        # í•™ìŠµ ì¤‘ lossë¥¼ ê¸°ë¡(log) â†’ prog_bar=Trueì´ë©´ ì§„í–‰ë°”ì— í‘œì‹œë¨\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "\n",
        "        # Lightningì´ ì´ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì—­ì „íŒŒ(Backpropagation) ì§„í–‰\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        ê²€ì¦ ë‹¨ê³„ì—ì„œ í•œ ë°°ì¹˜(batch)ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
        "        - í•™ìŠµê³¼ ë‹¬ë¦¬ lossë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šê³ , ì„±ëŠ¥ë§Œ í™•ì¸\n",
        "        \"\"\"\n",
        "\n",
        "        # ë°°ì¹˜ì—ì„œ ê° ë°ì´í„° êº¼ë‚´ê¸°\n",
        "        item_sales, category, color, fabric, temporal_features, gtrends, images = batch\n",
        "\n",
        "        # forward()ë¡œ ì˜ˆì¸¡ê°’ ê³„ì‚°\n",
        "        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n",
        "\n",
        "        # validation ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (ë‚˜ì¤‘ì— í•œ ë²ˆì— ê³„ì‚°í•˜ê¸° ìœ„í•¨)\n",
        "        if not hasattr(self, 'validation_step_outputs'):\n",
        "            self.validation_step_outputs = []\n",
        "        self.validation_step_outputs.append((item_sales.squeeze(), forecasted_sales.squeeze()))\n",
        "\n",
        "        # í˜„ì¬ ë°°ì¹˜ì˜ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ë°˜í™˜ (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)\n",
        "        return item_sales.squeeze(), forecasted_sales.squeeze()"
      ],
      "metadata": {
        "id": "1dzOAxR3FAtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def on_validation_epoch_end(self):\n",
        "        \"\"\"\n",
        "        í•œ ë²ˆì˜ ê²€ì¦(epoch)ì´ ëë‚œ í›„ ì„±ëŠ¥ì„ ê³„ì‚°í•˜ê³  ê¸°ë¡í•˜ëŠ” í•¨ìˆ˜\n",
        "        - validation_stepì—ì„œ ì €ì¥í•œ ëª¨ë“  ë°°ì¹˜ì˜ ê²°ê³¼ë¥¼ ëª¨ì•„ í‰ê· ì ì¸ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°\n",
        "        \"\"\"\n",
        "\n",
        "        # validation_stepì—ì„œ ê²°ê³¼ë¥¼ ì €ì¥í•œ ë¦¬ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
        "        if hasattr(self, 'validation_step_outputs'):\n",
        "            val_step_outputs = self.validation_step_outputs  # [(ì‹¤ì œê°’, ì˜ˆì¸¡ê°’), ...] í˜•íƒœ\n",
        "\n",
        "            # ë¦¬ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ íŒë§¤ëŸ‰(item_sales)ê³¼ ì˜ˆì¸¡ê°’(forecasted_sales)ë§Œ ê°ê° ì¶”ì¶œ\n",
        "            item_sales = [x[0] for x in val_step_outputs]\n",
        "            forecasted_sales = [x[1] for x in val_step_outputs]\n",
        "\n",
        "            # ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ì„œë¡œ í•©ì¹˜ê¸° (stack)\n",
        "            item_sales = torch.stack(item_sales)\n",
        "            forecasted_sales = torch.stack(forecasted_sales)\n",
        "\n",
        "            # ì˜ˆì¸¡ê³¼ ì‹¤ì œê°’ì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì› (í•™ìŠµ ì‹œ ì •ê·œí™”í–ˆìœ¼ë¯€ë¡œ 1065 ê³±í•¨)\n",
        "            rescaled_item_sales = item_sales * 1065\n",
        "            rescaled_forecasted_sales = forecasted_sales * 1065\n",
        "\n",
        "            # MSE(í‰ê· ì œê³±ì˜¤ì°¨) ê³„ì‚° - ì •ê·œí™”ëœ ê°’ ê¸°ì¤€\n",
        "            loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
        "\n",
        "            # MAE(í‰ê· ì ˆëŒ€ì˜¤ì°¨) ê³„ì‚° - ì›ë˜ ìŠ¤ì¼€ì¼ ê¸°ì¤€\n",
        "            mae = F.l1_loss(rescaled_item_sales, rescaled_forecasted_sales)\n",
        "\n",
        "            # ì„±ëŠ¥ ì§€í‘œ ê¸°ë¡ (ì§„í–‰ë°”ì—ë„ í‘œì‹œ)\n",
        "            self.log('val_mae', mae, prog_bar=True)   # ì˜ˆì¸¡ ì •í™•ë„\n",
        "            self.log('val_loss', loss, prog_bar=True) # ì†ì‹¤ê°’\n",
        "\n",
        "            # ë‹¤ìŒ epochì„ ìœ„í•´ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
        "            self.validation_step_outputs.clear()"
      ],
      "metadata": {
        "id": "3Xti99ULFMIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iFYv579HNQD"
      },
      "source": [
        "## 4. ğŸ“Š ë°ì´í„°ì…‹ í´ë˜ìŠ¤"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ZeroShotDataset():\n",
        "    \"\"\"\n",
        "    íŒë§¤ëŸ‰ ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
        "    - Google Trends, ì¹´í…Œê³ ë¦¬/ìƒ‰ìƒ/ì›ë‹¨ ì •ë³´, ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë¬¶ì–´ì„œ ëª¨ë¸ì— ì „ë‹¬í•  ìˆ˜ ìˆê²Œ ì „ì²˜ë¦¬í•¨\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_df, img_root, gtrends, cat_dict, col_dict, fab_dict, trend_len):\n",
        "        # ì´ˆê¸° ì„¤ì • (ë°ì´í„°ì™€ í•„ìš”í•œ ë§¤í•‘ ì •ë³´ ì €ì¥)\n",
        "        self.data_df = data_df               # ìƒí’ˆ ì •ë³´ê°€ ë‹´ê¸´ ë°ì´í„°í”„ë ˆì„\n",
        "        self.gtrends = gtrends               # Google Trends ë°ì´í„°\n",
        "        self.cat_dict = cat_dict             # ì¹´í…Œê³ ë¦¬ â†’ ìˆ«ì ë§¤í•‘\n",
        "        self.col_dict = col_dict             # ìƒ‰ìƒ â†’ ìˆ«ì ë§¤í•‘\n",
        "        self.fab_dict = fab_dict             # ì›ë‹¨ â†’ ìˆ«ì ë§¤í•‘\n",
        "        self.trend_len = trend_len           # Google Trends ì‹œê³„ì—´ ê¸¸ì´\n",
        "        self.img_root = img_root             # ì´ë¯¸ì§€ íŒŒì¼ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ\n",
        "\n",
        "    def __len__(self):\n",
        "        # ì „ì²´ ë°ì´í„° ê°œìˆ˜ ë°˜í™˜ (len(dataset) í•  ë•Œ í˜¸ì¶œë¨)\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # íŠ¹ì • ì¸ë±ìŠ¤ì˜ ë°ì´í„° í•œ ì¤„ì„ ë°˜í™˜\n",
        "        return self.data_df.iloc[idx, :]\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"\n",
        "        ì›ë³¸ ë°ì´í„°ë¥¼ ëª¨ë¸ì´ í•™ìŠµí•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
        "        1) Google Trends ë°ì´í„°ë¥¼ ì¶”ì¶œ & ì •ê·œí™”\n",
        "        2) ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì™€ì„œ í…ì„œë¡œ ë³€í™˜\n",
        "        3) ì¹´í…Œê³ ë¦¬, ìƒ‰ìƒ, ì›ë‹¨ì„ ìˆ«ìë¡œ ë³€í™˜\n",
        "        4) í…ì„œ í˜•íƒœì˜ í•™ìŠµìš© ë°ì´í„°ì…‹ìœ¼ë¡œ ë°˜í™˜\n",
        "        \"\"\"\n",
        "\n",
        "        data = self.data_df\n",
        "        gtrends, image_features = [], []\n",
        "\n",
        "        # ì´ë¯¸ì§€ ë³€í™˜ (í¬ê¸° ì¡°ì •, í…ì„œ ë³€í™˜, ì •ê·œí™”)\n",
        "        img_transforms = Compose([\n",
        "            Resize((256, 256)),\n",
        "            ToTensor(),\n",
        "            Normalize(mean=[0.485, 0.456, 0.406],  # ì´ë¯¸ì§€ ìƒ‰ìƒ í‰ê· \n",
        "                      std=[0.229, 0.224, 0.225])  # ì´ë¯¸ì§€ ìƒ‰ìƒ í‘œì¤€í¸ì°¨\n",
        "        ])\n",
        "\n",
        "        # ë°ì´í„°í”„ë ˆì„ì˜ ê° í–‰(row)ì„ í•˜ë‚˜ì”© ì²˜ë¦¬\n",
        "        for (idx, row) in tqdm(data.iterrows(), total=len(data), ascii=True, desc=\"ë°ì´í„° ì „ì²˜ë¦¬\"):\n",
        "            cat, col, fab, fiq_attr, start_date, img_path = \\\n",
        "                row['category'], row['color'], row['fabric'], row['extra'], \\\n",
        "                row['release_date'], row['image_path']\n",
        "\n",
        "            # Google Trends ë°ì´í„°: ì¶œì‹œì¼ ê¸°ì¤€ 1ë…„(52ì£¼) ì „ë¶€í„° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
        "            gtrend_start = start_date - pd.DateOffset(weeks=52)\n",
        "            cat_gtrend = self.gtrends.loc[gtrend_start:start_date][cat][-52:].values[:self.trend_len]\n",
        "            col_gtrend = self.gtrends.loc[gtrend_start:start_date][col][-52:].values[:self.trend_len]\n",
        "            fab_gtrend = self.gtrends.loc[gtrend_start:start_date][fab][-52:].values[:self.trend_len]\n",
        "\n",
        "            # Min-Max ì •ê·œí™” (0~1 ë²”ìœ„ë¡œ)\n",
        "            cat_gtrend = MinMaxScaler().fit_transform(cat_gtrend.reshape(-1,1)).flatten()\n",
        "            col_gtrend = MinMaxScaler().fit_transform(col_gtrend.reshape(-1,1)).flatten()\n",
        "            fab_gtrend = MinMaxScaler().fit_transform(fab_gtrend.reshape(-1,1)).flatten()\n",
        "\n",
        "            # 3ê°œì˜ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°\n",
        "            multitrends = np.vstack([cat_gtrend, col_gtrend, fab_gtrend])\n",
        "\n",
        "            # ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° (RGB ë³€í™˜)\n",
        "            img = Image.open(os.path.join(self.img_root, img_path)).convert('RGB')\n",
        "\n",
        "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
        "            gtrends.append(multitrends)\n",
        "            image_features.append(img_transforms(img))\n",
        "\n",
        "        # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜\n",
        "        gtrends = np.array(gtrends)\n",
        "\n",
        "        # í•„ìš” ì—†ëŠ” ì»¬ëŸ¼ ì‚­ì œ\n",
        "        data = data.copy()\n",
        "        data.drop(['external_code', 'season', 'release_date', 'image_path'], axis=1, inplace=True)\n",
        "\n",
        "        # í…ì„œ í˜•íƒœë¡œ ë³€í™˜\n",
        "        item_sales = torch.FloatTensor(data.iloc[:, :12].values)     # íŒë§¤ëŸ‰\n",
        "        temporal_features = torch.FloatTensor(data.iloc[:, 13:17].values)  # ì‹œê°„ ê´€ë ¨ íŠ¹ì„±\n",
        "\n",
        "        # ì¹´í…Œê³ ë¦¬/ìƒ‰ìƒ/ì›ë‹¨ â†’ ìˆ«ì ID ë³€í™˜\n",
        "        categories = [self.cat_dict[val] for val in data.category.values]\n",
        "        colors = [self.col_dict[val] for val in data.color.values]\n",
        "        fabrics = [self.fab_dict[val] for val in data.fabric.values]\n",
        "\n",
        "        categories = torch.LongTensor(categories)\n",
        "        colors = torch.LongTensor(colors)\n",
        "        fabrics = torch.LongTensor(fabrics)\n",
        "\n",
        "        gtrends = torch.FloatTensor(gtrends)        # Google Trends ë°ì´í„°\n",
        "        images = torch.stack(image_features)        # ì´ë¯¸ì§€ ë°ì´í„°\n",
        "\n",
        "        # í•™ìŠµì— ì‚¬ìš©í•  TensorDataset ë°˜í™˜\n",
        "        return TensorDataset(item_sales, categories, colors, fabrics, temporal_features, gtrends, images)\n",
        "\n",
        "    def get_loader(self, batch_size, train=True):\n",
        "        \"\"\"\n",
        "        DataLoader ìƒì„± í•¨ìˆ˜\n",
        "        - batch_size í¬ê¸°ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆ ì„œ ëª¨ë¸ì— ê³µê¸‰\n",
        "        - í•™ìŠµ ëª¨ë“œì¼ ë•ŒëŠ” ë°ì´í„° ìˆœì„œë¥¼ ì„ìŒ(shuffle=True)\n",
        "        \"\"\"\n",
        "        print(' 1ë‹¨ê³„ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...')\n",
        "        data_with_gtrends = self.preprocess_data()\n",
        "        if train:\n",
        "            data_loader = DataLoader(data_with_gtrends, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "        else:\n",
        "            data_loader = DataLoader(data_with_gtrends, batch_size=1, shuffle=False, num_workers=2)\n",
        "        print(' 1ë‹¨ê³„ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ')\n",
        "        return data_loader\n",
        "\n",
        "print(\" ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "naHwill1KkBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dK7aLl2HNQE"
      },
      "source": [
        "## 5. ğŸš€ 1ë‹¨ê³„ ì‹¤í–‰ ì½”ë“œ\n",
        "### ë°ì´í„° ë¡œë”©ë¶€í„° ëª¨ë¸ í›ˆë ¨ê¹Œì§€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ojcXcBnHNQE"
      },
      "outputs": [],
      "source": [
        "# ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
        "dataset_path = Path('/content/drive/MyDrive/GTM-dataset-small/')\n",
        "\n",
        "# ë°ì´í„° ë¡œë”©\n",
        "print(\" ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
        "train_df = pd.read_csv(dataset_path / 'train.csv', parse_dates=['release_date'])\n",
        "test_df = pd.read_csv(dataset_path / 'test.csv', parse_dates=['release_date'])\n",
        "gtrends = pd.read_csv(dataset_path / 'gtrends.csv', index_col=[0], parse_dates=True)\n",
        "\n",
        "cat_dict = torch.load(dataset_path / 'category_labels.pt', weights_only=False)\n",
        "col_dict = torch.load(dataset_path / 'color_labels.pt', weights_only=False)\n",
        "fab_dict = torch.load(dataset_path / 'fabric_labels.pt', weights_only=False)\n",
        "\n",
        "print(f\" í›ˆë ¨ ë°ì´í„°: {len(train_df):,}ê°œ\")\n",
        "print(f\" í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ\")\n",
        "print(f\" Google Trends: {len(gtrends):,}ê°œ ì‹œì \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL3Uurx5HNQE"
      },
      "outputs": [],
      "source": [
        "# ë°ì´í„°ì…‹ ìƒì„±\n",
        "train_dataset = ZeroShotDataset(train_df, dataset_path / 'images', gtrends, cat_dict, col_dict, fab_dict, trend_len=52)\n",
        "test_dataset = ZeroShotDataset(test_df, dataset_path / 'images', gtrends, cat_dict, col_dict, fab_dict, trend_len=52)\n",
        "\n",
        "BATCH_SIZE = 8 if torch.cuda.is_available() else 4\n",
        "train_loader = train_dataset.get_loader(batch_size=BATCH_SIZE, train=True)\n",
        "test_loader = test_dataset.get_loader(batch_size=1, train=False)\n",
        "\n",
        "print(f\" ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
        "print(f\" í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}\")\n",
        "print(f\" í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìˆ˜: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4OuU5wmHNQE"
      },
      "outputs": [],
      "source": [
        "# 1ë‹¨ê³„ ëª¨ë¸ ìƒì„±\n",
        "print(\" GTM Step 1 ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
        "\n",
        "model = GTM_Step1(\n",
        "    embedding_dim=32,\n",
        "    hidden_dim=64,\n",
        "    output_dim=12,\n",
        "    num_heads=4,\n",
        "    num_layers=1,\n",
        "    cat_dict=cat_dict,\n",
        "    col_dict=col_dict,\n",
        "    fab_dict=fab_dict,\n",
        "    trend_len=52,\n",
        "    num_trends=3,\n",
        "    gpu_num=0,\n",
        "    use_encoder_mask=1,\n",
        "    autoregressive=False\n",
        ")\n",
        "\n",
        "print(f\" Step 1 ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
        "print(f\" ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(\"\\n ì‚¬ìš© ëª¨ë‹¬ë¦¬í‹°: Temporal Features (ì‹œê°„ ì •ë³´) + Google Trends\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reAKbS6OHNQE"
      },
      "outputs": [],
      "source": [
        "# Trainer ì„¤ì • ë° í›ˆë ¨\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.loggers import CSVLogger\n",
        "\n",
        "EPOCHS = 5\n",
        "ACCELERATOR = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath='./checkpoints/',\n",
        "    filename='gtm-step1-{epoch:02d}-{val_mae:.2f}',\n",
        "    monitor='val_mae',\n",
        "    mode='min',\n",
        "    save_top_k=2\n",
        ")\n",
        "\n",
        "csv_logger = CSVLogger(save_dir='./logs/', name='gtm_step1')\n",
        "\n",
        "trainer = L.Trainer(\n",
        "    devices=1,\n",
        "    accelerator=ACCELERATOR,\n",
        "    max_epochs=EPOCHS,\n",
        "    logger=csv_logger,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    enable_progress_bar=True,\n",
        "    gradient_clip_val=1.0\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ GTM Step 1 í›ˆë ¨ ì‹œì‘!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)\n",
        "    print(\"\\n Step 1 í›ˆë ¨ ì™„ë£Œ!\")\n",
        "    print(f\"ğŸ’¾ ìµœê³  ëª¨ë¸: {checkpoint_callback.best_model_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Step 1 í›ˆë ¨ ì‹¤íŒ¨: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HrK9vcHHNQE"
      },
      "source": [
        "## ğŸ“‹ 1ë‹¨ê³„ ìš”ì•½\n",
        "\n",
        "### âœ… êµ¬í˜„ ì™„ë£Œ\n",
        "- **ì‹œê°„ì  íŠ¹ì„± ì„ë² ë”©**: ë‚ ì§œ ì •ë³´ (ì¼, ì£¼, ì›”, ë…„)ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
        "- **Google Trends ì¸ì½”ë”©**: Transformer Encoderë¡œ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ\n",
        "- **ê¸°ë³¸ Cross-Attention**: ì‹œê°„ ì •ë³´ì™€ íŠ¸ë Œë“œ ë°ì´í„° ê°„ ê´€ê³„ í•™ìŠµ\n",
        "\n",
        "### ğŸ¯ í•™ìŠµ ëª©í‘œ ë‹¬ì„±\n",
        "- Transformer ê¸°ë³¸ êµ¬ì¡° ì´í•´\n",
        "- ì‹œê³„ì—´ ë°ì´í„° ì¸ì½”ë”© ë°©ë²•\n",
        "- Multi-modal ì…ë ¥ ì²˜ë¦¬ ê¸°ì´ˆ\n",
        "\n",
        "### ğŸ”œ ë‹¤ìŒ ë‹¨ê³„ ì˜ˆê³ \n",
        "**Step 2**ì—ì„œëŠ” **ì´ë¯¸ì§€ ì •ë³´**ë¥¼ ì¶”ê°€í•˜ì—¬ ì‹œê°ì  íŠ¹ì„±ë„ í•¨ê»˜ í•™ìŠµí•©ë‹ˆë‹¤!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}