{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeSaeBom/GTM-Transformer-Jupyter/blob/main/GTM_Step1_Dummy_Only_%EA%B0%95%EC%9D%98%EC%9A%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOdUqdq8HNQB"
      },
      "source": [
        "# 🚀 GTM Step 1: Temporal Features + Google Trends\n",
        "\n",
        "## 📚 특강 1단계: 기본 시계열 예측\n",
        "- **사용 모달리티**: Temporal Features (날짜 정보) + Google Trends\n",
        "- **목적**: 시계열 데이터만으로 매출 예측의 기초 구현\n",
        "- **학습 목표**:\n",
        "  - Transformer 기본 구조 이해\n",
        "  - 시계열 인코딩 (Positional Encoding)\n",
        "  - Google Trends 데이터 활용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gl57um5HNQC"
      },
      "source": [
        "## 1. 📦 패키지 설치 및 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR9ZTTFOHNQC",
        "outputId": "48bbb795-fee5-4443-a1d4-2b94210e7049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/363.4 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 패키지 설치\n",
        "!pip install lightning --upgrade --quiet\n",
        "!pip install transformers scikit-learn pillow --quiet\n",
        "\n",
        "# Import\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import lightning as L\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageFile\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.transforms import Resize, ToTensor, Normalize, Compose\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from transformers import Adafactor\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# Google Drive 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(f\" PyTorch: {torch.__version__}\")\n",
        "print(f\" Lightning: {L.__version__}\")\n",
        "print(f\" CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\" GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kn6FjsVHNQC"
      },
      "source": [
        "## 2. 🧠 모델 컴포넌트 정의\n",
        "### 1단계에서는 최소한의 컴포넌트만 사용"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 모듈들\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=52):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # 드롭아웃: 위치 임베딩이 추가된 후 과적합을 줄이기 위한 정규화\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # pe: [max_len, d_model] 크기의 위치 임베딩 테이블(고정값, 학습되지 않음)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # position: [max_len, 1] 형태로 0 ~ max_len-1까지의 정수 위치\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # div_term: 주파수 스케일(지수적으로 커지는 간격)\n",
        "        #   - 논문 \"Attention Is All You Need\"의 사인/코사인 위치 인코딩 공식을 그대로 구현\n",
        "        #   - d_model의 짝수 인덱스에 대응하는 주파수들만 계산\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # 짝수 채널(0,2,4,...)에는 sin, 홀수 채널(1,3,5,...)에는 cos를 채운다.\n",
        "        # position * div_term의 브로드캐스팅으로 [max_len, d_model/2]에 해당하는 값을 만든 뒤 할당\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd  indices\n",
        "\n",
        "        # Transformer의 기본 입력 형태(S, N, E = seq_len, batch, embed)에 맞추기 위해\n",
        "        # pe를 [max_len, 1, d_model]로 바꿔 배치 차원으로 브로드캐스팅되게 만든다.\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)  # [1, max_len, d_model] -> [max_len, 1, d_model]\n",
        "\n",
        "        # register_buffer: 학습 파라미터는 아니지만 모델과 함께 디바이스 이동/저장되도록 등록\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [seq_len, batch_size, d_model] 형태를 기대 (PyTorch nn.Transformer 표준)\n",
        "        # 현재 시퀀스 길이(seq_len)에 해당하는 위치 임베딩을 잘라 더한다.\n",
        "        # self.pe[:x.size(0), :] -> [seq_len, 1, d_model] 이고 배치 차원으로 브로드캐스팅됨\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)  # 위치 인코딩이 더해진 후 드롭아웃 적용"
      ],
      "metadata": {
        "id": "i-ZYkybguPpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeDistributed(nn.Module):\n",
        "    def __init__(self, module, batch_first=True):\n",
        "        super(TimeDistributed, self).__init__()\n",
        "        # module: 시간축 각 스텝에 똑같이 적용할 하위 모듈\n",
        "        #   예: nn.Linear, 작은 CNN, MLP 등\n",
        "        self.module = module\n",
        "        # batch_first:\n",
        "        #   True  → 입력 형태가 [batch, time, feature]\n",
        "        #   False → 입력 형태가 [time, batch, feature]\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (1) 입력이 2차원 이하이면 (시간축이 없으면)\n",
        "        # 예: [batch, feature] 또는 [time, feature]\n",
        "        # 그냥 module에 바로 넣어서 처리\n",
        "        if len(x.size()) <= 2:\n",
        "            return self.module(x)\n",
        "\n",
        "        # (2) 시간축이 있는 경우 → 시간축과 배치축을 합쳐서 한 번에 처리\n",
        "        # 예: batch_first=True  -> [B, T, F] → [B*T, F]\n",
        "        #     batch_first=False -> [T, B, F] → [T*B, F]\n",
        "        x_reshape = x.contiguous().view(-1, x.size(-1))\n",
        "        # contiguous(): 메모리 상에서 연속적으로 만들어줘서 view()가 안전하게 동작하도록 함\n",
        "\n",
        "        # (3) 평탄화된 데이터를 module에 통과\n",
        "        # 이렇게 하면 for문 없이 한 번에 모든 시점 데이터 처리 가능\n",
        "        y = self.module(x_reshape)  # shape: [B*T, F_out] 또는 [T*B, F_out]\n",
        "\n",
        "        # (4) 원래 모양으로 복원\n",
        "        if self.batch_first:\n",
        "            # batch_first=True → [B, T, F_out] 형태로 복원\n",
        "            y = y.contiguous().view(x.size(0), -1, y.size(-1))\n",
        "        else:\n",
        "            # batch_first=False → [T, B, F_out] 형태로 복원\n",
        "            y = y.view(-1, x.size(1), y.size(-1))\n",
        "\n",
        "        # (5) 최종 결과 반환\n",
        "        return y\n",
        "\n",
        "\n",
        "print(\" 기본 모듈 정의 완료\")\n"
      ],
      "metadata": {
        "id": "xlTLPA4EuRrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1단계: Dummy (시간) + GTrends 인코더만 사용\n",
        "class DummyEmbedder(nn.Module):\n",
        "    \"\"\"시간 정보(일/주/월/연)를 각각 임베딩한 뒤 하나로 합쳐 단일 임베딩으로 투영\"\"\"\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        # 각 스칼라 시간값(일/주/월/연: 모두 1차원)을 임베딩 차원으로 선형 사상\n",
        "        self.day_embedding   = nn.Linear(1, embedding_dim)\n",
        "        self.week_embedding  = nn.Linear(1, embedding_dim)\n",
        "        self.month_embedding = nn.Linear(1, embedding_dim)\n",
        "        self.year_embedding  = nn.Linear(1, embedding_dim)\n",
        "        # 4개 임베딩을 concat하여(embedding_dim*4) 다시 embedding_dim으로 축소\n",
        "        self.dummy_fusion = nn.Linear(embedding_dim*4, embedding_dim)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, temporal_features):\n",
        "        \"\"\"\n",
        "        temporal_features: [B, 4] 가정 (컬럼: day, week, month, year 순서)\n",
        "          - 각 컬럼은 스칼라이므로 Linear 입력을 위해 [B, 1]로 확장\n",
        "        반환: temporal_embeddings ∈ ℝ[B, embedding_dim]\n",
        "        \"\"\"\n",
        "        # 각 시간 성분 분리 + [B, 1]로 차원 확장\n",
        "        d, w, m, y = temporal_features[:, 0].unsqueeze(1), temporal_features[:, 1].unsqueeze(1), \\\n",
        "                     temporal_features[:, 2].unsqueeze(1), temporal_features[:, 3].unsqueeze(1)\n",
        "\n",
        "        # 각각 선형 임베딩\n",
        "        d_emb = self.day_embedding(d)     # [B, D]\n",
        "        w_emb = self.week_embedding(w)    # [B, D]\n",
        "        m_emb = self.month_embedding(m)   # [B, D]\n",
        "        y_emb = self.year_embedding(y)    # [B, D]\n",
        "\n",
        "        # concat 후 차원 축소(융합)\n",
        "        temporal_embeddings = self.dummy_fusion(torch.cat([d_emb, w_emb, m_emb, y_emb], dim=1))  # [B, D]\n",
        "        temporal_embeddings = self.dropout(temporal_embeddings)\n",
        "        return temporal_embeddings"
      ],
      "metadata": {
        "id": "wo8Ijl_nzL52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GTrendEmbedder(nn.Module):\n",
        "    \"\"\"Google Trends 데이터 인코딩\"\"\"\n",
        "    def __init__(self, forecast_horizon, embedding_dim, use_mask, trend_len, num_trends, gpu_num):\n",
        "        super().__init__()\n",
        "        # 예측 지평(horizon): 마스크 생성 시 블록 크기 결정에 사용\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "\n",
        "        # 시점별 입력(길이 num_trends 벡터)을 embedding_dim으로 투영\n",
        "        # 기대 입력: [B, T, num_trends]  → 출력: [B, T, embedding_dim]\n",
        "        # (forward에서 permute로 [B, num_trends, T]를 [B, T, num_trends]로 바꾼 뒤 사용)\n",
        "        self.input_linear = TimeDistributed(nn.Linear(num_trends, embedding_dim))\n",
        "\n",
        "        # 위치 인코딩: Transformer가 순서를 알 수 있도록 추가\n",
        "        # PositionalEncoding은 [S, N, E] (seq, batch, embed) 형태를 기대함\n",
        "        # trend_len은 사용할 최대 시퀀스 길이(≥ 실제 T)로 설정해야 함\n",
        "        self.pos_embedding = PositionalEncoding(embedding_dim, max_len=trend_len)\n",
        "\n",
        "        # 시계열 내 상관관계 학습을 위한 Transformer Encoder (2층)\n",
        "        # d_model=embedding_dim, nhead=4, dropout=0.2\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dropout=0.2)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "\n",
        "        # 마스크 사용 여부(1: 사용, 그 외: 미사용)\n",
        "        self.use_mask = use_mask\n",
        "\n",
        "        # (참고) 현재 코드에서 gpu_num은 직접 사용되지 않음\n",
        "        self.gpu_num = gpu_num\n",
        "\n",
        "    def _generate_encoder_mask(self, size, forecast_horizon):\n",
        "        # 어텐션 마스크(추가 가중치 방식, additive mask) 생성\n",
        "        # - 입력/출력 shape: [size, size] (size = 시퀀스 길이 T)\n",
        "        # - split = gcd(T, horizon)로 대각선 블록을 만들고 블록 내부만 어텐션 허용\n",
        "        # - PyTorch 규약: 허용=0.0, 차단=-inf (가중치에 더해져 softmax에서 무시됨)\n",
        "        mask = torch.zeros((size, size))\n",
        "        split = math.gcd(size, forecast_horizon)\n",
        "        for i in range(0, size, split):\n",
        "            mask[i:i+split, i:i+split] = 1\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, gtrends):\n",
        "        # gtrends 예상 입력: [B, num_trends, T]  (채널 우선)\n",
        "        # TimeDistributed 적용을 위해 [B, T, num_trends]로 변환\n",
        "        gtrend_emb = self.input_linear(gtrends.permute(0,2,1))   # → [B, T, D]\n",
        "\n",
        "        # PositionalEncoding은 [S, N, E] 형태를 기대하므로 [T, B, D]로 변환하여 적용\n",
        "        gtrend_emb = self.pos_embedding(gtrend_emb.permute(1,0,2))  # → [T, B, D]\n",
        "\n",
        "        # 시퀀스 길이 T에 맞춰 블록 대각선 마스크 생성 (데이터 누수 방지/주기 반영)\n",
        "        input_mask = self._generate_encoder_mask(gtrend_emb.shape[0], self.forecast_horizon).to(gtrend_emb.device)\n",
        "\n",
        "        # 마스크 사용 여부에 따라 Encoder에 전달\n",
        "        if self.use_mask == 1:\n",
        "            gtrend_emb = self.encoder(gtrend_emb, input_mask)   # [T, B, D] (마스크 적용)\n",
        "        else:\n",
        "            gtrend_emb = self.encoder(gtrend_emb)               # [T, B, D] (전체 어텐션)\n",
        "\n",
        "        # 반환: 시퀀스 우선 텐서 [T, B, D]\n",
        "        return gtrend_emb"
      ],
      "metadata": {
        "id": "AGGPSQw2z38X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"커스텀 트랜스포머 디코더 레이어\n",
        "    - 구성: (1) 디코더 자기어텐션 → (2) 인코더-디코더(크로스) 어텐션 → (3) 위치별 FFN\n",
        "    - 각 블록마다: 잔차 연결(Residual) + LayerNorm + Dropout\n",
        "    - 기본 텐서 형상(기본값 batch_first=False 가정):\n",
        "        tgt    : [T_tgt, B, D]   (디코더 입력/이전 단계 출력)\n",
        "        memory : [S_src, B, D]   (인코더 출력)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "\n",
        "        # (1) 디코더 자기어텐션: 디코더의 현재 토큰들이 자기 자신 시퀀스를 참조\n",
        "        #  - 오토리그레시브(미래 차단)로 쓰려면 forward에서 tgt_mask(혹은 causal) 전달\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        # (2) 인코더-디코더(크로스) 어텐션:\n",
        "        #  - 디코더가 인코더의 정보를 끌어와 현재 토큰을 더 정확히 예측\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        # (3) 위치별 피드포워드 네트워크(FFN): 각 시점의 피처를 비선형 변환\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        # 정규화 및 드롭아웃 (각 서브레이어 뒤에 사용)\n",
        "        self.norm1 = nn.LayerNorm(d_model)  # self-attn 뒤\n",
        "        self.norm2 = nn.LayerNorm(d_model)  # cross-attn 뒤\n",
        "        self.norm3 = nn.LayerNorm(d_model)  # FFN 뒤\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        # 활성함수 (필요시 GELU 등으로 교체 가능)\n",
        "        self.activation = F.relu\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None,\n",
        "            memory_key_padding_mask=None, tgt_is_causal=None, memory_is_causal=None):\n",
        "        \"\"\"\n",
        "        인자 설명:\n",
        "          - tgt : 디코더 입력/이전 디코더 레이어 출력  (shape: [T_tgt, B, D])\n",
        "          - memory : 인코더 출력(소스 시퀀스 인코딩 결과) (shape: [S_src, B, D])\n",
        "          - tgt_mask : 디코더 자기어텐션용 attn mask (예: 미래 차단용 causal mask)\n",
        "          - memory_mask : 인코더-디코더 어텐션용 mask (특정 소스 위치 차단 등)\n",
        "          - *_key_padding_mask : 패딩 토큰(True=무시) 가리기 위한 마스크 (batch 차원 기준)\n",
        "\n",
        "        반환:\n",
        "          - tgt : 현재 디코더 레이어의 출력 (shape: [T_tgt, B, D])\n",
        "          - attn_weights : 크로스 어텐션 가중치(디버깅/가시화 용)\n",
        "        \"\"\"\n",
        "\n",
        "        # ---------------------------\n",
        "        # (A) 디코더 자기어텐션 블록\n",
        "        #  - Query/Key/Value 모두 tgt\n",
        "        #  - 일반적으로 tgt_mask로 미래 토큰 차단(causal)하거나,\n",
        "        #    tgt_key_padding_mask로 패딩 위치 무시\n",
        "        # ---------------------------\n",
        "        tgt2 = self.self_attn(\n",
        "            tgt, tgt, tgt,\n",
        "            attn_mask=tgt_mask,\n",
        "            key_padding_mask=tgt_key_padding_mask\n",
        "        )[0]  # 반환: (attn_output, attn_weights); 여기선 출력만 사용\n",
        "        tgt = tgt + self.dropout1(tgt2)  # 잔차 연결\n",
        "        tgt = self.norm1(tgt)            # 정규화\n",
        "\n",
        "        # ---------------------------\n",
        "        # (B) 인코더-디코더(크로스) 어텐션 블록\n",
        "        #  - Query=tgt, Key/Value=memory(인코더 출력)\n",
        "        #  - memory_mask로 특정 소스 위치 차단 가능,\n",
        "        #    memory_key_padding_mask로 소스 패딩 무시\n",
        "        # ---------------------------\n",
        "        tgt2, attn_weights = self.multihead_attn(\n",
        "            tgt, memory, memory,\n",
        "            attn_mask=memory_mask,\n",
        "            key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "        tgt = tgt + self.dropout2(tgt2)  # 잔차 연결\n",
        "        tgt = self.norm2(tgt)            # 정규화\n",
        "\n",
        "        # ---------------------------\n",
        "        # (C) 위치별 FFN 블록\n",
        "        #  - 각 시점(feature 벡터)에 독립적으로 적용되는 2층 MLP\n",
        "        # ---------------------------\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)  # 잔차 연결\n",
        "        tgt = self.norm3(tgt)            # 정규화\n",
        "\n",
        "        # 디코더 레이어 출력과, 크로스 어텐션 가중치 반환\n",
        "        return tgt, attn_weights\n",
        "\n",
        "print(\"1단계 인코더 (Dummy + GTrends) 정의 완료\")"
      ],
      "metadata": {
        "id": "q3nHSo2y-K4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5gkl4dMHNQD"
      },
      "source": [
        "## 3. 🎯 GTM Step 1 모델"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GTM_Step1(L.LightningModule):\n",
        "    \"\"\"1단계: Temporal Features(시간 특성) + Google Trends 데이터만 사용해서 판매 예측\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_heads, num_layers,\n",
        "                 cat_dict, col_dict, fab_dict, trend_len, num_trends, gpu_num,\n",
        "                 use_encoder_mask=1, autoregressive=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # 주요 하이퍼파라미터 저장\n",
        "        self.hidden_dim = hidden_dim            # 내부 처리 차원 (은닉 벡터 크기)\n",
        "        self.embedding_dim = embedding_dim      # 임베딩 차원 (시간 특성 임베딩 크기)\n",
        "        self.output_len = output_dim            # 예측할 시점 개수\n",
        "        self.use_encoder_mask = use_encoder_mask# GTrends 인코더 마스크 사용 여부\n",
        "        self.autoregressive = autoregressive    # 자기회귀 예측 모드 여부\n",
        "        self.gpu_num = gpu_num                  # 사용 GPU 번호 (데이터 이동 시 사용)\n",
        "        self.save_hyperparameters()             # Lightning에서 하이퍼파라미터 자동 저장\n",
        "\n",
        "        # (1) 시간 특성 인코더 — 날짜 관련 스칼라(day/week/month/year)를 벡터로 변환\n",
        "        self.dummy_encoder = DummyEmbedder(embedding_dim)\n",
        "\n",
        "        # (2) Google Trends 인코더 — 시계열 데이터를 Transformer 인코더로 변환\n",
        "        #    - output_dim: 예측 길이\n",
        "        #    - hidden_dim: 내부 처리 차원\n",
        "        #    - trend_len: 시계열 길이\n",
        "        #    - num_trends: 트렌드 채널 수\n",
        "        self.gtrend_encoder = GTrendEmbedder(output_dim, hidden_dim,\n",
        "                                             use_encoder_mask, trend_len,\n",
        "                                             num_trends, gpu_num)\n",
        "\n",
        "        # (3) 시간 특성과 트렌드 정보를 결합하기 전에,\n",
        "        #     시간 특성 벡터를 은닉 차원 크기로 변환하는 작은 네트워크\n",
        "        self.feature_fusion = nn.Sequential(\n",
        "            nn.BatchNorm1d(embedding_dim),      # 배치 정규화 (학습 안정화)\n",
        "            nn.Linear(embedding_dim, hidden_dim),# 차원 변환\n",
        "            nn.ReLU(),                          # 비선형 활성화\n",
        "            nn.Dropout(0.2)                     # 과적합 방지\n",
        "        )\n",
        "\n",
        "        # (4) Transformer 디코더 레이어\n",
        "        #     - Cross-Attention을 사용해 시간 특성(tgt)와 트렌드 인코딩(memory) 연결\n",
        "        self.decoder_layer = TransformerDecoderLayer(\n",
        "            d_model=self.hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=self.hidden_dim * 4,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        # (5) 자기회귀 모드인 경우, 디코더 입력에 Positional Encoding 추가\n",
        "        if self.autoregressive:\n",
        "            self.pos_encoder = PositionalEncoding(hidden_dim, max_len=12)\n",
        "\n",
        "        # (6) 디코더 출력 → 최종 예측값 변환하는 FC 레이어\n",
        "        #     - 비자가회귀: 한 번에 output_len 길이 예측\n",
        "        #     - 자기회귀: 한 번에 1 시점만 예측\n",
        "        self.decoder_fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim,\n",
        "                      self.output_len if not self.autoregressive else 1),\n",
        "            nn.Dropout(0.2)\n",
        "        )"
      ],
      "metadata": {
        "id": "vUebnojPEAVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def _generate_square_subsequent_mask(self, size):\n",
        "        \"\"\"\n",
        "        자기회귀(Autoregressive) 예측에서 미래 시점 정보를 보지 않도록 만드는 마스크 생성 함수\n",
        "        - size: 시퀀스 길이\n",
        "        - torch.triu(...): 상삼각행렬을 만들어, 현재 시점 이후(미래) 값은 차단\n",
        "        - transpose(0,1): Transformer 규칙에 맞게 차원 전환\n",
        "        - masked_fill: 0인 부분(미래 시점)은 -inf로 채워서 attention에서 무시\n",
        "                       1인 부분(현재 및 과거 시점)은 0.0으로 채워서 attention 허용\n",
        "        \"\"\"\n",
        "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, category, color, fabric, temporal_features, gtrends, images):\n",
        "        \"\"\"\n",
        "        모델 순전파(forward) 과정\n",
        "        category, color, fabric: (현재 단계에서는 사용 안 함)\n",
        "        temporal_features: 날짜/시간 관련 특성 (day/week/month/year 등)\n",
        "        gtrends: Google Trends 시계열 데이터\n",
        "        images: (현재 단계에서는 사용 안 함)\n",
        "        \"\"\"\n",
        "\n",
        "        # (1) 날짜/시간 특성 → 임베딩\n",
        "        dummy_encoding = self.dummy_encoder(temporal_features)\n",
        "\n",
        "        # (2) Google Trends 시계열 데이터 → Transformer 인코딩\n",
        "        gtrend_encoding = self.gtrend_encoder(gtrends)\n",
        "\n",
        "        # (3) 시간 특성 임베딩을 은닉 차원 크기로 변환 (BatchNorm → Linear → ReLU → Dropout)\n",
        "        static_feature_fusion = self.feature_fusion(dummy_encoding)\n",
        "\n",
        "        # (4) 디코더 입력 준비\n",
        "        #     - Transformer 디코더는 [시퀀스 길이, 배치 크기, 특성 차원] 형태를 기대\n",
        "        #     - 여기서는 한 시점 정보만 넣으므로 unsqueeze(0)으로 seq_len = 1 추가\n",
        "        tgt = static_feature_fusion.unsqueeze(0)\n",
        "\n",
        "        # (5) 디코더의 'memory'는 인코더에서 나온 Google Trends 인코딩 벡터\n",
        "        memory = gtrend_encoding\n",
        "\n",
        "        # (6) Transformer 디코더 레이어 통과\n",
        "        #     - tgt: 시간 특성\n",
        "        #     - memory: 트렌드 인코딩 (Cross-Attention)\n",
        "        decoder_out, attn_weights = self.decoder_layer(tgt, memory)\n",
        "\n",
        "        # (7) 디코더 출력 → 최종 예측값 변환\n",
        "        forecast = self.decoder_fc(decoder_out)\n",
        "\n",
        "        # (8) [seq_len, batch, output_len] → [batch, output_len] 형태로 변환\n",
        "        return forecast.view(-1, self.output_len), attn_weights"
      ],
      "metadata": {
        "id": "HfEG0TxKE3ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        학습에 사용할 옵티마이저(Optimizer) 설정\n",
        "        - Adafactor: Adam의 변형으로, 메모리를 절약하고 학습 속도를 높이는 최적화 알고리즘\n",
        "        - scale_parameter, relative_step, warmup_init: 학습률 자동 조정 옵션\n",
        "        - lr=None: 학습률은 Adafactor 내부에서 자동 결정\n",
        "        \"\"\"\n",
        "        optimizer = Adafactor(self.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        학습 단계에서 한 배치(batch)를 처리하는 함수\n",
        "        - batch: (item_sales, category, color, fabric, temporal_features, gtrends, images) 형태\n",
        "        - batch_idx: 현재 배치 번호\n",
        "        \"\"\"\n",
        "\n",
        "        # 배치에서 각 데이터 꺼내기\n",
        "        item_sales, category, color, fabric, temporal_features, gtrends, images = batch\n",
        "\n",
        "        # temporal_features와 gtrends가 학습 중에 기울기를 계산할 수 있도록 설정\n",
        "        temporal_features = temporal_features.requires_grad_(True)\n",
        "        gtrends = gtrends.requires_grad_(True)\n",
        "\n",
        "        # 모델을 통과시켜 예측값(forecasted_sales) 구하기\n",
        "        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n",
        "\n",
        "        # 예측값과 실제 판매량(item_sales)의 차이를 MSE(평균제곱오차)로 계산\n",
        "        loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
        "\n",
        "        # 학습 중 loss를 기록(log) → prog_bar=True이면 진행바에 표시됨\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "\n",
        "        # Lightning이 이 값을 기반으로 역전파(Backpropagation) 진행\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        검증 단계에서 한 배치(batch)를 처리하는 함수\n",
        "        - 학습과 달리 loss로 가중치를 업데이트하지 않고, 성능만 확인\n",
        "        \"\"\"\n",
        "\n",
        "        # 배치에서 각 데이터 꺼내기\n",
        "        item_sales, category, color, fabric, temporal_features, gtrends, images = batch\n",
        "\n",
        "        # forward()로 예측값 계산\n",
        "        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n",
        "\n",
        "        # validation 결과를 리스트에 저장 (나중에 한 번에 계산하기 위함)\n",
        "        if not hasattr(self, 'validation_step_outputs'):\n",
        "            self.validation_step_outputs = []\n",
        "        self.validation_step_outputs.append((item_sales.squeeze(), forecasted_sales.squeeze()))\n",
        "\n",
        "        # 현재 배치의 실제값과 예측값 반환 (다음 단계에서 사용 가능)\n",
        "        return item_sales.squeeze(), forecasted_sales.squeeze()"
      ],
      "metadata": {
        "id": "1dzOAxR3FAtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def on_validation_epoch_end(self):\n",
        "        \"\"\"\n",
        "        한 번의 검증(epoch)이 끝난 후 성능을 계산하고 기록하는 함수\n",
        "        - validation_step에서 저장한 모든 배치의 결과를 모아 평균적인 성능 지표를 계산\n",
        "        \"\"\"\n",
        "\n",
        "        # validation_step에서 결과를 저장한 리스트가 있는지 확인\n",
        "        if hasattr(self, 'validation_step_outputs'):\n",
        "            val_step_outputs = self.validation_step_outputs  # [(실제값, 예측값), ...] 형태\n",
        "\n",
        "            # 리스트에서 실제 판매량(item_sales)과 예측값(forecasted_sales)만 각각 추출\n",
        "            item_sales = [x[0] for x in val_step_outputs]\n",
        "            forecasted_sales = [x[1] for x in val_step_outputs]\n",
        "\n",
        "            # 리스트를 하나의 텐서로 합치기 (stack)\n",
        "            item_sales = torch.stack(item_sales)\n",
        "            forecasted_sales = torch.stack(forecasted_sales)\n",
        "\n",
        "            # 예측과 실제값을 원래 스케일로 복원 (학습 시 정규화했으므로 1065 곱함)\n",
        "            rescaled_item_sales = item_sales * 1065\n",
        "            rescaled_forecasted_sales = forecasted_sales * 1065\n",
        "\n",
        "            # MSE(평균제곱오차) 계산 - 정규화된 값 기준\n",
        "            loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
        "\n",
        "            # MAE(평균절대오차) 계산 - 원래 스케일 기준\n",
        "            mae = F.l1_loss(rescaled_item_sales, rescaled_forecasted_sales)\n",
        "\n",
        "            # 성능 지표 기록 (진행바에도 표시)\n",
        "            self.log('val_mae', mae, prog_bar=True)   # 예측 정확도\n",
        "            self.log('val_loss', loss, prog_bar=True) # 손실값\n",
        "\n",
        "            # 다음 epoch을 위해 결과 리스트 초기화\n",
        "            self.validation_step_outputs.clear()"
      ],
      "metadata": {
        "id": "3Xti99ULFMIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iFYv579HNQD"
      },
      "source": [
        "## 4. 📊 데이터셋 클래스"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ZeroShotDataset():\n",
        "    \"\"\"\n",
        "    판매량 예측을 위한 데이터셋 클래스\n",
        "    - Google Trends, 카테고리/색상/원단 정보, 이미지 데이터를 묶어서 모델에 전달할 수 있게 전처리함\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_df, img_root, gtrends, cat_dict, col_dict, fab_dict, trend_len):\n",
        "        # 초기 설정 (데이터와 필요한 매핑 정보 저장)\n",
        "        self.data_df = data_df               # 상품 정보가 담긴 데이터프레임\n",
        "        self.gtrends = gtrends               # Google Trends 데이터\n",
        "        self.cat_dict = cat_dict             # 카테고리 → 숫자 매핑\n",
        "        self.col_dict = col_dict             # 색상 → 숫자 매핑\n",
        "        self.fab_dict = fab_dict             # 원단 → 숫자 매핑\n",
        "        self.trend_len = trend_len           # Google Trends 시계열 길이\n",
        "        self.img_root = img_root             # 이미지 파일이 저장된 폴더 경로\n",
        "\n",
        "    def __len__(self):\n",
        "        # 전체 데이터 개수 반환 (len(dataset) 할 때 호출됨)\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 특정 인덱스의 데이터 한 줄을 반환\n",
        "        return self.data_df.iloc[idx, :]\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"\n",
        "        원본 데이터를 모델이 학습하기 좋은 형태로 변환하는 함수\n",
        "        1) Google Trends 데이터를 추출 & 정규화\n",
        "        2) 이미지를 불러와서 텐서로 변환\n",
        "        3) 카테고리, 색상, 원단을 숫자로 변환\n",
        "        4) 텐서 형태의 학습용 데이터셋으로 반환\n",
        "        \"\"\"\n",
        "\n",
        "        data = self.data_df\n",
        "        gtrends, image_features = [], []\n",
        "\n",
        "        # 이미지 변환 (크기 조정, 텐서 변환, 정규화)\n",
        "        img_transforms = Compose([\n",
        "            Resize((256, 256)),\n",
        "            ToTensor(),\n",
        "            Normalize(mean=[0.485, 0.456, 0.406],  # 이미지 색상 평균\n",
        "                      std=[0.229, 0.224, 0.225])  # 이미지 색상 표준편차\n",
        "        ])\n",
        "\n",
        "        # 데이터프레임의 각 행(row)을 하나씩 처리\n",
        "        for (idx, row) in tqdm(data.iterrows(), total=len(data), ascii=True, desc=\"데이터 전처리\"):\n",
        "            cat, col, fab, fiq_attr, start_date, img_path = \\\n",
        "                row['category'], row['color'], row['fabric'], row['extra'], \\\n",
        "                row['release_date'], row['image_path']\n",
        "\n",
        "            # Google Trends 데이터: 출시일 기준 1년(52주) 전부터 데이터 가져오기\n",
        "            gtrend_start = start_date - pd.DateOffset(weeks=52)\n",
        "            cat_gtrend = self.gtrends.loc[gtrend_start:start_date][cat][-52:].values[:self.trend_len]\n",
        "            col_gtrend = self.gtrends.loc[gtrend_start:start_date][col][-52:].values[:self.trend_len]\n",
        "            fab_gtrend = self.gtrends.loc[gtrend_start:start_date][fab][-52:].values[:self.trend_len]\n",
        "\n",
        "            # Min-Max 정규화 (0~1 범위로)\n",
        "            cat_gtrend = MinMaxScaler().fit_transform(cat_gtrend.reshape(-1,1)).flatten()\n",
        "            col_gtrend = MinMaxScaler().fit_transform(col_gtrend.reshape(-1,1)).flatten()\n",
        "            fab_gtrend = MinMaxScaler().fit_transform(fab_gtrend.reshape(-1,1)).flatten()\n",
        "\n",
        "            # 3개의 트렌드 데이터를 하나로 합치기\n",
        "            multitrends = np.vstack([cat_gtrend, col_gtrend, fab_gtrend])\n",
        "\n",
        "            # 이미지 불러오기 (RGB 변환)\n",
        "            img = Image.open(os.path.join(self.img_root, img_path)).convert('RGB')\n",
        "\n",
        "            # 리스트에 저장\n",
        "            gtrends.append(multitrends)\n",
        "            image_features.append(img_transforms(img))\n",
        "\n",
        "        # 리스트를 numpy 배열로 변환\n",
        "        gtrends = np.array(gtrends)\n",
        "\n",
        "        # 필요 없는 컬럼 삭제\n",
        "        data = data.copy()\n",
        "        data.drop(['external_code', 'season', 'release_date', 'image_path'], axis=1, inplace=True)\n",
        "\n",
        "        # 텐서 형태로 변환\n",
        "        item_sales = torch.FloatTensor(data.iloc[:, :12].values)     # 판매량\n",
        "        temporal_features = torch.FloatTensor(data.iloc[:, 13:17].values)  # 시간 관련 특성\n",
        "\n",
        "        # 카테고리/색상/원단 → 숫자 ID 변환\n",
        "        categories = [self.cat_dict[val] for val in data.category.values]\n",
        "        colors = [self.col_dict[val] for val in data.color.values]\n",
        "        fabrics = [self.fab_dict[val] for val in data.fabric.values]\n",
        "\n",
        "        categories = torch.LongTensor(categories)\n",
        "        colors = torch.LongTensor(colors)\n",
        "        fabrics = torch.LongTensor(fabrics)\n",
        "\n",
        "        gtrends = torch.FloatTensor(gtrends)        # Google Trends 데이터\n",
        "        images = torch.stack(image_features)        # 이미지 데이터\n",
        "\n",
        "        # 학습에 사용할 TensorDataset 반환\n",
        "        return TensorDataset(item_sales, categories, colors, fabrics, temporal_features, gtrends, images)\n",
        "\n",
        "    def get_loader(self, batch_size, train=True):\n",
        "        \"\"\"\n",
        "        DataLoader 생성 함수\n",
        "        - batch_size 크기로 데이터를 나눠서 모델에 공급\n",
        "        - 학습 모드일 때는 데이터 순서를 섞음(shuffle=True)\n",
        "        \"\"\"\n",
        "        print(' 1단계 데이터셋 생성 시작...')\n",
        "        data_with_gtrends = self.preprocess_data()\n",
        "        if train:\n",
        "            data_loader = DataLoader(data_with_gtrends, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "        else:\n",
        "            data_loader = DataLoader(data_with_gtrends, batch_size=1, shuffle=False, num_workers=2)\n",
        "        print(' 1단계 데이터셋 생성 완료')\n",
        "        return data_loader\n",
        "\n",
        "print(\" 데이터셋 클래스 정의 완료\")"
      ],
      "metadata": {
        "id": "naHwill1KkBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dK7aLl2HNQE"
      },
      "source": [
        "## 5. 🚀 1단계 실행 코드\n",
        "### 데이터 로딩부터 모델 훈련까지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ojcXcBnHNQE"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 경로 설정\n",
        "dataset_path = Path('/content/drive/MyDrive/GTM-dataset-small/')\n",
        "\n",
        "# 데이터 로딩\n",
        "print(\" 데이터 로딩 중...\")\n",
        "train_df = pd.read_csv(dataset_path / 'train.csv', parse_dates=['release_date'])\n",
        "test_df = pd.read_csv(dataset_path / 'test.csv', parse_dates=['release_date'])\n",
        "gtrends = pd.read_csv(dataset_path / 'gtrends.csv', index_col=[0], parse_dates=True)\n",
        "\n",
        "cat_dict = torch.load(dataset_path / 'category_labels.pt', weights_only=False)\n",
        "col_dict = torch.load(dataset_path / 'color_labels.pt', weights_only=False)\n",
        "fab_dict = torch.load(dataset_path / 'fabric_labels.pt', weights_only=False)\n",
        "\n",
        "print(f\" 훈련 데이터: {len(train_df):,}개\")\n",
        "print(f\" 테스트 데이터: {len(test_df):,}개\")\n",
        "print(f\" Google Trends: {len(gtrends):,}개 시점\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL3Uurx5HNQE"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 생성\n",
        "train_dataset = ZeroShotDataset(train_df, dataset_path / 'images', gtrends, cat_dict, col_dict, fab_dict, trend_len=52)\n",
        "test_dataset = ZeroShotDataset(test_df, dataset_path / 'images', gtrends, cat_dict, col_dict, fab_dict, trend_len=52)\n",
        "\n",
        "BATCH_SIZE = 8 if torch.cuda.is_available() else 4\n",
        "train_loader = train_dataset.get_loader(batch_size=BATCH_SIZE, train=True)\n",
        "test_loader = test_dataset.get_loader(batch_size=1, train=False)\n",
        "\n",
        "print(f\" 배치 크기: {BATCH_SIZE}\")\n",
        "print(f\" 훈련 배치 수: {len(train_loader)}\")\n",
        "print(f\" 테스트 배치 수: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4OuU5wmHNQE"
      },
      "outputs": [],
      "source": [
        "# 1단계 모델 생성\n",
        "print(\" GTM Step 1 모델 생성 중...\")\n",
        "\n",
        "model = GTM_Step1(\n",
        "    embedding_dim=32,\n",
        "    hidden_dim=64,\n",
        "    output_dim=12,\n",
        "    num_heads=4,\n",
        "    num_layers=1,\n",
        "    cat_dict=cat_dict,\n",
        "    col_dict=col_dict,\n",
        "    fab_dict=fab_dict,\n",
        "    trend_len=52,\n",
        "    num_trends=3,\n",
        "    gpu_num=0,\n",
        "    use_encoder_mask=1,\n",
        "    autoregressive=False\n",
        ")\n",
        "\n",
        "print(f\" Step 1 모델 생성 완료!\")\n",
        "print(f\" 모델 파라미터: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(\"\\n 사용 모달리티: Temporal Features (시간 정보) + Google Trends\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reAKbS6OHNQE"
      },
      "outputs": [],
      "source": [
        "# Trainer 설정 및 훈련\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.loggers import CSVLogger\n",
        "\n",
        "EPOCHS = 5\n",
        "ACCELERATOR = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath='./checkpoints/',\n",
        "    filename='gtm-step1-{epoch:02d}-{val_mae:.2f}',\n",
        "    monitor='val_mae',\n",
        "    mode='min',\n",
        "    save_top_k=2\n",
        ")\n",
        "\n",
        "csv_logger = CSVLogger(save_dir='./logs/', name='gtm_step1')\n",
        "\n",
        "trainer = L.Trainer(\n",
        "    devices=1,\n",
        "    accelerator=ACCELERATOR,\n",
        "    max_epochs=EPOCHS,\n",
        "    logger=csv_logger,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    enable_progress_bar=True,\n",
        "    gradient_clip_val=1.0\n",
        ")\n",
        "\n",
        "print(\"🚀 GTM Step 1 훈련 시작!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)\n",
        "    print(\"\\n Step 1 훈련 완료!\")\n",
        "    print(f\"💾 최고 모델: {checkpoint_callback.best_model_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Step 1 훈련 실패: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HrK9vcHHNQE"
      },
      "source": [
        "## 📋 1단계 요약\n",
        "\n",
        "### ✅ 구현 완료\n",
        "- **시간적 특성 임베딩**: 날짜 정보 (일, 주, 월, 년)를 벡터로 변환\n",
        "- **Google Trends 인코딩**: Transformer Encoder로 시계열 패턴 학습\n",
        "- **기본 Cross-Attention**: 시간 정보와 트렌드 데이터 간 관계 학습\n",
        "\n",
        "### 🎯 학습 목표 달성\n",
        "- Transformer 기본 구조 이해\n",
        "- 시계열 데이터 인코딩 방법\n",
        "- Multi-modal 입력 처리 기초\n",
        "\n",
        "### 🔜 다음 단계 예고\n",
        "**Step 2**에서는 **이미지 정보**를 추가하여 시각적 특성도 함께 학습합니다!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}